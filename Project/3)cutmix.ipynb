{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfGrai_Qt7Ny"
   },
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgAiImV0uURP",
    "outputId": "948f3caa-995f-4d1d-a258-f324158afaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.4), ratio=(0.3, 3.3), value=(125,122,114), inplace=False)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hldipDVsv-Jt"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def train(epoch, net, criterion, trainloader,scheduler):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    beta = 1\n",
    "    cutmix_prob = 1\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        r = np.random.rand(1)\n",
    "        \n",
    "        if beta > 0 and r < cutmix_prob:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(beta, beta)\n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            # compute output\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "        else:\n",
    "            # compute output\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        #outputs = net(inputs)\n",
    "        #loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "            print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VkooK-hQu4a6"
   },
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss= criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jEj8J7xqwAxD"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vlCAjBEWwXNo"
   },
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "ArgupDVRwB8i",
    "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-838de3351049>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut_w = np.int(W * cut_rat)\n",
      "<ipython-input-6-838de3351049>:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut_h = np.int(H * cut_rat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 2.1604, accuracy : 20.09\n",
      "iteration : 100, loss : 2.1222, accuracy : 22.63\n",
      "iteration : 150, loss : 2.0871, accuracy : 24.71\n",
      "iteration : 200, loss : 2.0456, accuracy : 26.51\n",
      "iteration : 250, loss : 2.0145, accuracy : 27.87\n",
      "iteration : 300, loss : 1.9844, accuracy : 29.26\n",
      "iteration : 350, loss : 1.9477, accuracy : 31.05\n",
      "Epoch :   0, training loss : 1.9326, training accuracy : 31.82, test loss : 1.3525, test accuracy : 50.52\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.8020, accuracy : 37.53\n",
      "iteration : 100, loss : 1.7676, accuracy : 39.32\n",
      "iteration : 150, loss : 1.7527, accuracy : 39.95\n",
      "iteration : 200, loss : 1.7531, accuracy : 40.07\n",
      "iteration : 250, loss : 1.7355, accuracy : 41.03\n",
      "iteration : 300, loss : 1.7151, accuracy : 42.10\n",
      "iteration : 350, loss : 1.7044, accuracy : 42.52\n",
      "Epoch :   1, training loss : 1.7014, training accuracy : 42.52, test loss : 1.0973, test accuracy : 62.19\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 1.5965, accuracy : 45.17\n",
      "iteration : 100, loss : 1.6338, accuracy : 45.30\n",
      "iteration : 150, loss : 1.6492, accuracy : 44.77\n",
      "iteration : 200, loss : 1.6194, accuracy : 46.89\n",
      "iteration : 250, loss : 1.6081, accuracy : 47.57\n",
      "iteration : 300, loss : 1.5951, accuracy : 48.15\n",
      "iteration : 350, loss : 1.5942, accuracy : 48.10\n",
      "Epoch :   2, training loss : 1.5845, training accuracy : 48.56, test loss : 1.1761, test accuracy : 59.88\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 1.4557, accuracy : 54.16\n",
      "iteration : 100, loss : 1.4701, accuracy : 55.20\n",
      "iteration : 150, loss : 1.4870, accuracy : 53.08\n",
      "iteration : 200, loss : 1.4928, accuracy : 52.33\n",
      "iteration : 250, loss : 1.4993, accuracy : 51.81\n",
      "iteration : 300, loss : 1.4872, accuracy : 52.65\n",
      "iteration : 350, loss : 1.4946, accuracy : 52.22\n",
      "Epoch :   3, training loss : 1.4959, training accuracy : 52.00, test loss : 0.8468, test accuracy : 72.38\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 1.4714, accuracy : 53.98\n",
      "iteration : 100, loss : 1.4710, accuracy : 54.16\n",
      "iteration : 150, loss : 1.4698, accuracy : 55.19\n",
      "iteration : 200, loss : 1.4580, accuracy : 55.65\n",
      "iteration : 250, loss : 1.4580, accuracy : 55.14\n",
      "iteration : 300, loss : 1.4562, accuracy : 54.64\n",
      "iteration : 350, loss : 1.4536, accuracy : 54.47\n",
      "Epoch :   4, training loss : 1.4470, training accuracy : 54.73, test loss : 0.8106, test accuracy : 73.68\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 1.3873, accuracy : 56.47\n",
      "iteration : 100, loss : 1.4240, accuracy : 55.60\n",
      "iteration : 150, loss : 1.4094, accuracy : 57.03\n",
      "iteration : 200, loss : 1.4025, accuracy : 56.93\n",
      "iteration : 250, loss : 1.3933, accuracy : 57.33\n",
      "iteration : 300, loss : 1.3936, accuracy : 57.29\n",
      "iteration : 350, loss : 1.3916, accuracy : 57.54\n",
      "Epoch :   5, training loss : 1.3926, training accuracy : 56.88, test loss : 0.7636, test accuracy : 76.14\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 1.3190, accuracy : 59.42\n",
      "iteration : 100, loss : 1.3482, accuracy : 56.84\n",
      "iteration : 150, loss : 1.3379, accuracy : 58.40\n",
      "iteration : 200, loss : 1.3310, accuracy : 58.34\n",
      "iteration : 250, loss : 1.3288, accuracy : 58.78\n",
      "iteration : 300, loss : 1.3468, accuracy : 57.63\n",
      "iteration : 350, loss : 1.3529, accuracy : 57.58\n",
      "Epoch :   6, training loss : 1.3472, training accuracy : 58.03, test loss : 0.6904, test accuracy : 77.41\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 1.3376, accuracy : 58.00\n",
      "iteration : 100, loss : 1.3132, accuracy : 59.86\n",
      "iteration : 150, loss : 1.3158, accuracy : 59.76\n",
      "iteration : 200, loss : 1.3264, accuracy : 58.68\n",
      "iteration : 250, loss : 1.3272, accuracy : 58.39\n",
      "iteration : 300, loss : 1.3344, accuracy : 57.83\n",
      "iteration : 350, loss : 1.3323, accuracy : 57.98\n",
      "Epoch :   7, training loss : 1.3285, training accuracy : 58.40, test loss : 0.6221, test accuracy : 80.44\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 1.2866, accuracy : 63.19\n",
      "iteration : 100, loss : 1.3029, accuracy : 60.95\n",
      "iteration : 150, loss : 1.3167, accuracy : 59.50\n",
      "iteration : 200, loss : 1.3134, accuracy : 60.15\n",
      "iteration : 250, loss : 1.3030, accuracy : 60.90\n",
      "iteration : 300, loss : 1.3083, accuracy : 60.77\n",
      "iteration : 350, loss : 1.2973, accuracy : 60.96\n",
      "Epoch :   8, training loss : 1.2943, training accuracy : 61.39, test loss : 0.6901, test accuracy : 78.27\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 1.2989, accuracy : 58.86\n",
      "iteration : 100, loss : 1.2923, accuracy : 59.68\n",
      "iteration : 150, loss : 1.2756, accuracy : 60.75\n",
      "iteration : 200, loss : 1.2627, accuracy : 61.26\n",
      "iteration : 250, loss : 1.2639, accuracy : 61.60\n",
      "iteration : 300, loss : 1.2634, accuracy : 61.53\n",
      "iteration : 350, loss : 1.2671, accuracy : 60.75\n",
      "Epoch :   9, training loss : 1.2614, training accuracy : 61.10, test loss : 0.5921, test accuracy : 81.44\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 1.2720, accuracy : 59.34\n",
      "iteration : 100, loss : 1.3111, accuracy : 55.72\n",
      "iteration : 150, loss : 1.3072, accuracy : 57.92\n",
      "iteration : 200, loss : 1.2995, accuracy : 59.09\n",
      "iteration : 250, loss : 1.3022, accuracy : 58.85\n",
      "iteration : 300, loss : 1.2986, accuracy : 58.85\n",
      "iteration : 350, loss : 1.2905, accuracy : 59.36\n",
      "Epoch :  10, training loss : 1.2882, training accuracy : 59.26, test loss : 0.5545, test accuracy : 84.71\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 1.2132, accuracy : 65.09\n",
      "iteration : 100, loss : 1.2110, accuracy : 63.86\n",
      "iteration : 150, loss : 1.1947, accuracy : 63.60\n",
      "iteration : 200, loss : 1.2063, accuracy : 62.66\n",
      "iteration : 250, loss : 1.2062, accuracy : 62.95\n",
      "iteration : 300, loss : 1.2123, accuracy : 62.73\n",
      "iteration : 350, loss : 1.2153, accuracy : 62.89\n",
      "Epoch :  11, training loss : 1.2165, training accuracy : 62.99, test loss : 0.5593, test accuracy : 84.27\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 1.2223, accuracy : 59.09\n",
      "iteration : 100, loss : 1.2560, accuracy : 57.91\n",
      "iteration : 150, loss : 1.2463, accuracy : 59.59\n",
      "iteration : 200, loss : 1.2304, accuracy : 60.85\n",
      "iteration : 250, loss : 1.2304, accuracy : 60.97\n",
      "iteration : 300, loss : 1.2268, accuracy : 61.51\n",
      "iteration : 350, loss : 1.2276, accuracy : 61.58\n",
      "Epoch :  12, training loss : 1.2214, training accuracy : 62.31, test loss : 0.5152, test accuracy : 84.34\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 1.1466, accuracy : 69.34\n",
      "iteration : 100, loss : 1.2110, accuracy : 63.90\n",
      "iteration : 150, loss : 1.2094, accuracy : 64.02\n",
      "iteration : 200, loss : 1.2199, accuracy : 62.63\n",
      "iteration : 250, loss : 1.2152, accuracy : 62.70\n",
      "iteration : 300, loss : 1.2174, accuracy : 62.45\n",
      "iteration : 350, loss : 1.2118, accuracy : 62.81\n",
      "Epoch :  13, training loss : 1.2132, training accuracy : 62.69, test loss : 0.5809, test accuracy : 83.48\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 1.1996, accuracy : 64.98\n",
      "iteration : 100, loss : 1.1993, accuracy : 64.13\n",
      "iteration : 150, loss : 1.1788, accuracy : 65.40\n",
      "iteration : 200, loss : 1.1875, accuracy : 64.70\n",
      "iteration : 250, loss : 1.1654, accuracy : 65.36\n",
      "iteration : 300, loss : 1.1769, accuracy : 64.72\n",
      "iteration : 350, loss : 1.1782, accuracy : 64.37\n",
      "Epoch :  14, training loss : 1.1655, training accuracy : 65.10, test loss : 0.4845, test accuracy : 85.41\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 1.1702, accuracy : 64.75\n",
      "iteration : 100, loss : 1.1272, accuracy : 67.01\n",
      "iteration : 150, loss : 1.0996, accuracy : 68.04\n",
      "iteration : 200, loss : 1.1201, accuracy : 66.28\n",
      "iteration : 250, loss : 1.1290, accuracy : 66.05\n",
      "iteration : 300, loss : 1.1340, accuracy : 66.17\n",
      "iteration : 350, loss : 1.1359, accuracy : 66.09\n",
      "Epoch :  15, training loss : 1.1402, training accuracy : 65.50, test loss : 0.6064, test accuracy : 82.83\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 1.1250, accuracy : 64.39\n",
      "iteration : 100, loss : 1.1123, accuracy : 66.77\n",
      "iteration : 150, loss : 1.1094, accuracy : 66.26\n",
      "iteration : 200, loss : 1.1306, accuracy : 64.36\n",
      "iteration : 250, loss : 1.1255, accuracy : 64.84\n",
      "iteration : 300, loss : 1.1249, accuracy : 65.62\n",
      "iteration : 350, loss : 1.1345, accuracy : 65.34\n",
      "Epoch :  16, training loss : 1.1357, training accuracy : 64.92, test loss : 0.5409, test accuracy : 84.04\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 1.2406, accuracy : 62.16\n",
      "iteration : 100, loss : 1.2099, accuracy : 62.21\n",
      "iteration : 150, loss : 1.2045, accuracy : 62.66\n",
      "iteration : 200, loss : 1.1918, accuracy : 63.78\n",
      "iteration : 250, loss : 1.1977, accuracy : 63.70\n",
      "iteration : 300, loss : 1.1818, accuracy : 64.09\n",
      "iteration : 350, loss : 1.1664, accuracy : 65.26\n",
      "Epoch :  17, training loss : 1.1674, training accuracy : 65.61, test loss : 0.5017, test accuracy : 86.77\n",
      "\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 1.1363, accuracy : 60.83\n",
      "iteration : 100, loss : 1.1405, accuracy : 64.30\n",
      "iteration : 150, loss : 1.1239, accuracy : 64.81\n",
      "iteration : 200, loss : 1.1235, accuracy : 64.96\n",
      "iteration : 250, loss : 1.1256, accuracy : 65.50\n",
      "iteration : 300, loss : 1.1143, accuracy : 65.81\n",
      "iteration : 350, loss : 1.1219, accuracy : 65.36\n",
      "Epoch :  18, training loss : 1.1256, training accuracy : 65.01, test loss : 0.5425, test accuracy : 84.01\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 1.1240, accuracy : 65.36\n",
      "iteration : 100, loss : 1.0779, accuracy : 68.04\n",
      "iteration : 150, loss : 1.0688, accuracy : 68.71\n",
      "iteration : 200, loss : 1.1099, accuracy : 66.84\n",
      "iteration : 250, loss : 1.1317, accuracy : 66.07\n",
      "iteration : 300, loss : 1.1244, accuracy : 66.37\n",
      "iteration : 350, loss : 1.1166, accuracy : 66.65\n",
      "Epoch :  19, training loss : 1.1169, training accuracy : 66.41, test loss : 0.4795, test accuracy : 85.78\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 1.1084, accuracy : 70.12\n",
      "iteration : 100, loss : 1.1176, accuracy : 68.75\n",
      "iteration : 150, loss : 1.1101, accuracy : 68.37\n",
      "iteration : 200, loss : 1.1029, accuracy : 68.89\n",
      "iteration : 250, loss : 1.1058, accuracy : 67.92\n",
      "iteration : 300, loss : 1.1107, accuracy : 67.28\n",
      "iteration : 350, loss : 1.1150, accuracy : 66.64\n",
      "Epoch :  20, training loss : 1.1179, training accuracy : 66.84, test loss : 0.4652, test accuracy : 87.25\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 1.1366, accuracy : 70.56\n",
      "iteration : 100, loss : 1.1403, accuracy : 66.44\n",
      "iteration : 150, loss : 1.1468, accuracy : 66.14\n",
      "iteration : 200, loss : 1.1337, accuracy : 66.86\n",
      "iteration : 250, loss : 1.1309, accuracy : 67.76\n",
      "iteration : 300, loss : 1.1219, accuracy : 67.82\n",
      "iteration : 350, loss : 1.1180, accuracy : 67.54\n",
      "Epoch :  21, training loss : 1.1112, training accuracy : 68.05, test loss : 0.4613, test accuracy : 86.56\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 1.1170, accuracy : 68.11\n",
      "iteration : 100, loss : 1.1015, accuracy : 67.43\n",
      "iteration : 150, loss : 1.1045, accuracy : 67.59\n",
      "iteration : 200, loss : 1.1093, accuracy : 67.60\n",
      "iteration : 250, loss : 1.1096, accuracy : 67.53\n",
      "iteration : 300, loss : 1.1028, accuracy : 68.14\n",
      "iteration : 350, loss : 1.0903, accuracy : 68.66\n",
      "Epoch :  22, training loss : 1.0845, training accuracy : 68.73, test loss : 0.4753, test accuracy : 86.24\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 1.0812, accuracy : 71.56\n",
      "iteration : 100, loss : 1.0906, accuracy : 68.79\n",
      "iteration : 150, loss : 1.0914, accuracy : 68.42\n",
      "iteration : 200, loss : 1.0813, accuracy : 69.00\n",
      "iteration : 250, loss : 1.0660, accuracy : 70.03\n",
      "iteration : 300, loss : 1.0768, accuracy : 68.51\n",
      "iteration : 350, loss : 1.0793, accuracy : 68.31\n",
      "Epoch :  23, training loss : 1.0758, training accuracy : 68.52, test loss : 0.4935, test accuracy : 86.73\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 1.0371, accuracy : 71.48\n",
      "iteration : 100, loss : 1.0317, accuracy : 71.25\n",
      "iteration : 150, loss : 1.0326, accuracy : 72.15\n",
      "iteration : 200, loss : 1.0291, accuracy : 72.09\n",
      "iteration : 250, loss : 1.0383, accuracy : 70.99\n",
      "iteration : 300, loss : 1.0422, accuracy : 70.26\n",
      "iteration : 350, loss : 1.0454, accuracy : 70.14\n",
      "Epoch :  24, training loss : 1.0554, training accuracy : 69.62, test loss : 0.4606, test accuracy : 88.04\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 1.0759, accuracy : 66.27\n",
      "iteration : 100, loss : 1.0652, accuracy : 67.23\n",
      "iteration : 150, loss : 1.0467, accuracy : 68.94\n",
      "iteration : 200, loss : 1.0501, accuracy : 68.45\n",
      "iteration : 250, loss : 1.0394, accuracy : 69.58\n",
      "iteration : 300, loss : 1.0313, accuracy : 70.39\n",
      "iteration : 350, loss : 1.0409, accuracy : 69.72\n",
      "Epoch :  25, training loss : 1.0455, training accuracy : 68.91, test loss : 0.4242, test accuracy : 88.20\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 1.0352, accuracy : 70.45\n",
      "iteration : 100, loss : 1.0292, accuracy : 68.63\n",
      "iteration : 150, loss : 1.0400, accuracy : 68.93\n",
      "iteration : 200, loss : 1.0509, accuracy : 68.52\n",
      "iteration : 250, loss : 1.0571, accuracy : 68.59\n",
      "iteration : 300, loss : 1.0610, accuracy : 68.76\n",
      "iteration : 350, loss : 1.0568, accuracy : 69.21\n",
      "Epoch :  26, training loss : 1.0622, training accuracy : 68.99, test loss : 0.4435, test accuracy : 88.35\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.9777, accuracy : 73.78\n",
      "iteration : 100, loss : 0.9995, accuracy : 71.79\n",
      "iteration : 150, loss : 1.0034, accuracy : 70.27\n",
      "iteration : 200, loss : 1.0276, accuracy : 68.76\n",
      "iteration : 250, loss : 1.0400, accuracy : 67.57\n",
      "iteration : 300, loss : 1.0400, accuracy : 67.47\n",
      "iteration : 350, loss : 1.0408, accuracy : 67.79\n",
      "Epoch :  27, training loss : 1.0464, training accuracy : 68.01, test loss : 0.4815, test accuracy : 86.61\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 1.0407, accuracy : 65.23\n",
      "iteration : 100, loss : 1.0162, accuracy : 70.38\n",
      "iteration : 150, loss : 1.0284, accuracy : 69.21\n",
      "iteration : 200, loss : 1.0303, accuracy : 68.57\n",
      "iteration : 250, loss : 1.0139, accuracy : 69.77\n",
      "iteration : 300, loss : 1.0248, accuracy : 70.15\n",
      "iteration : 350, loss : 1.0272, accuracy : 69.77\n",
      "Epoch :  28, training loss : 1.0279, training accuracy : 70.16, test loss : 0.4481, test accuracy : 87.10\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 1.0697, accuracy : 67.97\n",
      "iteration : 100, loss : 1.0359, accuracy : 69.39\n",
      "iteration : 150, loss : 1.0348, accuracy : 69.41\n",
      "iteration : 200, loss : 1.0269, accuracy : 70.05\n",
      "iteration : 250, loss : 1.0273, accuracy : 70.19\n",
      "iteration : 300, loss : 1.0334, accuracy : 69.38\n",
      "iteration : 350, loss : 1.0335, accuracy : 69.27\n",
      "Epoch :  29, training loss : 1.0292, training accuracy : 69.34, test loss : 0.3908, test accuracy : 89.56\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 1.0644, accuracy : 69.98\n",
      "iteration : 100, loss : 1.0556, accuracy : 70.48\n",
      "iteration : 150, loss : 1.0506, accuracy : 69.06\n",
      "iteration : 200, loss : 1.0522, accuracy : 69.64\n",
      "iteration : 250, loss : 1.0515, accuracy : 68.73\n",
      "iteration : 300, loss : 1.0475, accuracy : 69.14\n",
      "iteration : 350, loss : 1.0435, accuracy : 69.41\n",
      "Epoch :  30, training loss : 1.0398, training accuracy : 69.48, test loss : 0.3843, test accuracy : 89.37\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.9999, accuracy : 69.64\n",
      "iteration : 100, loss : 0.9823, accuracy : 71.80\n",
      "iteration : 150, loss : 0.9984, accuracy : 71.26\n",
      "iteration : 200, loss : 1.0018, accuracy : 71.55\n",
      "iteration : 250, loss : 1.0056, accuracy : 71.60\n",
      "iteration : 300, loss : 1.0022, accuracy : 71.93\n",
      "iteration : 350, loss : 1.0175, accuracy : 71.22\n",
      "Epoch :  31, training loss : 1.0228, training accuracy : 70.99, test loss : 0.4560, test accuracy : 87.14\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 1.0575, accuracy : 66.36\n",
      "iteration : 100, loss : 1.0524, accuracy : 66.53\n",
      "iteration : 150, loss : 1.0462, accuracy : 67.92\n",
      "iteration : 200, loss : 1.0337, accuracy : 68.44\n",
      "iteration : 250, loss : 1.0251, accuracy : 69.37\n",
      "iteration : 300, loss : 1.0172, accuracy : 70.09\n",
      "iteration : 350, loss : 1.0110, accuracy : 70.25\n",
      "Epoch :  32, training loss : 1.0137, training accuracy : 70.81, test loss : 0.3886, test accuracy : 90.00\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 1.0509, accuracy : 74.70\n",
      "iteration : 100, loss : 1.0381, accuracy : 72.09\n",
      "iteration : 150, loss : 1.0089, accuracy : 71.81\n",
      "iteration : 200, loss : 1.0046, accuracy : 71.41\n",
      "iteration : 250, loss : 1.0104, accuracy : 71.11\n",
      "iteration : 300, loss : 1.0178, accuracy : 71.05\n",
      "iteration : 350, loss : 1.0116, accuracy : 70.89\n",
      "Epoch :  33, training loss : 1.0158, training accuracy : 70.54, test loss : 0.3864, test accuracy : 90.08\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.9887, accuracy : 70.91\n",
      "iteration : 100, loss : 0.9700, accuracy : 74.59\n",
      "iteration : 150, loss : 0.9825, accuracy : 72.28\n",
      "iteration : 200, loss : 0.9871, accuracy : 71.72\n",
      "iteration : 250, loss : 0.9900, accuracy : 72.30\n",
      "iteration : 300, loss : 0.9974, accuracy : 72.01\n",
      "iteration : 350, loss : 0.9942, accuracy : 71.94\n",
      "Epoch :  34, training loss : 0.9865, training accuracy : 71.95, test loss : 0.3474, test accuracy : 90.57\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 1.0214, accuracy : 65.47\n",
      "iteration : 100, loss : 1.0143, accuracy : 68.78\n",
      "iteration : 150, loss : 1.0016, accuracy : 70.12\n",
      "iteration : 200, loss : 0.9954, accuracy : 71.21\n",
      "iteration : 250, loss : 1.0055, accuracy : 70.86\n",
      "iteration : 300, loss : 0.9961, accuracy : 71.95\n",
      "iteration : 350, loss : 0.9942, accuracy : 71.54\n",
      "Epoch :  35, training loss : 0.9960, training accuracy : 71.67, test loss : 0.4628, test accuracy : 86.91\n",
      "\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 1.0285, accuracy : 69.81\n",
      "iteration : 100, loss : 1.0470, accuracy : 68.48\n",
      "iteration : 150, loss : 1.0134, accuracy : 70.08\n",
      "iteration : 200, loss : 0.9994, accuracy : 69.37\n",
      "iteration : 250, loss : 1.0071, accuracy : 69.42\n",
      "iteration : 300, loss : 1.0028, accuracy : 69.80\n",
      "iteration : 350, loss : 1.0036, accuracy : 69.56\n",
      "Epoch :  36, training loss : 0.9947, training accuracy : 70.30, test loss : 0.3611, test accuracy : 89.17\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.9558, accuracy : 73.09\n",
      "iteration : 100, loss : 1.0026, accuracy : 71.66\n",
      "iteration : 150, loss : 1.0027, accuracy : 69.43\n",
      "iteration : 200, loss : 1.0012, accuracy : 70.84\n",
      "iteration : 250, loss : 1.0034, accuracy : 70.73\n",
      "iteration : 300, loss : 1.0043, accuracy : 70.92\n",
      "iteration : 350, loss : 1.0059, accuracy : 70.95\n",
      "Epoch :  37, training loss : 1.0055, training accuracy : 70.67, test loss : 0.3525, test accuracy : 90.59\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 1.0023, accuracy : 69.97\n",
      "iteration : 100, loss : 0.9806, accuracy : 70.61\n",
      "iteration : 150, loss : 0.9755, accuracy : 71.41\n",
      "iteration : 200, loss : 0.9700, accuracy : 72.66\n",
      "iteration : 250, loss : 0.9641, accuracy : 72.88\n",
      "iteration : 300, loss : 0.9756, accuracy : 71.55\n",
      "iteration : 350, loss : 0.9749, accuracy : 71.87\n",
      "Epoch :  38, training loss : 0.9734, training accuracy : 71.35, test loss : 0.3498, test accuracy : 90.39\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.9997, accuracy : 70.17\n",
      "iteration : 100, loss : 1.0113, accuracy : 69.12\n",
      "iteration : 150, loss : 1.0031, accuracy : 67.91\n",
      "iteration : 200, loss : 0.9842, accuracy : 70.39\n",
      "iteration : 250, loss : 0.9786, accuracy : 71.23\n",
      "iteration : 300, loss : 0.9830, accuracy : 70.72\n",
      "iteration : 350, loss : 0.9823, accuracy : 71.30\n",
      "Epoch :  39, training loss : 0.9789, training accuracy : 71.50, test loss : 0.3970, test accuracy : 88.76\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.9660, accuracy : 73.53\n",
      "iteration : 100, loss : 0.9969, accuracy : 73.09\n",
      "iteration : 150, loss : 0.9783, accuracy : 72.73\n",
      "iteration : 200, loss : 0.9700, accuracy : 72.17\n",
      "iteration : 250, loss : 0.9696, accuracy : 71.83\n",
      "iteration : 300, loss : 0.9706, accuracy : 72.03\n",
      "iteration : 350, loss : 0.9695, accuracy : 72.92\n",
      "Epoch :  40, training loss : 0.9691, training accuracy : 73.10, test loss : 0.3780, test accuracy : 89.37\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.8836, accuracy : 80.34\n",
      "iteration : 100, loss : 0.9260, accuracy : 75.05\n",
      "iteration : 150, loss : 0.9626, accuracy : 71.16\n",
      "iteration : 200, loss : 0.9648, accuracy : 70.36\n",
      "iteration : 250, loss : 0.9718, accuracy : 70.32\n",
      "iteration : 300, loss : 0.9687, accuracy : 71.00\n",
      "iteration : 350, loss : 0.9780, accuracy : 70.42\n",
      "Epoch :  41, training loss : 0.9813, training accuracy : 70.41, test loss : 0.3729, test accuracy : 89.85\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.8990, accuracy : 72.73\n",
      "iteration : 100, loss : 0.9043, accuracy : 71.72\n",
      "iteration : 150, loss : 0.9475, accuracy : 70.35\n",
      "iteration : 200, loss : 0.9580, accuracy : 69.69\n",
      "iteration : 250, loss : 0.9631, accuracy : 69.30\n",
      "iteration : 300, loss : 0.9487, accuracy : 70.81\n",
      "iteration : 350, loss : 0.9442, accuracy : 71.76\n",
      "Epoch :  42, training loss : 0.9451, training accuracy : 71.81, test loss : 0.3613, test accuracy : 90.63\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.9515, accuracy : 68.73\n",
      "iteration : 100, loss : 0.9571, accuracy : 69.78\n",
      "iteration : 150, loss : 0.9509, accuracy : 71.06\n",
      "iteration : 200, loss : 0.9457, accuracy : 71.91\n",
      "iteration : 250, loss : 0.9568, accuracy : 71.47\n",
      "iteration : 300, loss : 0.9524, accuracy : 71.91\n",
      "iteration : 350, loss : 0.9538, accuracy : 71.71\n",
      "Epoch :  43, training loss : 0.9423, training accuracy : 72.52, test loss : 0.3335, test accuracy : 91.11\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.9383, accuracy : 71.97\n",
      "iteration : 100, loss : 0.9224, accuracy : 69.68\n",
      "iteration : 150, loss : 0.9284, accuracy : 70.03\n",
      "iteration : 200, loss : 0.9442, accuracy : 70.00\n",
      "iteration : 250, loss : 0.9334, accuracy : 71.25\n",
      "iteration : 300, loss : 0.9323, accuracy : 71.57\n",
      "iteration : 350, loss : 0.9259, accuracy : 72.70\n",
      "Epoch :  44, training loss : 0.9235, training accuracy : 72.80, test loss : 0.3551, test accuracy : 90.30\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.9464, accuracy : 69.11\n",
      "iteration : 100, loss : 0.9515, accuracy : 72.24\n",
      "iteration : 150, loss : 0.9427, accuracy : 71.59\n",
      "iteration : 200, loss : 0.9476, accuracy : 71.81\n",
      "iteration : 250, loss : 0.9488, accuracy : 72.23\n",
      "iteration : 300, loss : 0.9298, accuracy : 72.99\n",
      "iteration : 350, loss : 0.9234, accuracy : 73.08\n",
      "Epoch :  45, training loss : 0.9342, training accuracy : 73.03, test loss : 0.3664, test accuracy : 89.84\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.9637, accuracy : 68.72\n",
      "iteration : 100, loss : 0.9492, accuracy : 69.89\n",
      "iteration : 150, loss : 0.9501, accuracy : 73.15\n",
      "iteration : 200, loss : 0.9350, accuracy : 73.06\n",
      "iteration : 250, loss : 0.9397, accuracy : 72.13\n",
      "iteration : 300, loss : 0.9472, accuracy : 71.25\n",
      "iteration : 350, loss : 0.9575, accuracy : 70.85\n",
      "Epoch :  46, training loss : 0.9589, training accuracy : 70.50, test loss : 0.3308, test accuracy : 91.31\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.9346, accuracy : 69.36\n",
      "iteration : 100, loss : 0.9542, accuracy : 71.21\n",
      "iteration : 150, loss : 0.9333, accuracy : 72.15\n",
      "iteration : 200, loss : 0.9245, accuracy : 72.82\n",
      "iteration : 250, loss : 0.9359, accuracy : 71.16\n",
      "iteration : 300, loss : 0.9323, accuracy : 71.49\n",
      "iteration : 350, loss : 0.9430, accuracy : 70.80\n",
      "Epoch :  47, training loss : 0.9439, training accuracy : 71.15, test loss : 0.3435, test accuracy : 90.85\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.8961, accuracy : 71.58\n",
      "iteration : 100, loss : 0.8760, accuracy : 74.78\n",
      "iteration : 150, loss : 0.9067, accuracy : 75.25\n",
      "iteration : 200, loss : 0.9103, accuracy : 75.29\n",
      "iteration : 250, loss : 0.9116, accuracy : 75.11\n",
      "iteration : 300, loss : 0.9171, accuracy : 74.71\n",
      "iteration : 350, loss : 0.9157, accuracy : 75.04\n",
      "Epoch :  48, training loss : 0.9151, training accuracy : 75.09, test loss : 0.3213, test accuracy : 91.10\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.9455, accuracy : 67.34\n",
      "iteration : 100, loss : 0.9545, accuracy : 69.98\n",
      "iteration : 150, loss : 0.9323, accuracy : 71.10\n",
      "iteration : 200, loss : 0.9176, accuracy : 73.07\n",
      "iteration : 250, loss : 0.9129, accuracy : 73.18\n",
      "iteration : 300, loss : 0.9286, accuracy : 71.96\n",
      "iteration : 350, loss : 0.9230, accuracy : 73.48\n",
      "Epoch :  49, training loss : 0.9260, training accuracy : 72.64, test loss : 0.3561, test accuracy : 90.35\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.8738, accuracy : 77.58\n",
      "iteration : 100, loss : 0.9209, accuracy : 73.05\n",
      "iteration : 150, loss : 0.9172, accuracy : 74.58\n",
      "iteration : 200, loss : 0.9169, accuracy : 73.53\n",
      "iteration : 250, loss : 0.9170, accuracy : 73.41\n",
      "iteration : 300, loss : 0.9237, accuracy : 72.71\n",
      "iteration : 350, loss : 0.9095, accuracy : 73.47\n",
      "Epoch :  50, training loss : 0.9049, training accuracy : 74.10, test loss : 0.3497, test accuracy : 90.16\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.9199, accuracy : 66.53\n",
      "iteration : 100, loss : 0.9222, accuracy : 68.29\n",
      "iteration : 150, loss : 0.9390, accuracy : 68.35\n",
      "iteration : 200, loss : 0.9538, accuracy : 69.81\n",
      "iteration : 250, loss : 0.9285, accuracy : 71.41\n",
      "iteration : 300, loss : 0.9283, accuracy : 72.07\n",
      "iteration : 350, loss : 0.9230, accuracy : 71.94\n",
      "Epoch :  51, training loss : 0.9195, training accuracy : 72.21, test loss : 0.3129, test accuracy : 91.69\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.8722, accuracy : 78.78\n",
      "iteration : 100, loss : 0.8672, accuracy : 81.53\n",
      "iteration : 150, loss : 0.8526, accuracy : 81.84\n",
      "iteration : 200, loss : 0.8850, accuracy : 78.20\n",
      "iteration : 250, loss : 0.8942, accuracy : 77.04\n",
      "iteration : 300, loss : 0.8967, accuracy : 75.76\n",
      "iteration : 350, loss : 0.9018, accuracy : 75.29\n",
      "Epoch :  52, training loss : 0.8987, training accuracy : 75.51, test loss : 0.3290, test accuracy : 90.57\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.8791, accuracy : 77.14\n",
      "iteration : 100, loss : 0.9139, accuracy : 72.45\n",
      "iteration : 150, loss : 0.9059, accuracy : 73.93\n",
      "iteration : 200, loss : 0.9085, accuracy : 74.30\n",
      "iteration : 250, loss : 0.9070, accuracy : 73.57\n",
      "iteration : 300, loss : 0.9160, accuracy : 73.79\n",
      "iteration : 350, loss : 0.9090, accuracy : 74.62\n",
      "Epoch :  53, training loss : 0.9094, training accuracy : 74.63, test loss : 0.3598, test accuracy : 89.89\n",
      "\n",
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.8977, accuracy : 72.78\n",
      "iteration : 100, loss : 0.9014, accuracy : 71.77\n",
      "iteration : 150, loss : 0.8908, accuracy : 73.33\n",
      "iteration : 200, loss : 0.9026, accuracy : 72.97\n",
      "iteration : 250, loss : 0.9092, accuracy : 73.63\n",
      "iteration : 300, loss : 0.9067, accuracy : 73.62\n",
      "iteration : 350, loss : 0.9051, accuracy : 73.65\n",
      "Epoch :  54, training loss : 0.9134, training accuracy : 73.45, test loss : 0.3279, test accuracy : 91.00\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.9114, accuracy : 68.83\n",
      "iteration : 100, loss : 0.8857, accuracy : 71.18\n",
      "iteration : 150, loss : 0.8847, accuracy : 71.68\n",
      "iteration : 200, loss : 0.8926, accuracy : 72.95\n",
      "iteration : 250, loss : 0.8834, accuracy : 74.39\n",
      "iteration : 300, loss : 0.8824, accuracy : 74.17\n",
      "iteration : 350, loss : 0.8862, accuracy : 73.50\n",
      "Epoch :  55, training loss : 0.8856, training accuracy : 73.95, test loss : 0.2996, test accuracy : 92.30\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.8647, accuracy : 70.97\n",
      "iteration : 100, loss : 0.8913, accuracy : 73.58\n",
      "iteration : 150, loss : 0.8887, accuracy : 75.21\n",
      "iteration : 200, loss : 0.8848, accuracy : 75.84\n",
      "iteration : 250, loss : 0.8900, accuracy : 76.12\n",
      "iteration : 300, loss : 0.8955, accuracy : 75.77\n",
      "iteration : 350, loss : 0.9001, accuracy : 76.00\n",
      "Epoch :  56, training loss : 0.8976, training accuracy : 76.00, test loss : 0.2858, test accuracy : 91.87\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.8621, accuracy : 73.22\n",
      "iteration : 100, loss : 0.8609, accuracy : 73.19\n",
      "iteration : 150, loss : 0.8614, accuracy : 74.85\n",
      "iteration : 200, loss : 0.8830, accuracy : 73.53\n",
      "iteration : 250, loss : 0.8868, accuracy : 73.14\n",
      "iteration : 300, loss : 0.8913, accuracy : 72.60\n",
      "iteration : 350, loss : 0.8824, accuracy : 74.17\n",
      "Epoch :  57, training loss : 0.8823, training accuracy : 74.49, test loss : 0.3033, test accuracy : 91.91\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.8792, accuracy : 76.11\n",
      "iteration : 100, loss : 0.8862, accuracy : 74.93\n",
      "iteration : 150, loss : 0.8918, accuracy : 75.24\n",
      "iteration : 200, loss : 0.8996, accuracy : 74.72\n",
      "iteration : 250, loss : 0.8920, accuracy : 75.68\n",
      "iteration : 300, loss : 0.8871, accuracy : 75.24\n",
      "iteration : 350, loss : 0.8908, accuracy : 74.22\n",
      "Epoch :  58, training loss : 0.8965, training accuracy : 74.38, test loss : 0.3206, test accuracy : 90.72\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.9097, accuracy : 69.11\n",
      "iteration : 100, loss : 0.8591, accuracy : 73.55\n",
      "iteration : 150, loss : 0.8822, accuracy : 73.55\n",
      "iteration : 200, loss : 0.8803, accuracy : 74.23\n",
      "iteration : 250, loss : 0.8806, accuracy : 74.72\n",
      "iteration : 300, loss : 0.8772, accuracy : 74.96\n",
      "iteration : 350, loss : 0.8714, accuracy : 75.66\n",
      "Epoch :  59, training loss : 0.8775, training accuracy : 75.15, test loss : 0.3314, test accuracy : 91.23\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.8670, accuracy : 71.05\n",
      "iteration : 100, loss : 0.8523, accuracy : 74.34\n",
      "iteration : 150, loss : 0.8536, accuracy : 75.43\n",
      "iteration : 200, loss : 0.8815, accuracy : 74.53\n",
      "iteration : 250, loss : 0.8837, accuracy : 74.54\n",
      "iteration : 300, loss : 0.8852, accuracy : 74.70\n",
      "iteration : 350, loss : 0.8870, accuracy : 74.98\n",
      "Epoch :  60, training loss : 0.8838, training accuracy : 74.95, test loss : 0.2938, test accuracy : 92.56\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.8892, accuracy : 68.33\n",
      "iteration : 100, loss : 0.8967, accuracy : 71.63\n",
      "iteration : 150, loss : 0.8746, accuracy : 74.24\n",
      "iteration : 200, loss : 0.8796, accuracy : 75.26\n",
      "iteration : 250, loss : 0.8844, accuracy : 74.35\n",
      "iteration : 300, loss : 0.8765, accuracy : 74.41\n",
      "iteration : 350, loss : 0.8771, accuracy : 73.40\n",
      "Epoch :  61, training loss : 0.8766, training accuracy : 73.84, test loss : 0.2832, test accuracy : 92.76\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.8546, accuracy : 78.48\n",
      "iteration : 100, loss : 0.8591, accuracy : 77.61\n",
      "iteration : 150, loss : 0.8542, accuracy : 77.61\n",
      "iteration : 200, loss : 0.8481, accuracy : 76.72\n",
      "iteration : 250, loss : 0.8623, accuracy : 76.68\n",
      "iteration : 300, loss : 0.8703, accuracy : 75.74\n",
      "iteration : 350, loss : 0.8742, accuracy : 74.97\n",
      "Epoch :  62, training loss : 0.8744, training accuracy : 74.63, test loss : 0.2733, test accuracy : 92.86\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.8640, accuracy : 78.67\n",
      "iteration : 100, loss : 0.8606, accuracy : 78.58\n",
      "iteration : 150, loss : 0.8593, accuracy : 78.21\n",
      "iteration : 200, loss : 0.8686, accuracy : 76.79\n",
      "iteration : 250, loss : 0.8722, accuracy : 75.64\n",
      "iteration : 300, loss : 0.8721, accuracy : 76.11\n",
      "iteration : 350, loss : 0.8750, accuracy : 76.03\n",
      "Epoch :  63, training loss : 0.8779, training accuracy : 75.41, test loss : 0.2857, test accuracy : 92.68\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.8833, accuracy : 76.14\n",
      "iteration : 100, loss : 0.9231, accuracy : 73.20\n",
      "iteration : 150, loss : 0.8950, accuracy : 74.90\n",
      "iteration : 200, loss : 0.8897, accuracy : 74.72\n",
      "iteration : 250, loss : 0.8842, accuracy : 75.41\n",
      "iteration : 300, loss : 0.8897, accuracy : 74.86\n",
      "iteration : 350, loss : 0.8887, accuracy : 75.23\n",
      "Epoch :  64, training loss : 0.8893, training accuracy : 75.30, test loss : 0.3015, test accuracy : 92.38\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.8341, accuracy : 76.00\n",
      "iteration : 100, loss : 0.8654, accuracy : 73.53\n",
      "iteration : 150, loss : 0.8429, accuracy : 76.26\n",
      "iteration : 200, loss : 0.8550, accuracy : 74.95\n",
      "iteration : 250, loss : 0.8601, accuracy : 75.47\n",
      "iteration : 300, loss : 0.8745, accuracy : 75.21\n",
      "iteration : 350, loss : 0.8774, accuracy : 75.58\n",
      "Epoch :  65, training loss : 0.8692, training accuracy : 76.46, test loss : 0.2820, test accuracy : 92.28\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.8451, accuracy : 78.20\n",
      "iteration : 100, loss : 0.8453, accuracy : 77.25\n",
      "iteration : 150, loss : 0.8596, accuracy : 76.87\n",
      "iteration : 200, loss : 0.8693, accuracy : 75.68\n",
      "iteration : 250, loss : 0.8582, accuracy : 75.84\n",
      "iteration : 300, loss : 0.8563, accuracy : 75.59\n",
      "iteration : 350, loss : 0.8656, accuracy : 75.31\n",
      "Epoch :  66, training loss : 0.8654, training accuracy : 75.14, test loss : 0.2812, test accuracy : 92.69\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.8898, accuracy : 72.11\n",
      "iteration : 100, loss : 0.8466, accuracy : 73.56\n",
      "iteration : 150, loss : 0.8256, accuracy : 75.19\n",
      "iteration : 200, loss : 0.8548, accuracy : 73.42\n",
      "iteration : 250, loss : 0.8555, accuracy : 73.45\n",
      "iteration : 300, loss : 0.8491, accuracy : 74.51\n",
      "iteration : 350, loss : 0.8553, accuracy : 73.96\n",
      "Epoch :  67, training loss : 0.8558, training accuracy : 74.09, test loss : 0.2857, test accuracy : 92.46\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.8526, accuracy : 77.00\n",
      "iteration : 100, loss : 0.8718, accuracy : 73.59\n",
      "iteration : 150, loss : 0.8582, accuracy : 75.12\n",
      "iteration : 200, loss : 0.8469, accuracy : 74.50\n",
      "iteration : 250, loss : 0.8594, accuracy : 74.54\n",
      "iteration : 300, loss : 0.8658, accuracy : 74.25\n",
      "iteration : 350, loss : 0.8588, accuracy : 74.58\n",
      "Epoch :  68, training loss : 0.8580, training accuracy : 74.22, test loss : 0.2519, test accuracy : 92.89\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.8542, accuracy : 79.64\n",
      "iteration : 100, loss : 0.8453, accuracy : 76.66\n",
      "iteration : 150, loss : 0.8401, accuracy : 76.82\n",
      "iteration : 200, loss : 0.8380, accuracy : 77.52\n",
      "iteration : 250, loss : 0.8436, accuracy : 77.56\n",
      "iteration : 300, loss : 0.8470, accuracy : 76.58\n",
      "iteration : 350, loss : 0.8473, accuracy : 76.93\n",
      "Epoch :  69, training loss : 0.8520, training accuracy : 76.38, test loss : 0.2526, test accuracy : 93.45\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.8447, accuracy : 77.45\n",
      "iteration : 100, loss : 0.8670, accuracy : 74.30\n",
      "iteration : 150, loss : 0.8778, accuracy : 72.51\n",
      "iteration : 200, loss : 0.8742, accuracy : 72.20\n",
      "iteration : 250, loss : 0.8730, accuracy : 73.06\n",
      "iteration : 300, loss : 0.8606, accuracy : 73.26\n",
      "iteration : 350, loss : 0.8588, accuracy : 73.59\n",
      "Epoch :  70, training loss : 0.8597, training accuracy : 73.45, test loss : 0.2539, test accuracy : 92.85\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.8580, accuracy : 73.69\n",
      "iteration : 100, loss : 0.8326, accuracy : 76.61\n",
      "iteration : 150, loss : 0.8319, accuracy : 75.51\n",
      "iteration : 200, loss : 0.8289, accuracy : 75.01\n",
      "iteration : 250, loss : 0.8358, accuracy : 75.04\n",
      "iteration : 300, loss : 0.8447, accuracy : 74.47\n",
      "iteration : 350, loss : 0.8456, accuracy : 75.10\n",
      "Epoch :  71, training loss : 0.8489, training accuracy : 74.47, test loss : 0.3077, test accuracy : 91.57\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.7172, accuracy : 87.61\n",
      "iteration : 100, loss : 0.7921, accuracy : 80.58\n",
      "iteration : 150, loss : 0.8235, accuracy : 78.48\n",
      "iteration : 200, loss : 0.8372, accuracy : 77.00\n",
      "iteration : 250, loss : 0.8389, accuracy : 77.78\n",
      "iteration : 300, loss : 0.8370, accuracy : 77.80\n",
      "iteration : 350, loss : 0.8360, accuracy : 77.68\n",
      "Epoch :  72, training loss : 0.8391, training accuracy : 77.33, test loss : 0.2535, test accuracy : 92.74\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.8270, accuracy : 79.22\n",
      "iteration : 100, loss : 0.8684, accuracy : 75.90\n",
      "iteration : 150, loss : 0.8688, accuracy : 74.33\n",
      "iteration : 200, loss : 0.8761, accuracy : 74.15\n",
      "iteration : 250, loss : 0.8797, accuracy : 74.32\n",
      "iteration : 300, loss : 0.8789, accuracy : 73.98\n",
      "iteration : 350, loss : 0.8760, accuracy : 74.07\n",
      "Epoch :  73, training loss : 0.8666, training accuracy : 74.77, test loss : 0.2465, test accuracy : 93.02\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.8408, accuracy : 76.92\n",
      "iteration : 100, loss : 0.8508, accuracy : 77.95\n",
      "iteration : 150, loss : 0.8344, accuracy : 77.78\n",
      "iteration : 200, loss : 0.8514, accuracy : 76.81\n",
      "iteration : 250, loss : 0.8554, accuracy : 75.33\n",
      "iteration : 300, loss : 0.8513, accuracy : 74.83\n",
      "iteration : 350, loss : 0.8626, accuracy : 74.11\n",
      "Epoch :  74, training loss : 0.8543, training accuracy : 74.97, test loss : 0.2618, test accuracy : 92.78\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.7834, accuracy : 80.28\n",
      "iteration : 100, loss : 0.8161, accuracy : 78.56\n",
      "iteration : 150, loss : 0.8211, accuracy : 78.14\n",
      "iteration : 200, loss : 0.8224, accuracy : 77.61\n",
      "iteration : 250, loss : 0.8230, accuracy : 77.77\n",
      "iteration : 300, loss : 0.8220, accuracy : 77.38\n",
      "iteration : 350, loss : 0.8308, accuracy : 77.06\n",
      "Epoch :  75, training loss : 0.8261, training accuracy : 77.39, test loss : 0.2375, test accuracy : 93.50\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.8569, accuracy : 76.80\n",
      "iteration : 100, loss : 0.8652, accuracy : 75.85\n",
      "iteration : 150, loss : 0.8601, accuracy : 75.16\n",
      "iteration : 200, loss : 0.8465, accuracy : 76.68\n",
      "iteration : 250, loss : 0.8513, accuracy : 75.88\n",
      "iteration : 300, loss : 0.8446, accuracy : 76.75\n",
      "iteration : 350, loss : 0.8421, accuracy : 76.82\n",
      "Epoch :  76, training loss : 0.8427, training accuracy : 76.94, test loss : 0.2526, test accuracy : 93.32\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.8397, accuracy : 78.09\n",
      "iteration : 100, loss : 0.7955, accuracy : 79.07\n",
      "iteration : 150, loss : 0.8061, accuracy : 79.26\n",
      "iteration : 200, loss : 0.8253, accuracy : 77.13\n",
      "iteration : 250, loss : 0.8159, accuracy : 77.98\n",
      "iteration : 300, loss : 0.8202, accuracy : 77.31\n",
      "iteration : 350, loss : 0.8248, accuracy : 77.18\n",
      "Epoch :  77, training loss : 0.8286, training accuracy : 76.74, test loss : 0.2424, test accuracy : 93.38\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.8107, accuracy : 76.14\n",
      "iteration : 100, loss : 0.8263, accuracy : 75.61\n",
      "iteration : 150, loss : 0.8202, accuracy : 77.27\n",
      "iteration : 200, loss : 0.8279, accuracy : 77.13\n",
      "iteration : 250, loss : 0.8299, accuracy : 77.38\n",
      "iteration : 300, loss : 0.8231, accuracy : 77.43\n",
      "iteration : 350, loss : 0.8235, accuracy : 78.03\n",
      "Epoch :  78, training loss : 0.8169, training accuracy : 78.50, test loss : 0.2552, test accuracy : 92.85\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.8707, accuracy : 77.23\n",
      "iteration : 100, loss : 0.8618, accuracy : 76.19\n",
      "iteration : 150, loss : 0.8527, accuracy : 73.77\n",
      "iteration : 200, loss : 0.8621, accuracy : 73.55\n",
      "iteration : 250, loss : 0.8521, accuracy : 73.69\n",
      "iteration : 300, loss : 0.8326, accuracy : 75.41\n",
      "iteration : 350, loss : 0.8282, accuracy : 75.50\n",
      "Epoch :  79, training loss : 0.8261, training accuracy : 75.44, test loss : 0.2609, test accuracy : 92.98\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.8078, accuracy : 76.55\n",
      "iteration : 100, loss : 0.7963, accuracy : 75.48\n",
      "iteration : 150, loss : 0.8237, accuracy : 75.12\n",
      "iteration : 200, loss : 0.8171, accuracy : 74.66\n",
      "iteration : 250, loss : 0.8202, accuracy : 73.97\n",
      "iteration : 300, loss : 0.8239, accuracy : 74.45\n",
      "iteration : 350, loss : 0.8192, accuracy : 74.64\n",
      "Epoch :  80, training loss : 0.8280, training accuracy : 73.70, test loss : 0.2411, test accuracy : 93.66\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.8224, accuracy : 79.44\n",
      "iteration : 100, loss : 0.8329, accuracy : 78.05\n",
      "iteration : 150, loss : 0.8293, accuracy : 78.71\n",
      "iteration : 200, loss : 0.8265, accuracy : 77.43\n",
      "iteration : 250, loss : 0.8395, accuracy : 75.11\n",
      "iteration : 300, loss : 0.8419, accuracy : 75.28\n",
      "iteration : 350, loss : 0.8314, accuracy : 75.98\n",
      "Epoch :  81, training loss : 0.8341, training accuracy : 75.87, test loss : 0.2672, test accuracy : 92.56\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.8414, accuracy : 75.61\n",
      "iteration : 100, loss : 0.8060, accuracy : 79.25\n",
      "iteration : 150, loss : 0.8131, accuracy : 77.66\n",
      "iteration : 200, loss : 0.7974, accuracy : 78.22\n",
      "iteration : 250, loss : 0.7908, accuracy : 78.73\n",
      "iteration : 300, loss : 0.8001, accuracy : 77.56\n",
      "iteration : 350, loss : 0.8115, accuracy : 77.01\n",
      "Epoch :  82, training loss : 0.8029, training accuracy : 77.18, test loss : 0.2378, test accuracy : 93.41\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.8384, accuracy : 79.27\n",
      "iteration : 100, loss : 0.8101, accuracy : 77.63\n",
      "iteration : 150, loss : 0.8093, accuracy : 79.31\n",
      "iteration : 200, loss : 0.8104, accuracy : 77.96\n",
      "iteration : 250, loss : 0.8150, accuracy : 78.08\n",
      "iteration : 300, loss : 0.8188, accuracy : 78.75\n",
      "iteration : 350, loss : 0.8250, accuracy : 78.70\n",
      "Epoch :  83, training loss : 0.8143, training accuracy : 78.74, test loss : 0.2434, test accuracy : 93.16\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.7945, accuracy : 78.67\n",
      "iteration : 100, loss : 0.7996, accuracy : 81.29\n",
      "iteration : 150, loss : 0.7982, accuracy : 81.41\n",
      "iteration : 200, loss : 0.8049, accuracy : 80.10\n",
      "iteration : 250, loss : 0.8253, accuracy : 78.47\n",
      "iteration : 300, loss : 0.8140, accuracy : 78.85\n",
      "iteration : 350, loss : 0.8236, accuracy : 78.79\n",
      "Epoch :  84, training loss : 0.8310, training accuracy : 78.51, test loss : 0.2299, test accuracy : 93.65\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.7653, accuracy : 80.30\n",
      "iteration : 100, loss : 0.7600, accuracy : 80.04\n",
      "iteration : 150, loss : 0.7707, accuracy : 77.20\n",
      "iteration : 200, loss : 0.7666, accuracy : 79.43\n",
      "iteration : 250, loss : 0.7778, accuracy : 79.59\n",
      "iteration : 300, loss : 0.7840, accuracy : 78.87\n",
      "iteration : 350, loss : 0.7857, accuracy : 79.30\n",
      "Epoch :  85, training loss : 0.7908, training accuracy : 78.50, test loss : 0.2455, test accuracy : 93.01\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.8198, accuracy : 73.06\n",
      "iteration : 100, loss : 0.8295, accuracy : 75.63\n",
      "iteration : 150, loss : 0.8119, accuracy : 75.98\n",
      "iteration : 200, loss : 0.8164, accuracy : 74.83\n",
      "iteration : 250, loss : 0.8097, accuracy : 75.15\n",
      "iteration : 300, loss : 0.8080, accuracy : 75.44\n",
      "iteration : 350, loss : 0.8202, accuracy : 75.11\n",
      "Epoch :  86, training loss : 0.8154, training accuracy : 75.67, test loss : 0.2233, test accuracy : 93.78\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.7632, accuracy : 78.33\n",
      "iteration : 100, loss : 0.8219, accuracy : 76.39\n",
      "iteration : 150, loss : 0.8085, accuracy : 77.80\n",
      "iteration : 200, loss : 0.8138, accuracy : 76.68\n",
      "iteration : 250, loss : 0.8170, accuracy : 77.01\n",
      "iteration : 300, loss : 0.8126, accuracy : 77.09\n",
      "iteration : 350, loss : 0.8100, accuracy : 77.73\n",
      "Epoch :  87, training loss : 0.8077, training accuracy : 76.79, test loss : 0.2425, test accuracy : 93.11\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.8030, accuracy : 80.28\n",
      "iteration : 100, loss : 0.8100, accuracy : 78.95\n",
      "iteration : 150, loss : 0.8180, accuracy : 78.41\n",
      "iteration : 200, loss : 0.8124, accuracy : 79.03\n",
      "iteration : 250, loss : 0.8133, accuracy : 78.87\n",
      "iteration : 300, loss : 0.8154, accuracy : 78.97\n",
      "iteration : 350, loss : 0.8143, accuracy : 79.11\n",
      "Epoch :  88, training loss : 0.8142, training accuracy : 79.57, test loss : 0.2383, test accuracy : 93.13\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.7645, accuracy : 80.34\n",
      "iteration : 100, loss : 0.8003, accuracy : 76.70\n",
      "iteration : 150, loss : 0.8077, accuracy : 75.96\n",
      "iteration : 200, loss : 0.7990, accuracy : 75.38\n",
      "iteration : 250, loss : 0.7954, accuracy : 76.34\n",
      "iteration : 300, loss : 0.7846, accuracy : 76.93\n",
      "iteration : 350, loss : 0.7955, accuracy : 76.16\n",
      "Epoch :  89, training loss : 0.8045, training accuracy : 75.81, test loss : 0.2300, test accuracy : 93.39\n",
      "\n",
      "Epoch: 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.8058, accuracy : 70.69\n",
      "iteration : 100, loss : 0.7965, accuracy : 73.79\n",
      "iteration : 150, loss : 0.7959, accuracy : 72.71\n",
      "iteration : 200, loss : 0.8045, accuracy : 73.53\n",
      "iteration : 250, loss : 0.7991, accuracy : 74.22\n",
      "iteration : 300, loss : 0.7937, accuracy : 74.68\n",
      "iteration : 350, loss : 0.7873, accuracy : 75.42\n",
      "Epoch :  90, training loss : 0.7867, training accuracy : 76.03, test loss : 0.2248, test accuracy : 93.57\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.7679, accuracy : 81.62\n",
      "iteration : 100, loss : 0.8233, accuracy : 77.94\n",
      "iteration : 150, loss : 0.8223, accuracy : 77.69\n",
      "iteration : 200, loss : 0.8214, accuracy : 77.68\n",
      "iteration : 250, loss : 0.8276, accuracy : 77.74\n",
      "iteration : 300, loss : 0.8354, accuracy : 77.50\n",
      "iteration : 350, loss : 0.8275, accuracy : 77.10\n",
      "Epoch :  91, training loss : 0.8206, training accuracy : 77.08, test loss : 0.2295, test accuracy : 93.37\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.8193, accuracy : 76.55\n",
      "iteration : 100, loss : 0.7851, accuracy : 76.80\n",
      "iteration : 150, loss : 0.7950, accuracy : 77.26\n",
      "iteration : 200, loss : 0.7961, accuracy : 78.45\n",
      "iteration : 250, loss : 0.7976, accuracy : 78.54\n",
      "iteration : 300, loss : 0.7908, accuracy : 78.47\n",
      "iteration : 350, loss : 0.7901, accuracy : 78.33\n",
      "Epoch :  92, training loss : 0.7931, training accuracy : 78.64, test loss : 0.2305, test accuracy : 93.64\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.8355, accuracy : 79.62\n",
      "iteration : 100, loss : 0.8133, accuracy : 75.19\n",
      "iteration : 150, loss : 0.8275, accuracy : 74.66\n",
      "iteration : 200, loss : 0.8170, accuracy : 76.35\n",
      "iteration : 250, loss : 0.8193, accuracy : 76.59\n",
      "iteration : 300, loss : 0.8198, accuracy : 76.42\n",
      "iteration : 350, loss : 0.8190, accuracy : 77.16\n",
      "Epoch :  93, training loss : 0.8168, training accuracy : 76.88, test loss : 0.2636, test accuracy : 92.42\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.7739, accuracy : 81.98\n",
      "iteration : 100, loss : 0.7865, accuracy : 83.02\n",
      "iteration : 150, loss : 0.7977, accuracy : 81.54\n",
      "iteration : 200, loss : 0.8007, accuracy : 80.75\n",
      "iteration : 250, loss : 0.7933, accuracy : 81.01\n",
      "iteration : 300, loss : 0.8044, accuracy : 78.95\n",
      "iteration : 350, loss : 0.7977, accuracy : 79.12\n",
      "Epoch :  94, training loss : 0.7993, training accuracy : 78.24, test loss : 0.2228, test accuracy : 93.73\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.7809, accuracy : 81.36\n",
      "iteration : 100, loss : 0.7925, accuracy : 81.60\n",
      "iteration : 150, loss : 0.7908, accuracy : 82.12\n",
      "iteration : 200, loss : 0.7880, accuracy : 80.95\n",
      "iteration : 250, loss : 0.7752, accuracy : 80.32\n",
      "iteration : 300, loss : 0.7843, accuracy : 79.88\n",
      "iteration : 350, loss : 0.7979, accuracy : 77.91\n",
      "Epoch :  95, training loss : 0.7949, training accuracy : 77.54, test loss : 0.2063, test accuracy : 94.05\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.8079, accuracy : 78.73\n",
      "iteration : 100, loss : 0.8084, accuracy : 76.94\n",
      "iteration : 150, loss : 0.7932, accuracy : 78.79\n",
      "iteration : 200, loss : 0.7941, accuracy : 78.62\n",
      "iteration : 250, loss : 0.8010, accuracy : 78.52\n",
      "iteration : 300, loss : 0.8022, accuracy : 78.09\n",
      "iteration : 350, loss : 0.8060, accuracy : 77.87\n",
      "Epoch :  96, training loss : 0.8044, training accuracy : 78.23, test loss : 0.2182, test accuracy : 93.73\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.7996, accuracy : 81.48\n",
      "iteration : 100, loss : 0.7747, accuracy : 77.98\n",
      "iteration : 150, loss : 0.7687, accuracy : 77.62\n",
      "iteration : 200, loss : 0.7728, accuracy : 77.09\n",
      "iteration : 250, loss : 0.7758, accuracy : 77.65\n",
      "iteration : 300, loss : 0.7832, accuracy : 77.04\n",
      "iteration : 350, loss : 0.7853, accuracy : 76.58\n",
      "Epoch :  97, training loss : 0.7884, training accuracy : 76.68, test loss : 0.2031, test accuracy : 94.15\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.7865, accuracy : 79.91\n",
      "iteration : 100, loss : 0.7503, accuracy : 79.73\n",
      "iteration : 150, loss : 0.7624, accuracy : 79.19\n",
      "iteration : 200, loss : 0.7718, accuracy : 78.37\n",
      "iteration : 250, loss : 0.7719, accuracy : 77.72\n",
      "iteration : 300, loss : 0.7784, accuracy : 78.39\n",
      "iteration : 350, loss : 0.7797, accuracy : 78.76\n",
      "Epoch :  98, training loss : 0.7826, training accuracy : 78.80, test loss : 0.2008, test accuracy : 94.26\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.8391, accuracy : 72.12\n",
      "iteration : 100, loss : 0.8179, accuracy : 72.54\n",
      "iteration : 150, loss : 0.8070, accuracy : 71.48\n",
      "iteration : 200, loss : 0.7928, accuracy : 74.06\n",
      "iteration : 250, loss : 0.7901, accuracy : 74.26\n",
      "iteration : 300, loss : 0.7810, accuracy : 75.73\n",
      "iteration : 350, loss : 0.7828, accuracy : 75.66\n",
      "Epoch :  99, training loss : 0.7892, training accuracy : 75.93, test loss : 0.2186, test accuracy : 94.15\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.7619, accuracy : 76.77\n",
      "iteration : 100, loss : 0.7622, accuracy : 78.78\n",
      "iteration : 150, loss : 0.7884, accuracy : 76.51\n",
      "iteration : 200, loss : 0.7900, accuracy : 77.24\n",
      "iteration : 250, loss : 0.7992, accuracy : 76.87\n",
      "iteration : 300, loss : 0.8020, accuracy : 77.39\n",
      "iteration : 350, loss : 0.8045, accuracy : 76.97\n",
      "Epoch : 100, training loss : 0.8032, training accuracy : 77.68, test loss : 0.2241, test accuracy : 93.48\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.7572, accuracy : 78.64\n",
      "iteration : 100, loss : 0.7517, accuracy : 77.73\n",
      "iteration : 150, loss : 0.7538, accuracy : 78.72\n",
      "iteration : 200, loss : 0.7536, accuracy : 78.11\n",
      "iteration : 250, loss : 0.7659, accuracy : 78.19\n",
      "iteration : 300, loss : 0.7665, accuracy : 78.36\n",
      "iteration : 350, loss : 0.7734, accuracy : 77.90\n",
      "Epoch : 101, training loss : 0.7766, training accuracy : 77.99, test loss : 0.2268, test accuracy : 93.45\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.7772, accuracy : 79.42\n",
      "iteration : 100, loss : 0.7755, accuracy : 80.44\n",
      "iteration : 150, loss : 0.7837, accuracy : 80.54\n",
      "iteration : 200, loss : 0.7764, accuracy : 81.25\n",
      "iteration : 250, loss : 0.7761, accuracy : 81.05\n",
      "iteration : 300, loss : 0.7760, accuracy : 81.44\n",
      "iteration : 350, loss : 0.7863, accuracy : 80.21\n",
      "Epoch : 102, training loss : 0.7925, training accuracy : 80.70, test loss : 0.1978, test accuracy : 94.29\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.8156, accuracy : 75.12\n",
      "iteration : 100, loss : 0.8096, accuracy : 78.63\n",
      "iteration : 150, loss : 0.8100, accuracy : 79.53\n",
      "iteration : 200, loss : 0.7944, accuracy : 80.59\n",
      "iteration : 250, loss : 0.7874, accuracy : 81.01\n",
      "iteration : 300, loss : 0.7789, accuracy : 81.29\n",
      "iteration : 350, loss : 0.7786, accuracy : 80.53\n",
      "Epoch : 103, training loss : 0.7764, training accuracy : 80.15, test loss : 0.2138, test accuracy : 94.19\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.7568, accuracy : 83.33\n",
      "iteration : 100, loss : 0.7759, accuracy : 77.66\n",
      "iteration : 150, loss : 0.7762, accuracy : 79.75\n",
      "iteration : 200, loss : 0.7913, accuracy : 78.20\n",
      "iteration : 250, loss : 0.7894, accuracy : 77.27\n",
      "iteration : 300, loss : 0.7873, accuracy : 76.88\n",
      "iteration : 350, loss : 0.7946, accuracy : 76.73\n",
      "Epoch : 104, training loss : 0.8007, training accuracy : 76.40, test loss : 0.2238, test accuracy : 93.41\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.8050, accuracy : 72.19\n",
      "iteration : 100, loss : 0.7948, accuracy : 75.47\n",
      "iteration : 150, loss : 0.7968, accuracy : 74.40\n",
      "iteration : 200, loss : 0.7974, accuracy : 75.38\n",
      "iteration : 250, loss : 0.7826, accuracy : 76.57\n",
      "iteration : 300, loss : 0.7690, accuracy : 78.04\n",
      "iteration : 350, loss : 0.7773, accuracy : 77.30\n",
      "Epoch : 105, training loss : 0.7786, training accuracy : 77.48, test loss : 0.2113, test accuracy : 93.76\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.7728, accuracy : 81.31\n",
      "iteration : 100, loss : 0.7865, accuracy : 79.28\n",
      "iteration : 150, loss : 0.7908, accuracy : 79.31\n",
      "iteration : 200, loss : 0.7692, accuracy : 80.16\n",
      "iteration : 250, loss : 0.7731, accuracy : 79.16\n",
      "iteration : 300, loss : 0.7727, accuracy : 78.86\n",
      "iteration : 350, loss : 0.7738, accuracy : 79.00\n",
      "Epoch : 106, training loss : 0.7726, training accuracy : 79.18, test loss : 0.1908, test accuracy : 94.39\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.7407, accuracy : 84.62\n",
      "iteration : 100, loss : 0.7732, accuracy : 80.26\n",
      "iteration : 150, loss : 0.7558, accuracy : 81.16\n",
      "iteration : 200, loss : 0.7650, accuracy : 79.96\n",
      "iteration : 250, loss : 0.7727, accuracy : 79.22\n",
      "iteration : 300, loss : 0.7716, accuracy : 78.02\n",
      "iteration : 350, loss : 0.7664, accuracy : 77.68\n",
      "Epoch : 107, training loss : 0.7699, training accuracy : 77.65, test loss : 0.2356, test accuracy : 93.14\n",
      "\n",
      "Epoch: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.7461, accuracy : 81.44\n",
      "iteration : 100, loss : 0.7309, accuracy : 83.80\n",
      "iteration : 150, loss : 0.7382, accuracy : 81.79\n",
      "iteration : 200, loss : 0.7510, accuracy : 81.05\n",
      "iteration : 250, loss : 0.7495, accuracy : 81.35\n",
      "iteration : 300, loss : 0.7519, accuracy : 81.19\n",
      "iteration : 350, loss : 0.7493, accuracy : 80.87\n",
      "Epoch : 108, training loss : 0.7494, training accuracy : 80.40, test loss : 0.2251, test accuracy : 93.20\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.7713, accuracy : 80.84\n",
      "iteration : 100, loss : 0.7528, accuracy : 80.00\n",
      "iteration : 150, loss : 0.7719, accuracy : 78.83\n",
      "iteration : 200, loss : 0.7639, accuracy : 80.04\n",
      "iteration : 250, loss : 0.7629, accuracy : 80.39\n",
      "iteration : 300, loss : 0.7687, accuracy : 80.69\n",
      "iteration : 350, loss : 0.7670, accuracy : 80.04\n",
      "Epoch : 109, training loss : 0.7594, training accuracy : 79.90, test loss : 0.2016, test accuracy : 94.05\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.6998, accuracy : 80.81\n",
      "iteration : 100, loss : 0.7428, accuracy : 79.72\n",
      "iteration : 150, loss : 0.7376, accuracy : 79.49\n",
      "iteration : 200, loss : 0.7356, accuracy : 80.25\n",
      "iteration : 250, loss : 0.7452, accuracy : 80.59\n",
      "iteration : 300, loss : 0.7507, accuracy : 80.28\n",
      "iteration : 350, loss : 0.7569, accuracy : 79.24\n",
      "Epoch : 110, training loss : 0.7597, training accuracy : 79.18, test loss : 0.1989, test accuracy : 94.09\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.7309, accuracy : 78.83\n",
      "iteration : 100, loss : 0.7234, accuracy : 78.59\n",
      "iteration : 150, loss : 0.7362, accuracy : 78.10\n",
      "iteration : 200, loss : 0.7509, accuracy : 77.91\n",
      "iteration : 250, loss : 0.7514, accuracy : 78.09\n",
      "iteration : 300, loss : 0.7534, accuracy : 77.89\n",
      "iteration : 350, loss : 0.7543, accuracy : 77.80\n",
      "Epoch : 111, training loss : 0.7574, training accuracy : 77.69, test loss : 0.1954, test accuracy : 94.17\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.7382, accuracy : 77.02\n",
      "iteration : 100, loss : 0.7319, accuracy : 78.41\n",
      "iteration : 150, loss : 0.7564, accuracy : 76.95\n",
      "iteration : 200, loss : 0.7551, accuracy : 78.03\n",
      "iteration : 250, loss : 0.7540, accuracy : 77.77\n",
      "iteration : 300, loss : 0.7595, accuracy : 78.57\n",
      "iteration : 350, loss : 0.7605, accuracy : 78.56\n",
      "Epoch : 112, training loss : 0.7593, training accuracy : 79.03, test loss : 0.2001, test accuracy : 94.31\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.7463, accuracy : 80.08\n",
      "iteration : 100, loss : 0.7713, accuracy : 80.98\n",
      "iteration : 150, loss : 0.7687, accuracy : 80.32\n",
      "iteration : 200, loss : 0.7794, accuracy : 78.86\n",
      "iteration : 250, loss : 0.7729, accuracy : 78.50\n",
      "iteration : 300, loss : 0.7743, accuracy : 77.55\n",
      "iteration : 350, loss : 0.7700, accuracy : 78.30\n",
      "Epoch : 113, training loss : 0.7652, training accuracy : 79.02, test loss : 0.2134, test accuracy : 94.24\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.7487, accuracy : 75.20\n",
      "iteration : 100, loss : 0.7530, accuracy : 76.65\n",
      "iteration : 150, loss : 0.7578, accuracy : 77.29\n",
      "iteration : 200, loss : 0.7520, accuracy : 77.79\n",
      "iteration : 250, loss : 0.7516, accuracy : 77.34\n",
      "iteration : 300, loss : 0.7523, accuracy : 77.05\n",
      "iteration : 350, loss : 0.7551, accuracy : 77.98\n",
      "Epoch : 114, training loss : 0.7570, training accuracy : 77.37, test loss : 0.2177, test accuracy : 93.67\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.6898, accuracy : 80.06\n",
      "iteration : 100, loss : 0.7201, accuracy : 78.07\n",
      "iteration : 150, loss : 0.7276, accuracy : 77.15\n",
      "iteration : 200, loss : 0.7446, accuracy : 75.07\n",
      "iteration : 250, loss : 0.7606, accuracy : 74.55\n",
      "iteration : 300, loss : 0.7433, accuracy : 76.14\n",
      "iteration : 350, loss : 0.7465, accuracy : 76.00\n",
      "Epoch : 115, training loss : 0.7527, training accuracy : 75.98, test loss : 0.1904, test accuracy : 94.56\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.7517, accuracy : 75.92\n",
      "iteration : 100, loss : 0.7174, accuracy : 78.93\n",
      "iteration : 150, loss : 0.7476, accuracy : 78.99\n",
      "iteration : 200, loss : 0.7356, accuracy : 80.13\n",
      "iteration : 250, loss : 0.7435, accuracy : 79.42\n",
      "iteration : 300, loss : 0.7461, accuracy : 79.38\n",
      "iteration : 350, loss : 0.7440, accuracy : 79.51\n",
      "Epoch : 116, training loss : 0.7411, training accuracy : 79.81, test loss : 0.1891, test accuracy : 94.43\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.7008, accuracy : 77.03\n",
      "iteration : 100, loss : 0.7340, accuracy : 76.95\n",
      "iteration : 150, loss : 0.7253, accuracy : 78.41\n",
      "iteration : 200, loss : 0.7299, accuracy : 77.70\n",
      "iteration : 250, loss : 0.7266, accuracy : 78.58\n",
      "iteration : 300, loss : 0.7229, accuracy : 79.09\n",
      "iteration : 350, loss : 0.7316, accuracy : 79.22\n",
      "Epoch : 117, training loss : 0.7366, training accuracy : 79.53, test loss : 0.2152, test accuracy : 93.77\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.7491, accuracy : 78.33\n",
      "iteration : 100, loss : 0.7617, accuracy : 76.53\n",
      "iteration : 150, loss : 0.7644, accuracy : 76.85\n",
      "iteration : 200, loss : 0.7491, accuracy : 78.30\n",
      "iteration : 250, loss : 0.7428, accuracy : 79.48\n",
      "iteration : 300, loss : 0.7415, accuracy : 80.54\n",
      "iteration : 350, loss : 0.7447, accuracy : 78.84\n",
      "Epoch : 118, training loss : 0.7359, training accuracy : 79.04, test loss : 0.2082, test accuracy : 94.25\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.6678, accuracy : 88.11\n",
      "iteration : 100, loss : 0.7102, accuracy : 82.88\n",
      "iteration : 150, loss : 0.7207, accuracy : 82.98\n",
      "iteration : 200, loss : 0.7301, accuracy : 81.69\n",
      "iteration : 250, loss : 0.7351, accuracy : 80.91\n",
      "iteration : 300, loss : 0.7370, accuracy : 81.35\n",
      "iteration : 350, loss : 0.7427, accuracy : 80.31\n",
      "Epoch : 119, training loss : 0.7462, training accuracy : 80.50, test loss : 0.2065, test accuracy : 94.20\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.7126, accuracy : 86.67\n",
      "iteration : 100, loss : 0.7161, accuracy : 83.19\n",
      "iteration : 150, loss : 0.7278, accuracy : 83.66\n",
      "iteration : 200, loss : 0.7423, accuracy : 81.86\n",
      "iteration : 250, loss : 0.7496, accuracy : 81.35\n",
      "iteration : 300, loss : 0.7461, accuracy : 80.67\n",
      "iteration : 350, loss : 0.7452, accuracy : 81.37\n",
      "Epoch : 120, training loss : 0.7468, training accuracy : 81.31, test loss : 0.1920, test accuracy : 94.50\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.7436, accuracy : 79.17\n",
      "iteration : 100, loss : 0.7314, accuracy : 79.55\n",
      "iteration : 150, loss : 0.7351, accuracy : 80.17\n",
      "iteration : 200, loss : 0.7330, accuracy : 80.50\n",
      "iteration : 250, loss : 0.7512, accuracy : 79.15\n",
      "iteration : 300, loss : 0.7565, accuracy : 77.82\n",
      "iteration : 350, loss : 0.7513, accuracy : 78.33\n",
      "Epoch : 121, training loss : 0.7564, training accuracy : 77.98, test loss : 0.2001, test accuracy : 94.48\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.7488, accuracy : 79.91\n",
      "iteration : 100, loss : 0.7582, accuracy : 77.73\n",
      "iteration : 150, loss : 0.7507, accuracy : 79.32\n",
      "iteration : 200, loss : 0.7516, accuracy : 79.67\n",
      "iteration : 250, loss : 0.7523, accuracy : 80.25\n",
      "iteration : 300, loss : 0.7509, accuracy : 79.99\n",
      "iteration : 350, loss : 0.7468, accuracy : 80.91\n",
      "Epoch : 122, training loss : 0.7458, training accuracy : 81.15, test loss : 0.2224, test accuracy : 93.79\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.7753, accuracy : 72.19\n",
      "iteration : 100, loss : 0.7373, accuracy : 75.34\n",
      "iteration : 150, loss : 0.7380, accuracy : 76.12\n",
      "iteration : 200, loss : 0.7366, accuracy : 77.24\n",
      "iteration : 250, loss : 0.7306, accuracy : 77.92\n",
      "iteration : 300, loss : 0.7294, accuracy : 78.39\n",
      "iteration : 350, loss : 0.7301, accuracy : 79.45\n",
      "Epoch : 123, training loss : 0.7339, training accuracy : 79.13, test loss : 0.2263, test accuracy : 93.69\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.7811, accuracy : 81.91\n",
      "iteration : 100, loss : 0.7578, accuracy : 78.38\n",
      "iteration : 150, loss : 0.7464, accuracy : 79.05\n",
      "iteration : 200, loss : 0.7384, accuracy : 80.72\n",
      "iteration : 250, loss : 0.7426, accuracy : 79.82\n",
      "iteration : 300, loss : 0.7419, accuracy : 79.43\n",
      "iteration : 350, loss : 0.7264, accuracy : 79.86\n",
      "Epoch : 124, training loss : 0.7225, training accuracy : 79.87, test loss : 0.1913, test accuracy : 94.57\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.6898, accuracy : 82.09\n",
      "iteration : 100, loss : 0.6812, accuracy : 83.12\n",
      "iteration : 150, loss : 0.6957, accuracy : 81.60\n",
      "iteration : 200, loss : 0.7037, accuracy : 80.80\n",
      "iteration : 250, loss : 0.7155, accuracy : 81.48\n",
      "iteration : 300, loss : 0.7137, accuracy : 81.77\n",
      "iteration : 350, loss : 0.7192, accuracy : 80.57\n",
      "Epoch : 125, training loss : 0.7207, training accuracy : 81.03, test loss : 0.1946, test accuracy : 94.45\n",
      "\n",
      "Epoch: 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.7527, accuracy : 74.70\n",
      "iteration : 100, loss : 0.7326, accuracy : 77.47\n",
      "iteration : 150, loss : 0.7442, accuracy : 77.80\n",
      "iteration : 200, loss : 0.7336, accuracy : 78.27\n",
      "iteration : 250, loss : 0.7324, accuracy : 77.47\n",
      "iteration : 300, loss : 0.7387, accuracy : 76.57\n",
      "iteration : 350, loss : 0.7227, accuracy : 77.78\n",
      "Epoch : 126, training loss : 0.7295, training accuracy : 78.08, test loss : 0.1840, test accuracy : 94.63\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.7172, accuracy : 80.55\n",
      "iteration : 100, loss : 0.7110, accuracy : 82.03\n",
      "iteration : 150, loss : 0.7165, accuracy : 80.09\n",
      "iteration : 200, loss : 0.7140, accuracy : 80.12\n",
      "iteration : 250, loss : 0.7174, accuracy : 79.49\n",
      "iteration : 300, loss : 0.7105, accuracy : 79.91\n",
      "iteration : 350, loss : 0.7189, accuracy : 79.17\n",
      "Epoch : 127, training loss : 0.7211, training accuracy : 79.14, test loss : 0.1976, test accuracy : 94.38\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.7671, accuracy : 79.70\n",
      "iteration : 100, loss : 0.7561, accuracy : 80.35\n",
      "iteration : 150, loss : 0.7499, accuracy : 81.41\n",
      "iteration : 200, loss : 0.7499, accuracy : 80.83\n",
      "iteration : 250, loss : 0.7420, accuracy : 80.21\n",
      "iteration : 300, loss : 0.7502, accuracy : 79.54\n",
      "iteration : 350, loss : 0.7502, accuracy : 79.89\n",
      "Epoch : 128, training loss : 0.7418, training accuracy : 79.79, test loss : 0.1962, test accuracy : 94.67\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.6929, accuracy : 80.05\n",
      "iteration : 100, loss : 0.7231, accuracy : 79.04\n",
      "iteration : 150, loss : 0.7152, accuracy : 79.64\n",
      "iteration : 200, loss : 0.7142, accuracy : 80.55\n",
      "iteration : 250, loss : 0.7196, accuracy : 79.30\n",
      "iteration : 300, loss : 0.7175, accuracy : 79.71\n",
      "iteration : 350, loss : 0.7106, accuracy : 80.47\n",
      "Epoch : 129, training loss : 0.7148, training accuracy : 80.53, test loss : 0.1814, test accuracy : 94.92\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.6969, accuracy : 81.53\n",
      "iteration : 100, loss : 0.6983, accuracy : 83.53\n",
      "iteration : 150, loss : 0.7106, accuracy : 79.29\n",
      "iteration : 200, loss : 0.7045, accuracy : 80.64\n",
      "iteration : 250, loss : 0.6968, accuracy : 80.88\n",
      "iteration : 300, loss : 0.7062, accuracy : 80.45\n",
      "iteration : 350, loss : 0.7127, accuracy : 80.21\n",
      "Epoch : 130, training loss : 0.7039, training accuracy : 80.79, test loss : 0.2143, test accuracy : 94.20\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.7136, accuracy : 84.36\n",
      "iteration : 100, loss : 0.7039, accuracy : 82.15\n",
      "iteration : 150, loss : 0.7129, accuracy : 81.24\n",
      "iteration : 200, loss : 0.7095, accuracy : 82.27\n",
      "iteration : 250, loss : 0.7209, accuracy : 80.33\n",
      "iteration : 300, loss : 0.7242, accuracy : 79.36\n",
      "iteration : 350, loss : 0.7306, accuracy : 79.49\n",
      "Epoch : 131, training loss : 0.7319, training accuracy : 80.04, test loss : 0.1857, test accuracy : 94.47\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.7195, accuracy : 74.36\n",
      "iteration : 100, loss : 0.7284, accuracy : 74.95\n",
      "iteration : 150, loss : 0.7215, accuracy : 79.01\n",
      "iteration : 200, loss : 0.7192, accuracy : 79.01\n",
      "iteration : 250, loss : 0.7280, accuracy : 77.82\n",
      "iteration : 300, loss : 0.7247, accuracy : 78.22\n",
      "iteration : 350, loss : 0.7262, accuracy : 78.26\n",
      "Epoch : 132, training loss : 0.7308, training accuracy : 77.35, test loss : 0.1940, test accuracy : 94.49\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.6835, accuracy : 75.02\n",
      "iteration : 100, loss : 0.7188, accuracy : 78.68\n",
      "iteration : 150, loss : 0.7220, accuracy : 77.85\n",
      "iteration : 200, loss : 0.7279, accuracy : 77.54\n",
      "iteration : 250, loss : 0.7283, accuracy : 78.20\n",
      "iteration : 300, loss : 0.7147, accuracy : 80.46\n",
      "iteration : 350, loss : 0.7155, accuracy : 79.51\n",
      "Epoch : 133, training loss : 0.7145, training accuracy : 80.00, test loss : 0.2000, test accuracy : 94.24\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.6979, accuracy : 88.91\n",
      "iteration : 100, loss : 0.7149, accuracy : 85.96\n",
      "iteration : 150, loss : 0.7226, accuracy : 83.20\n",
      "iteration : 200, loss : 0.7216, accuracy : 82.55\n",
      "iteration : 250, loss : 0.7231, accuracy : 82.38\n",
      "iteration : 300, loss : 0.7187, accuracy : 81.61\n",
      "iteration : 350, loss : 0.7211, accuracy : 80.73\n",
      "Epoch : 134, training loss : 0.7153, training accuracy : 81.44, test loss : 0.1975, test accuracy : 94.53\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.7617, accuracy : 79.94\n",
      "iteration : 100, loss : 0.7367, accuracy : 78.92\n",
      "iteration : 150, loss : 0.7302, accuracy : 78.91\n",
      "iteration : 200, loss : 0.7287, accuracy : 79.24\n",
      "iteration : 250, loss : 0.7384, accuracy : 78.83\n",
      "iteration : 300, loss : 0.7401, accuracy : 78.83\n",
      "iteration : 350, loss : 0.7410, accuracy : 78.83\n",
      "Epoch : 135, training loss : 0.7331, training accuracy : 79.05, test loss : 0.1899, test accuracy : 94.61\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.7707, accuracy : 77.11\n",
      "iteration : 100, loss : 0.7311, accuracy : 81.47\n",
      "iteration : 150, loss : 0.7239, accuracy : 83.86\n",
      "iteration : 200, loss : 0.7149, accuracy : 85.02\n",
      "iteration : 250, loss : 0.7058, accuracy : 84.44\n",
      "iteration : 300, loss : 0.7026, accuracy : 83.79\n",
      "iteration : 350, loss : 0.7023, accuracy : 83.07\n",
      "Epoch : 136, training loss : 0.7071, training accuracy : 82.43, test loss : 0.1847, test accuracy : 94.83\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.7177, accuracy : 87.28\n",
      "iteration : 100, loss : 0.7322, accuracy : 80.05\n",
      "iteration : 150, loss : 0.7294, accuracy : 80.70\n",
      "iteration : 200, loss : 0.7169, accuracy : 80.60\n",
      "iteration : 250, loss : 0.7119, accuracy : 80.83\n",
      "iteration : 300, loss : 0.7102, accuracy : 80.23\n",
      "iteration : 350, loss : 0.7192, accuracy : 80.00\n",
      "Epoch : 137, training loss : 0.7195, training accuracy : 79.92, test loss : 0.1871, test accuracy : 94.77\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.7256, accuracy : 82.62\n",
      "iteration : 100, loss : 0.7159, accuracy : 82.19\n",
      "iteration : 150, loss : 0.7218, accuracy : 79.39\n",
      "iteration : 200, loss : 0.7165, accuracy : 78.75\n",
      "iteration : 250, loss : 0.7231, accuracy : 79.28\n",
      "iteration : 300, loss : 0.7172, accuracy : 78.94\n",
      "iteration : 350, loss : 0.7208, accuracy : 78.48\n",
      "Epoch : 138, training loss : 0.7190, training accuracy : 79.18, test loss : 0.1807, test accuracy : 94.98\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.7479, accuracy : 76.70\n",
      "iteration : 100, loss : 0.7274, accuracy : 77.62\n",
      "iteration : 150, loss : 0.7341, accuracy : 75.32\n",
      "iteration : 200, loss : 0.7294, accuracy : 76.52\n",
      "iteration : 250, loss : 0.7199, accuracy : 77.64\n",
      "iteration : 300, loss : 0.7256, accuracy : 77.28\n",
      "iteration : 350, loss : 0.7204, accuracy : 78.10\n",
      "Epoch : 139, training loss : 0.7234, training accuracy : 77.87, test loss : 0.1754, test accuracy : 95.14\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.7000, accuracy : 78.64\n",
      "iteration : 100, loss : 0.6684, accuracy : 83.16\n",
      "iteration : 150, loss : 0.6759, accuracy : 81.90\n",
      "iteration : 200, loss : 0.6886, accuracy : 80.32\n",
      "iteration : 250, loss : 0.6963, accuracy : 80.55\n",
      "iteration : 300, loss : 0.6980, accuracy : 81.30\n",
      "iteration : 350, loss : 0.6921, accuracy : 81.76\n",
      "Epoch : 140, training loss : 0.6915, training accuracy : 81.86, test loss : 0.1825, test accuracy : 94.89\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.6696, accuracy : 84.86\n",
      "iteration : 100, loss : 0.6715, accuracy : 84.24\n",
      "iteration : 150, loss : 0.6793, accuracy : 83.47\n",
      "iteration : 200, loss : 0.6816, accuracy : 82.89\n",
      "iteration : 250, loss : 0.6870, accuracy : 81.84\n",
      "iteration : 300, loss : 0.6954, accuracy : 81.36\n",
      "iteration : 350, loss : 0.6981, accuracy : 82.04\n",
      "Epoch : 141, training loss : 0.6961, training accuracy : 82.16, test loss : 0.1761, test accuracy : 94.95\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.6918, accuracy : 82.80\n",
      "iteration : 100, loss : 0.6949, accuracy : 80.41\n",
      "iteration : 150, loss : 0.6878, accuracy : 80.24\n",
      "iteration : 200, loss : 0.6850, accuracy : 81.70\n",
      "iteration : 250, loss : 0.6853, accuracy : 82.39\n",
      "iteration : 300, loss : 0.6841, accuracy : 82.71\n",
      "iteration : 350, loss : 0.6948, accuracy : 81.62\n",
      "Epoch : 142, training loss : 0.6922, training accuracy : 81.84, test loss : 0.1824, test accuracy : 95.12\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.7162, accuracy : 84.31\n",
      "iteration : 100, loss : 0.7050, accuracy : 83.77\n",
      "iteration : 150, loss : 0.7088, accuracy : 82.34\n",
      "iteration : 200, loss : 0.6925, accuracy : 83.79\n",
      "iteration : 250, loss : 0.6845, accuracy : 83.11\n",
      "iteration : 300, loss : 0.6839, accuracy : 82.24\n",
      "iteration : 350, loss : 0.6849, accuracy : 82.47\n",
      "Epoch : 143, training loss : 0.6801, training accuracy : 82.53, test loss : 0.1793, test accuracy : 95.06\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.6405, accuracy : 88.17\n",
      "iteration : 100, loss : 0.6792, accuracy : 83.85\n",
      "iteration : 150, loss : 0.7037, accuracy : 80.80\n",
      "iteration : 200, loss : 0.6998, accuracy : 79.91\n",
      "iteration : 250, loss : 0.6915, accuracy : 81.42\n",
      "iteration : 300, loss : 0.6916, accuracy : 81.98\n",
      "iteration : 350, loss : 0.6904, accuracy : 81.60\n",
      "Epoch : 144, training loss : 0.6869, training accuracy : 81.79, test loss : 0.1800, test accuracy : 95.12\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.6977, accuracy : 84.62\n",
      "iteration : 100, loss : 0.6877, accuracy : 84.72\n",
      "iteration : 150, loss : 0.6968, accuracy : 83.17\n",
      "iteration : 200, loss : 0.6920, accuracy : 82.92\n",
      "iteration : 250, loss : 0.6884, accuracy : 83.27\n",
      "iteration : 300, loss : 0.6862, accuracy : 82.85\n",
      "iteration : 350, loss : 0.6829, accuracy : 83.38\n",
      "Epoch : 145, training loss : 0.6804, training accuracy : 83.56, test loss : 0.1706, test accuracy : 95.32\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.6876, accuracy : 84.31\n",
      "iteration : 100, loss : 0.6936, accuracy : 83.07\n",
      "iteration : 150, loss : 0.6924, accuracy : 83.34\n",
      "iteration : 200, loss : 0.6863, accuracy : 83.64\n",
      "iteration : 250, loss : 0.6814, accuracy : 83.35\n",
      "iteration : 300, loss : 0.6870, accuracy : 82.88\n",
      "iteration : 350, loss : 0.6898, accuracy : 82.47\n",
      "Epoch : 146, training loss : 0.6897, training accuracy : 82.61, test loss : 0.1751, test accuracy : 95.19\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.6628, accuracy : 81.91\n",
      "iteration : 100, loss : 0.7002, accuracy : 81.38\n",
      "iteration : 150, loss : 0.6957, accuracy : 80.36\n",
      "iteration : 200, loss : 0.7006, accuracy : 79.53\n",
      "iteration : 250, loss : 0.6958, accuracy : 79.94\n",
      "iteration : 300, loss : 0.7008, accuracy : 80.03\n",
      "iteration : 350, loss : 0.7023, accuracy : 80.14\n",
      "Epoch : 147, training loss : 0.7018, training accuracy : 80.58, test loss : 0.1690, test accuracy : 95.34\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.7093, accuracy : 78.88\n",
      "iteration : 100, loss : 0.7185, accuracy : 78.32\n",
      "iteration : 150, loss : 0.6984, accuracy : 80.79\n",
      "iteration : 200, loss : 0.6906, accuracy : 80.54\n",
      "iteration : 250, loss : 0.7009, accuracy : 78.92\n",
      "iteration : 300, loss : 0.6970, accuracy : 78.85\n",
      "iteration : 350, loss : 0.7009, accuracy : 78.84\n",
      "Epoch : 148, training loss : 0.6998, training accuracy : 79.21, test loss : 0.1706, test accuracy : 95.28\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.6691, accuracy : 85.16\n",
      "iteration : 100, loss : 0.6810, accuracy : 84.63\n",
      "iteration : 150, loss : 0.6745, accuracy : 83.38\n",
      "iteration : 200, loss : 0.6682, accuracy : 83.62\n",
      "iteration : 250, loss : 0.6679, accuracy : 84.62\n",
      "iteration : 300, loss : 0.6664, accuracy : 83.40\n",
      "iteration : 350, loss : 0.6734, accuracy : 82.29\n",
      "Epoch : 149, training loss : 0.6725, training accuracy : 81.37, test loss : 0.1645, test accuracy : 95.37\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.6894, accuracy : 76.19\n",
      "iteration : 100, loss : 0.6932, accuracy : 80.26\n",
      "iteration : 150, loss : 0.6891, accuracy : 80.53\n",
      "iteration : 200, loss : 0.6823, accuracy : 79.79\n",
      "iteration : 250, loss : 0.6756, accuracy : 79.84\n",
      "iteration : 300, loss : 0.6764, accuracy : 80.41\n",
      "iteration : 350, loss : 0.6765, accuracy : 80.15\n",
      "Epoch : 150, training loss : 0.6768, training accuracy : 80.12, test loss : 0.1868, test accuracy : 94.91\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.6510, accuracy : 85.89\n",
      "iteration : 100, loss : 0.6674, accuracy : 81.72\n",
      "iteration : 150, loss : 0.6435, accuracy : 83.96\n",
      "iteration : 200, loss : 0.6526, accuracy : 83.97\n",
      "iteration : 250, loss : 0.6671, accuracy : 83.22\n",
      "iteration : 300, loss : 0.6639, accuracy : 83.82\n",
      "iteration : 350, loss : 0.6673, accuracy : 83.52\n",
      "Epoch : 151, training loss : 0.6644, training accuracy : 84.26, test loss : 0.1796, test accuracy : 95.17\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.5935, accuracy : 87.56\n",
      "iteration : 100, loss : 0.6456, accuracy : 82.48\n",
      "iteration : 150, loss : 0.6614, accuracy : 81.93\n",
      "iteration : 200, loss : 0.6600, accuracy : 82.83\n",
      "iteration : 250, loss : 0.6627, accuracy : 83.43\n",
      "iteration : 300, loss : 0.6488, accuracy : 84.48\n",
      "iteration : 350, loss : 0.6517, accuracy : 84.15\n",
      "Epoch : 152, training loss : 0.6598, training accuracy : 83.84, test loss : 0.1751, test accuracy : 95.27\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.6668, accuracy : 79.62\n",
      "iteration : 100, loss : 0.6647, accuracy : 81.99\n",
      "iteration : 150, loss : 0.6716, accuracy : 83.93\n",
      "iteration : 200, loss : 0.6695, accuracy : 84.05\n",
      "iteration : 250, loss : 0.6705, accuracy : 82.96\n",
      "iteration : 300, loss : 0.6622, accuracy : 82.45\n",
      "iteration : 350, loss : 0.6717, accuracy : 81.32\n",
      "Epoch : 153, training loss : 0.6769, training accuracy : 80.55, test loss : 0.1836, test accuracy : 95.08\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.6308, accuracy : 78.58\n",
      "iteration : 100, loss : 0.6579, accuracy : 83.35\n",
      "iteration : 150, loss : 0.6829, accuracy : 81.58\n",
      "iteration : 200, loss : 0.6785, accuracy : 82.71\n",
      "iteration : 250, loss : 0.6804, accuracy : 82.47\n",
      "iteration : 300, loss : 0.6777, accuracy : 82.96\n",
      "iteration : 350, loss : 0.6766, accuracy : 82.88\n",
      "Epoch : 154, training loss : 0.6754, training accuracy : 82.13, test loss : 0.1735, test accuracy : 95.34\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.6027, accuracy : 90.16\n",
      "iteration : 100, loss : 0.6341, accuracy : 86.56\n",
      "iteration : 150, loss : 0.6236, accuracy : 87.15\n",
      "iteration : 200, loss : 0.6435, accuracy : 85.11\n",
      "iteration : 250, loss : 0.6498, accuracy : 83.09\n",
      "iteration : 300, loss : 0.6531, accuracy : 83.29\n",
      "iteration : 350, loss : 0.6532, accuracy : 83.35\n",
      "Epoch : 155, training loss : 0.6552, training accuracy : 83.09, test loss : 0.1717, test accuracy : 95.31\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.6704, accuracy : 83.48\n",
      "iteration : 100, loss : 0.6675, accuracy : 83.05\n",
      "iteration : 150, loss : 0.6867, accuracy : 80.93\n",
      "iteration : 200, loss : 0.6898, accuracy : 80.70\n",
      "iteration : 250, loss : 0.6878, accuracy : 80.89\n",
      "iteration : 300, loss : 0.6820, accuracy : 81.57\n",
      "iteration : 350, loss : 0.6784, accuracy : 80.88\n",
      "Epoch : 156, training loss : 0.6765, training accuracy : 80.58, test loss : 0.1771, test accuracy : 95.22\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.6301, accuracy : 82.59\n",
      "iteration : 100, loss : 0.6327, accuracy : 83.45\n",
      "iteration : 150, loss : 0.6400, accuracy : 85.70\n",
      "iteration : 200, loss : 0.6376, accuracy : 86.58\n",
      "iteration : 250, loss : 0.6407, accuracy : 85.42\n",
      "iteration : 300, loss : 0.6451, accuracy : 84.57\n",
      "iteration : 350, loss : 0.6528, accuracy : 84.36\n",
      "Epoch : 157, training loss : 0.6539, training accuracy : 84.88, test loss : 0.1668, test accuracy : 95.32\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.6685, accuracy : 82.67\n",
      "iteration : 100, loss : 0.6921, accuracy : 81.90\n",
      "iteration : 150, loss : 0.7000, accuracy : 80.05\n",
      "iteration : 200, loss : 0.6977, accuracy : 79.61\n",
      "iteration : 250, loss : 0.6795, accuracy : 80.12\n",
      "iteration : 300, loss : 0.6838, accuracy : 80.57\n",
      "iteration : 350, loss : 0.6846, accuracy : 80.54\n",
      "Epoch : 158, training loss : 0.6810, training accuracy : 80.93, test loss : 0.1745, test accuracy : 95.23\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.7069, accuracy : 79.59\n",
      "iteration : 100, loss : 0.7157, accuracy : 77.48\n",
      "iteration : 150, loss : 0.7035, accuracy : 80.38\n",
      "iteration : 200, loss : 0.7085, accuracy : 80.16\n",
      "iteration : 250, loss : 0.6937, accuracy : 80.35\n",
      "iteration : 300, loss : 0.6898, accuracy : 81.37\n",
      "iteration : 350, loss : 0.6923, accuracy : 80.81\n",
      "Epoch : 159, training loss : 0.6924, training accuracy : 80.59, test loss : 0.1661, test accuracy : 95.34\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.6682, accuracy : 86.72\n",
      "iteration : 100, loss : 0.6721, accuracy : 83.09\n",
      "iteration : 150, loss : 0.6723, accuracy : 82.58\n",
      "iteration : 200, loss : 0.6751, accuracy : 82.61\n",
      "iteration : 250, loss : 0.6755, accuracy : 81.42\n",
      "iteration : 300, loss : 0.6776, accuracy : 81.62\n",
      "iteration : 350, loss : 0.6753, accuracy : 81.85\n",
      "Epoch : 160, training loss : 0.6735, training accuracy : 81.69, test loss : 0.1688, test accuracy : 95.37\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.6050, accuracy : 86.30\n",
      "iteration : 100, loss : 0.6236, accuracy : 85.63\n",
      "iteration : 150, loss : 0.6514, accuracy : 82.06\n",
      "iteration : 200, loss : 0.6426, accuracy : 83.98\n",
      "iteration : 250, loss : 0.6465, accuracy : 83.82\n",
      "iteration : 300, loss : 0.6564, accuracy : 83.62\n",
      "iteration : 350, loss : 0.6611, accuracy : 82.78\n",
      "Epoch : 161, training loss : 0.6622, training accuracy : 82.73, test loss : 0.1685, test accuracy : 95.42\n",
      "\n",
      "Epoch: 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.6665, accuracy : 81.34\n",
      "iteration : 100, loss : 0.6749, accuracy : 76.52\n",
      "iteration : 150, loss : 0.6780, accuracy : 79.74\n",
      "iteration : 200, loss : 0.6862, accuracy : 79.42\n",
      "iteration : 250, loss : 0.6836, accuracy : 80.56\n",
      "iteration : 300, loss : 0.6879, accuracy : 80.20\n",
      "iteration : 350, loss : 0.6833, accuracy : 80.28\n",
      "Epoch : 162, training loss : 0.6810, training accuracy : 80.20, test loss : 0.1646, test accuracy : 95.63\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.6646, accuracy : 83.84\n",
      "iteration : 100, loss : 0.6413, accuracy : 82.88\n",
      "iteration : 150, loss : 0.6370, accuracy : 82.90\n",
      "iteration : 200, loss : 0.6519, accuracy : 83.24\n",
      "iteration : 250, loss : 0.6614, accuracy : 83.31\n",
      "iteration : 300, loss : 0.6529, accuracy : 83.14\n",
      "iteration : 350, loss : 0.6480, accuracy : 83.09\n",
      "Epoch : 163, training loss : 0.6552, training accuracy : 82.07, test loss : 0.1734, test accuracy : 95.16\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.6801, accuracy : 84.89\n",
      "iteration : 100, loss : 0.6746, accuracy : 82.16\n",
      "iteration : 150, loss : 0.6671, accuracy : 80.30\n",
      "iteration : 200, loss : 0.6655, accuracy : 79.45\n",
      "iteration : 250, loss : 0.6743, accuracy : 78.48\n",
      "iteration : 300, loss : 0.6807, accuracy : 77.81\n",
      "iteration : 350, loss : 0.6749, accuracy : 78.12\n",
      "Epoch : 164, training loss : 0.6718, training accuracy : 78.33, test loss : 0.1678, test accuracy : 95.37\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.6695, accuracy : 74.34\n",
      "iteration : 100, loss : 0.6698, accuracy : 77.13\n",
      "iteration : 150, loss : 0.6590, accuracy : 77.64\n",
      "iteration : 200, loss : 0.6549, accuracy : 80.49\n",
      "iteration : 250, loss : 0.6538, accuracy : 82.96\n",
      "iteration : 300, loss : 0.6597, accuracy : 82.64\n",
      "iteration : 350, loss : 0.6503, accuracy : 83.49\n",
      "Epoch : 165, training loss : 0.6485, training accuracy : 83.07, test loss : 0.1654, test accuracy : 95.50\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.6860, accuracy : 76.16\n",
      "iteration : 100, loss : 0.6772, accuracy : 80.75\n",
      "iteration : 150, loss : 0.6646, accuracy : 81.92\n",
      "iteration : 200, loss : 0.6745, accuracy : 80.97\n",
      "iteration : 250, loss : 0.6675, accuracy : 80.09\n",
      "iteration : 300, loss : 0.6670, accuracy : 79.47\n",
      "iteration : 350, loss : 0.6616, accuracy : 80.08\n",
      "Epoch : 166, training loss : 0.6645, training accuracy : 79.62, test loss : 0.1653, test accuracy : 95.45\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.6498, accuracy : 87.14\n",
      "iteration : 100, loss : 0.6737, accuracy : 84.31\n",
      "iteration : 150, loss : 0.6798, accuracy : 82.38\n",
      "iteration : 200, loss : 0.6672, accuracy : 82.90\n",
      "iteration : 250, loss : 0.6623, accuracy : 83.37\n",
      "iteration : 300, loss : 0.6703, accuracy : 82.90\n",
      "iteration : 350, loss : 0.6700, accuracy : 82.71\n",
      "Epoch : 167, training loss : 0.6648, training accuracy : 82.81, test loss : 0.1665, test accuracy : 95.53\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.6524, accuracy : 81.17\n",
      "iteration : 100, loss : 0.6507, accuracy : 83.57\n",
      "iteration : 150, loss : 0.6405, accuracy : 85.05\n",
      "iteration : 200, loss : 0.6413, accuracy : 84.29\n",
      "iteration : 250, loss : 0.6473, accuracy : 84.25\n",
      "iteration : 300, loss : 0.6444, accuracy : 84.66\n",
      "iteration : 350, loss : 0.6383, accuracy : 84.87\n",
      "Epoch : 168, training loss : 0.6484, training accuracy : 83.31, test loss : 0.1574, test accuracy : 95.57\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.6561, accuracy : 79.52\n",
      "iteration : 100, loss : 0.6226, accuracy : 81.95\n",
      "iteration : 150, loss : 0.6269, accuracy : 83.24\n",
      "iteration : 200, loss : 0.6324, accuracy : 84.12\n",
      "iteration : 250, loss : 0.6347, accuracy : 84.47\n",
      "iteration : 300, loss : 0.6410, accuracy : 84.88\n",
      "iteration : 350, loss : 0.6403, accuracy : 84.95\n",
      "Epoch : 169, training loss : 0.6432, training accuracy : 84.28, test loss : 0.1705, test accuracy : 95.42\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.6885, accuracy : 84.75\n",
      "iteration : 100, loss : 0.6706, accuracy : 81.11\n",
      "iteration : 150, loss : 0.6633, accuracy : 83.23\n",
      "iteration : 200, loss : 0.6592, accuracy : 83.24\n",
      "iteration : 250, loss : 0.6563, accuracy : 83.62\n",
      "iteration : 300, loss : 0.6479, accuracy : 82.89\n",
      "iteration : 350, loss : 0.6494, accuracy : 82.38\n",
      "Epoch : 170, training loss : 0.6472, training accuracy : 82.42, test loss : 0.1632, test accuracy : 95.47\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.6960, accuracy : 75.77\n",
      "iteration : 100, loss : 0.6835, accuracy : 80.85\n",
      "iteration : 150, loss : 0.6795, accuracy : 79.85\n",
      "iteration : 200, loss : 0.6829, accuracy : 78.78\n",
      "iteration : 250, loss : 0.6685, accuracy : 81.30\n",
      "iteration : 300, loss : 0.6743, accuracy : 81.29\n",
      "iteration : 350, loss : 0.6740, accuracy : 80.69\n",
      "Epoch : 171, training loss : 0.6724, training accuracy : 80.98, test loss : 0.1616, test accuracy : 95.44\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.6362, accuracy : 82.22\n",
      "iteration : 100, loss : 0.6147, accuracy : 84.77\n",
      "iteration : 150, loss : 0.6309, accuracy : 84.24\n",
      "iteration : 200, loss : 0.6379, accuracy : 83.23\n",
      "iteration : 250, loss : 0.6337, accuracy : 82.92\n",
      "iteration : 300, loss : 0.6408, accuracy : 81.71\n",
      "iteration : 350, loss : 0.6347, accuracy : 82.11\n",
      "Epoch : 172, training loss : 0.6394, training accuracy : 81.62, test loss : 0.1648, test accuracy : 95.44\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.5987, accuracy : 90.91\n",
      "iteration : 100, loss : 0.6342, accuracy : 85.52\n",
      "iteration : 150, loss : 0.6518, accuracy : 85.86\n",
      "iteration : 200, loss : 0.6633, accuracy : 83.33\n",
      "iteration : 250, loss : 0.6617, accuracy : 84.77\n",
      "iteration : 300, loss : 0.6586, accuracy : 84.38\n",
      "iteration : 350, loss : 0.6602, accuracy : 84.02\n",
      "Epoch : 173, training loss : 0.6626, training accuracy : 83.57, test loss : 0.1631, test accuracy : 95.41\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.6783, accuracy : 86.52\n",
      "iteration : 100, loss : 0.6554, accuracy : 85.45\n",
      "iteration : 150, loss : 0.6588, accuracy : 83.90\n",
      "iteration : 200, loss : 0.6649, accuracy : 82.97\n",
      "iteration : 250, loss : 0.6611, accuracy : 81.56\n",
      "iteration : 300, loss : 0.6632, accuracy : 82.24\n",
      "iteration : 350, loss : 0.6634, accuracy : 82.51\n",
      "Epoch : 174, training loss : 0.6617, training accuracy : 82.37, test loss : 0.1630, test accuracy : 95.63\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.6701, accuracy : 88.72\n",
      "iteration : 100, loss : 0.6522, accuracy : 85.35\n",
      "iteration : 150, loss : 0.6364, accuracy : 86.67\n",
      "iteration : 200, loss : 0.6339, accuracy : 87.21\n",
      "iteration : 250, loss : 0.6375, accuracy : 85.90\n",
      "iteration : 300, loss : 0.6356, accuracy : 85.92\n",
      "iteration : 350, loss : 0.6318, accuracy : 86.26\n",
      "Epoch : 175, training loss : 0.6359, training accuracy : 85.35, test loss : 0.1617, test accuracy : 95.41\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.6812, accuracy : 76.84\n",
      "iteration : 100, loss : 0.6784, accuracy : 79.35\n",
      "iteration : 150, loss : 0.6790, accuracy : 81.55\n",
      "iteration : 200, loss : 0.6546, accuracy : 82.82\n",
      "iteration : 250, loss : 0.6639, accuracy : 80.82\n",
      "iteration : 300, loss : 0.6622, accuracy : 78.88\n",
      "iteration : 350, loss : 0.6620, accuracy : 78.83\n",
      "Epoch : 176, training loss : 0.6603, training accuracy : 78.88, test loss : 0.1624, test accuracy : 95.49\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.6422, accuracy : 82.12\n",
      "iteration : 100, loss : 0.6651, accuracy : 84.19\n",
      "iteration : 150, loss : 0.6636, accuracy : 83.92\n",
      "iteration : 200, loss : 0.6640, accuracy : 83.47\n",
      "iteration : 250, loss : 0.6698, accuracy : 83.93\n",
      "iteration : 300, loss : 0.6629, accuracy : 83.76\n",
      "iteration : 350, loss : 0.6630, accuracy : 83.70\n",
      "Epoch : 177, training loss : 0.6636, training accuracy : 83.11, test loss : 0.1594, test accuracy : 95.60\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.6217, accuracy : 84.91\n",
      "iteration : 100, loss : 0.6530, accuracy : 80.85\n",
      "iteration : 150, loss : 0.6391, accuracy : 78.99\n",
      "iteration : 200, loss : 0.6529, accuracy : 78.48\n",
      "iteration : 250, loss : 0.6532, accuracy : 79.36\n",
      "iteration : 300, loss : 0.6554, accuracy : 80.05\n",
      "iteration : 350, loss : 0.6584, accuracy : 80.12\n",
      "Epoch : 178, training loss : 0.6632, training accuracy : 79.93, test loss : 0.1569, test accuracy : 95.48\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.6570, accuracy : 82.70\n",
      "iteration : 100, loss : 0.6438, accuracy : 78.69\n",
      "iteration : 150, loss : 0.6450, accuracy : 79.89\n",
      "iteration : 200, loss : 0.6448, accuracy : 80.29\n",
      "iteration : 250, loss : 0.6465, accuracy : 80.96\n",
      "iteration : 300, loss : 0.6479, accuracy : 81.19\n",
      "iteration : 350, loss : 0.6445, accuracy : 81.48\n",
      "Epoch : 179, training loss : 0.6412, training accuracy : 81.36, test loss : 0.1622, test accuracy : 95.58\n",
      "\n",
      "Epoch: 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.6094, accuracy : 85.53\n",
      "iteration : 100, loss : 0.6125, accuracy : 86.65\n",
      "iteration : 150, loss : 0.6071, accuracy : 86.35\n",
      "iteration : 200, loss : 0.6309, accuracy : 83.26\n",
      "iteration : 250, loss : 0.6376, accuracy : 83.01\n",
      "iteration : 300, loss : 0.6333, accuracy : 83.75\n",
      "iteration : 350, loss : 0.6357, accuracy : 83.91\n",
      "Epoch : 180, training loss : 0.6329, training accuracy : 84.06, test loss : 0.1629, test accuracy : 95.52\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.6363, accuracy : 83.38\n",
      "iteration : 100, loss : 0.6684, accuracy : 83.62\n",
      "iteration : 150, loss : 0.6696, accuracy : 83.14\n",
      "iteration : 200, loss : 0.6612, accuracy : 83.70\n",
      "iteration : 250, loss : 0.6579, accuracy : 83.04\n",
      "iteration : 300, loss : 0.6610, accuracy : 82.90\n",
      "iteration : 350, loss : 0.6547, accuracy : 82.58\n",
      "Epoch : 181, training loss : 0.6555, training accuracy : 82.95, test loss : 0.1597, test accuracy : 95.58\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.6613, accuracy : 82.66\n",
      "iteration : 100, loss : 0.6426, accuracy : 81.22\n",
      "iteration : 150, loss : 0.6309, accuracy : 82.68\n",
      "iteration : 200, loss : 0.6395, accuracy : 82.06\n",
      "iteration : 250, loss : 0.6442, accuracy : 81.20\n",
      "iteration : 300, loss : 0.6534, accuracy : 80.94\n",
      "iteration : 350, loss : 0.6528, accuracy : 80.68\n",
      "Epoch : 182, training loss : 0.6494, training accuracy : 81.30, test loss : 0.1613, test accuracy : 95.59\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.6347, accuracy : 81.58\n",
      "iteration : 100, loss : 0.6413, accuracy : 80.48\n",
      "iteration : 150, loss : 0.6447, accuracy : 80.36\n",
      "iteration : 200, loss : 0.6410, accuracy : 80.88\n",
      "iteration : 250, loss : 0.6418, accuracy : 81.19\n",
      "iteration : 300, loss : 0.6484, accuracy : 80.46\n",
      "iteration : 350, loss : 0.6432, accuracy : 81.84\n",
      "Epoch : 183, training loss : 0.6452, training accuracy : 82.07, test loss : 0.1586, test accuracy : 95.65\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.5694, accuracy : 84.00\n",
      "iteration : 100, loss : 0.6297, accuracy : 80.88\n",
      "iteration : 150, loss : 0.6177, accuracy : 83.01\n",
      "iteration : 200, loss : 0.6220, accuracy : 84.35\n",
      "iteration : 250, loss : 0.6291, accuracy : 83.58\n",
      "iteration : 300, loss : 0.6298, accuracy : 84.00\n",
      "iteration : 350, loss : 0.6324, accuracy : 83.67\n",
      "Epoch : 184, training loss : 0.6322, training accuracy : 83.98, test loss : 0.1596, test accuracy : 95.58\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.6677, accuracy : 81.95\n",
      "iteration : 100, loss : 0.6620, accuracy : 85.34\n",
      "iteration : 150, loss : 0.6761, accuracy : 81.82\n",
      "iteration : 200, loss : 0.6680, accuracy : 83.55\n",
      "iteration : 250, loss : 0.6667, accuracy : 83.19\n",
      "iteration : 300, loss : 0.6615, accuracy : 83.90\n",
      "iteration : 350, loss : 0.6534, accuracy : 83.98\n",
      "Epoch : 185, training loss : 0.6511, training accuracy : 83.59, test loss : 0.1605, test accuracy : 95.63\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.6885, accuracy : 82.28\n",
      "iteration : 100, loss : 0.6403, accuracy : 85.55\n",
      "iteration : 150, loss : 0.6527, accuracy : 83.20\n",
      "iteration : 200, loss : 0.6520, accuracy : 84.33\n",
      "iteration : 250, loss : 0.6552, accuracy : 83.77\n",
      "iteration : 300, loss : 0.6501, accuracy : 84.99\n",
      "iteration : 350, loss : 0.6525, accuracy : 84.20\n",
      "Epoch : 186, training loss : 0.6442, training accuracy : 84.43, test loss : 0.1648, test accuracy : 95.61\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.6458, accuracy : 85.67\n",
      "iteration : 100, loss : 0.6444, accuracy : 80.34\n",
      "iteration : 150, loss : 0.6539, accuracy : 78.91\n",
      "iteration : 200, loss : 0.6332, accuracy : 82.45\n",
      "iteration : 250, loss : 0.6263, accuracy : 83.42\n",
      "iteration : 300, loss : 0.6342, accuracy : 82.57\n",
      "iteration : 350, loss : 0.6389, accuracy : 81.99\n",
      "Epoch : 187, training loss : 0.6447, training accuracy : 81.73, test loss : 0.1591, test accuracy : 95.59\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.6335, accuracy : 86.17\n",
      "iteration : 100, loss : 0.6387, accuracy : 85.42\n",
      "iteration : 150, loss : 0.6347, accuracy : 85.71\n",
      "iteration : 200, loss : 0.6402, accuracy : 82.88\n",
      "iteration : 250, loss : 0.6429, accuracy : 83.55\n",
      "iteration : 300, loss : 0.6353, accuracy : 83.96\n",
      "iteration : 350, loss : 0.6370, accuracy : 84.42\n",
      "Epoch : 188, training loss : 0.6361, training accuracy : 84.58, test loss : 0.1600, test accuracy : 95.59\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.5897, accuracy : 87.94\n",
      "iteration : 100, loss : 0.6305, accuracy : 83.01\n",
      "iteration : 150, loss : 0.6213, accuracy : 85.03\n",
      "iteration : 200, loss : 0.6321, accuracy : 84.43\n",
      "iteration : 250, loss : 0.6349, accuracy : 85.17\n",
      "iteration : 300, loss : 0.6242, accuracy : 86.22\n",
      "iteration : 350, loss : 0.6334, accuracy : 84.19\n",
      "Epoch : 189, training loss : 0.6315, training accuracy : 83.97, test loss : 0.1601, test accuracy : 95.69\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.6515, accuracy : 82.19\n",
      "iteration : 100, loss : 0.6516, accuracy : 82.63\n",
      "iteration : 150, loss : 0.6508, accuracy : 83.06\n",
      "iteration : 200, loss : 0.6438, accuracy : 82.34\n",
      "iteration : 250, loss : 0.6377, accuracy : 83.25\n",
      "iteration : 300, loss : 0.6388, accuracy : 82.72\n",
      "iteration : 350, loss : 0.6379, accuracy : 82.56\n",
      "Epoch : 190, training loss : 0.6398, training accuracy : 82.67, test loss : 0.1585, test accuracy : 95.66\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.5955, accuracy : 83.45\n",
      "iteration : 100, loss : 0.6066, accuracy : 85.16\n",
      "iteration : 150, loss : 0.6369, accuracy : 84.26\n",
      "iteration : 200, loss : 0.6393, accuracy : 83.57\n",
      "iteration : 250, loss : 0.6372, accuracy : 83.59\n",
      "iteration : 300, loss : 0.6363, accuracy : 83.79\n",
      "iteration : 350, loss : 0.6368, accuracy : 83.97\n",
      "Epoch : 191, training loss : 0.6388, training accuracy : 84.06, test loss : 0.1578, test accuracy : 95.66\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.6318, accuracy : 82.77\n",
      "iteration : 100, loss : 0.6143, accuracy : 84.21\n",
      "iteration : 150, loss : 0.6180, accuracy : 82.64\n",
      "iteration : 200, loss : 0.6161, accuracy : 83.33\n",
      "iteration : 250, loss : 0.6273, accuracy : 82.63\n",
      "iteration : 300, loss : 0.6304, accuracy : 82.14\n",
      "iteration : 350, loss : 0.6399, accuracy : 81.84\n",
      "Epoch : 192, training loss : 0.6399, training accuracy : 82.01, test loss : 0.1599, test accuracy : 95.61\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.6307, accuracy : 87.98\n",
      "iteration : 100, loss : 0.6545, accuracy : 86.20\n",
      "iteration : 150, loss : 0.6589, accuracy : 84.89\n",
      "iteration : 200, loss : 0.6540, accuracy : 84.67\n",
      "iteration : 250, loss : 0.6565, accuracy : 84.32\n",
      "iteration : 300, loss : 0.6482, accuracy : 84.83\n",
      "iteration : 350, loss : 0.6470, accuracy : 84.92\n",
      "Epoch : 193, training loss : 0.6440, training accuracy : 84.68, test loss : 0.1598, test accuracy : 95.58\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.6258, accuracy : 83.42\n",
      "iteration : 100, loss : 0.6430, accuracy : 82.21\n",
      "iteration : 150, loss : 0.6490, accuracy : 80.73\n",
      "iteration : 200, loss : 0.6460, accuracy : 82.39\n",
      "iteration : 250, loss : 0.6375, accuracy : 82.52\n",
      "iteration : 300, loss : 0.6432, accuracy : 82.98\n",
      "iteration : 350, loss : 0.6427, accuracy : 83.90\n",
      "Epoch : 194, training loss : 0.6462, training accuracy : 83.08, test loss : 0.1586, test accuracy : 95.58\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.6222, accuracy : 82.44\n",
      "iteration : 100, loss : 0.6343, accuracy : 82.16\n",
      "iteration : 150, loss : 0.6411, accuracy : 82.97\n",
      "iteration : 200, loss : 0.6498, accuracy : 81.93\n",
      "iteration : 250, loss : 0.6453, accuracy : 82.77\n",
      "iteration : 300, loss : 0.6479, accuracy : 82.73\n",
      "iteration : 350, loss : 0.6575, accuracy : 80.89\n",
      "Epoch : 195, training loss : 0.6583, training accuracy : 80.73, test loss : 0.1584, test accuracy : 95.57\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.6714, accuracy : 80.75\n",
      "iteration : 100, loss : 0.6476, accuracy : 82.96\n",
      "iteration : 150, loss : 0.6531, accuracy : 82.65\n",
      "iteration : 200, loss : 0.6594, accuracy : 82.36\n",
      "iteration : 250, loss : 0.6542, accuracy : 82.76\n",
      "iteration : 300, loss : 0.6533, accuracy : 82.08\n",
      "iteration : 350, loss : 0.6508, accuracy : 82.50\n",
      "Epoch : 196, training loss : 0.6466, training accuracy : 82.59, test loss : 0.1603, test accuracy : 95.55\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.6489, accuracy : 86.75\n",
      "iteration : 100, loss : 0.6367, accuracy : 84.47\n",
      "iteration : 150, loss : 0.6252, accuracy : 83.49\n",
      "iteration : 200, loss : 0.6283, accuracy : 83.98\n",
      "iteration : 250, loss : 0.6256, accuracy : 85.22\n",
      "iteration : 300, loss : 0.6228, accuracy : 84.23\n",
      "iteration : 350, loss : 0.6310, accuracy : 82.99\n",
      "Epoch : 197, training loss : 0.6293, training accuracy : 83.34, test loss : 0.1603, test accuracy : 95.63\n",
      "\n",
      "Epoch: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.6167, accuracy : 83.78\n",
      "iteration : 100, loss : 0.5877, accuracy : 85.17\n",
      "iteration : 150, loss : 0.5957, accuracy : 86.29\n",
      "iteration : 200, loss : 0.6062, accuracy : 84.93\n",
      "iteration : 250, loss : 0.6050, accuracy : 85.28\n",
      "iteration : 300, loss : 0.6065, accuracy : 86.06\n",
      "iteration : 350, loss : 0.6226, accuracy : 84.01\n",
      "Epoch : 198, training loss : 0.6253, training accuracy : 83.95, test loss : 0.1581, test accuracy : 95.67\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.6344, accuracy : 83.72\n",
      "iteration : 100, loss : 0.6192, accuracy : 83.70\n",
      "iteration : 150, loss : 0.6346, accuracy : 84.33\n",
      "iteration : 200, loss : 0.6419, accuracy : 84.39\n",
      "iteration : 250, loss : 0.6421, accuracy : 84.14\n",
      "iteration : 300, loss : 0.6337, accuracy : 84.25\n",
      "iteration : 350, loss : 0.6452, accuracy : 82.23\n",
      "Epoch : 199, training loss : 0.6408, training accuracy : 82.95, test loss : 0.1605, test accuracy : 95.59\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list  = []\n",
    "test_loss_list  = []\n",
    "test_acc_list  = []\n",
    "\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "#print(scheduler)\n",
    "for epoch in range(0, 200):\n",
    "    # normal - train, jaco- jaco_train\n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader,scheduler)\n",
    "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
    "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
    "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
    "\n",
    "plt.xlabel(\"N0. of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs No. of epochs\")\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
    "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
    "plt.xlabel(\"NO. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\" Accuracy vs N0. of epochs\")\n",
    "plt.legend(['train', 'test']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
