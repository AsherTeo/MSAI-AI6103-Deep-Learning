{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfGrai_Qt7Ny"
   },
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgAiImV0uURP",
    "outputId": "948f3caa-995f-4d1d-a258-f324158afaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.4), ratio=(0.3, 3.3), value=(125,122,114), inplace=False)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hldipDVsv-Jt"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, criterion, trainloader,scheduler):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    beta = 1\n",
    "    cutmix_prob = 1\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        r = np.random.rand(1)\n",
    "        \n",
    "        if beta > 0 and r < cutmix_prob:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(beta, beta)\n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            # compute output\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "        else:\n",
    "            # compute output\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "            print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VkooK-hQu4a6"
   },
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #inputs.requires_grad = True\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss= criterion(outputs, targets)\n",
    "            #R = reg(inputs, outputs)   # Jacobian regularization\n",
    "            #loss = loss_super + lambda_JR*R # full loss\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jEj8J7xqwAxD"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vlCAjBEWwXNo"
   },
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "from dropblock import DropBlock2D, LinearScheduler\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, drop_prob=0.9, block_size=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.dropblock = LinearScheduler(\n",
    "            DropBlock2D(drop_prob=drop_prob, block_size=block_size),\n",
    "            start_value=0.,\n",
    "            stop_value=drop_prob,\n",
    "            nr_steps=5e3\n",
    "        )\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step() \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        #out = self.layer1(out)\n",
    "        #out = self.layer2(out)\n",
    "        out = self.dropblock(self.layer1(out))\n",
    "        out = self.dropblock(self.layer2(out))\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "ArgupDVRwB8i",
    "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-838de3351049>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut_w = np.int(W * cut_rat)\n",
      "<ipython-input-6-838de3351049>:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut_h = np.int(H * cut_rat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 2.1493, accuracy : 21.42\n",
      "iteration : 100, loss : 2.1186, accuracy : 22.96\n",
      "iteration : 150, loss : 2.0857, accuracy : 24.61\n",
      "iteration : 200, loss : 2.0518, accuracy : 25.96\n",
      "iteration : 250, loss : 2.0286, accuracy : 27.06\n",
      "iteration : 300, loss : 2.0034, accuracy : 27.84\n",
      "iteration : 350, loss : 1.9811, accuracy : 28.71\n",
      "Epoch :   0, training loss : 1.9632, training accuracy : 29.39, test loss : 1.5192, test accuracy : 45.71\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.8836, accuracy : 33.12\n",
      "iteration : 100, loss : 1.8561, accuracy : 33.67\n",
      "iteration : 150, loss : 1.8333, accuracy : 34.55\n",
      "iteration : 200, loss : 1.8324, accuracy : 34.66\n",
      "iteration : 250, loss : 1.8113, accuracy : 35.37\n",
      "iteration : 300, loss : 1.8029, accuracy : 35.87\n",
      "iteration : 350, loss : 1.8023, accuracy : 36.03\n",
      "Epoch :   1, training loss : 1.8012, training accuracy : 36.07, test loss : 1.2856, test accuracy : 53.17\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 1.7431, accuracy : 38.64\n",
      "iteration : 100, loss : 1.7114, accuracy : 40.00\n",
      "iteration : 150, loss : 1.6799, accuracy : 41.25\n",
      "iteration : 200, loss : 1.7001, accuracy : 40.59\n",
      "iteration : 250, loss : 1.6785, accuracy : 40.97\n",
      "iteration : 300, loss : 1.6827, accuracy : 40.78\n",
      "iteration : 350, loss : 1.6875, accuracy : 40.63\n",
      "Epoch :   2, training loss : 1.6803, training accuracy : 40.93, test loss : 1.3068, test accuracy : 54.85\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 1.6353, accuracy : 42.61\n",
      "iteration : 100, loss : 1.6336, accuracy : 42.60\n",
      "iteration : 150, loss : 1.6496, accuracy : 41.86\n",
      "iteration : 200, loss : 1.6466, accuracy : 41.98\n",
      "iteration : 250, loss : 1.6516, accuracy : 41.67\n",
      "iteration : 300, loss : 1.6464, accuracy : 41.78\n",
      "iteration : 350, loss : 1.6507, accuracy : 41.57\n",
      "Epoch :   3, training loss : 1.6539, training accuracy : 41.46, test loss : 1.4701, test accuracy : 51.16\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 1.7017, accuracy : 40.16\n",
      "iteration : 100, loss : 1.6658, accuracy : 41.81\n",
      "iteration : 150, loss : 1.6714, accuracy : 41.53\n",
      "iteration : 200, loss : 1.6782, accuracy : 41.18\n",
      "iteration : 250, loss : 1.6781, accuracy : 41.29\n",
      "iteration : 300, loss : 1.6889, accuracy : 40.95\n",
      "iteration : 350, loss : 1.6748, accuracy : 41.47\n",
      "Epoch :   4, training loss : 1.6693, training accuracy : 41.57, test loss : 1.0938, test accuracy : 61.77\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 1.6514, accuracy : 41.75\n",
      "iteration : 100, loss : 1.6607, accuracy : 41.86\n",
      "iteration : 150, loss : 1.6448, accuracy : 42.43\n",
      "iteration : 200, loss : 1.6493, accuracy : 42.39\n",
      "iteration : 250, loss : 1.6327, accuracy : 42.92\n",
      "iteration : 300, loss : 1.6376, accuracy : 42.82\n",
      "iteration : 350, loss : 1.6419, accuracy : 42.71\n",
      "Epoch :   5, training loss : 1.6358, training accuracy : 42.83, test loss : 1.2410, test accuracy : 57.28\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 1.6272, accuracy : 42.56\n",
      "iteration : 100, loss : 1.6109, accuracy : 43.26\n",
      "iteration : 150, loss : 1.6077, accuracy : 43.49\n",
      "iteration : 200, loss : 1.6185, accuracy : 43.29\n",
      "iteration : 250, loss : 1.6108, accuracy : 43.67\n",
      "iteration : 300, loss : 1.6128, accuracy : 43.75\n",
      "iteration : 350, loss : 1.6242, accuracy : 43.34\n",
      "Epoch :   6, training loss : 1.6250, training accuracy : 43.22, test loss : 1.1944, test accuracy : 57.85\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 1.5679, accuracy : 44.92\n",
      "iteration : 100, loss : 1.5544, accuracy : 45.40\n",
      "iteration : 150, loss : 1.5336, accuracy : 45.80\n",
      "iteration : 200, loss : 1.5438, accuracy : 45.84\n",
      "iteration : 250, loss : 1.5515, accuracy : 45.70\n",
      "iteration : 300, loss : 1.5564, accuracy : 45.72\n",
      "iteration : 350, loss : 1.5649, accuracy : 45.47\n",
      "Epoch :   7, training loss : 1.5594, training accuracy : 45.66, test loss : 1.0910, test accuracy : 61.93\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 1.5333, accuracy : 45.38\n",
      "iteration : 100, loss : 1.5902, accuracy : 44.29\n",
      "iteration : 150, loss : 1.5672, accuracy : 45.00\n",
      "iteration : 200, loss : 1.5565, accuracy : 45.60\n",
      "iteration : 250, loss : 1.5602, accuracy : 45.49\n",
      "iteration : 300, loss : 1.5501, accuracy : 45.72\n",
      "iteration : 350, loss : 1.5478, accuracy : 45.81\n",
      "Epoch :   8, training loss : 1.5466, training accuracy : 45.72, test loss : 1.0614, test accuracy : 61.92\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 1.5548, accuracy : 45.78\n",
      "iteration : 100, loss : 1.5339, accuracy : 46.88\n",
      "iteration : 150, loss : 1.5902, accuracy : 45.06\n",
      "iteration : 200, loss : 1.5794, accuracy : 45.39\n",
      "iteration : 250, loss : 1.5588, accuracy : 46.08\n",
      "iteration : 300, loss : 1.5514, accuracy : 46.36\n",
      "iteration : 350, loss : 1.5447, accuracy : 46.42\n",
      "Epoch :   9, training loss : 1.5413, training accuracy : 46.49, test loss : 1.0626, test accuracy : 63.80\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 1.5049, accuracy : 47.05\n",
      "iteration : 100, loss : 1.4956, accuracy : 47.62\n",
      "iteration : 150, loss : 1.4938, accuracy : 47.65\n",
      "iteration : 200, loss : 1.4801, accuracy : 48.18\n",
      "iteration : 250, loss : 1.4850, accuracy : 48.19\n",
      "iteration : 300, loss : 1.4824, accuracy : 48.26\n",
      "iteration : 350, loss : 1.4853, accuracy : 48.29\n",
      "Epoch :  10, training loss : 1.5069, training accuracy : 47.66, test loss : 1.0129, test accuracy : 66.21\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 1.5126, accuracy : 47.25\n",
      "iteration : 100, loss : 1.4490, accuracy : 49.36\n",
      "iteration : 150, loss : 1.4518, accuracy : 49.48\n",
      "iteration : 200, loss : 1.4653, accuracy : 49.02\n",
      "iteration : 250, loss : 1.4825, accuracy : 48.40\n",
      "iteration : 300, loss : 1.4768, accuracy : 48.48\n",
      "iteration : 350, loss : 1.4645, accuracy : 48.78\n",
      "Epoch :  11, training loss : 1.4649, training accuracy : 48.74, test loss : 0.9666, test accuracy : 66.57\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 1.4803, accuracy : 48.81\n",
      "iteration : 100, loss : 1.4438, accuracy : 49.48\n",
      "iteration : 150, loss : 1.4279, accuracy : 49.99\n",
      "iteration : 200, loss : 1.4419, accuracy : 49.42\n",
      "iteration : 250, loss : 1.4413, accuracy : 49.30\n",
      "iteration : 300, loss : 1.4425, accuracy : 49.28\n",
      "iteration : 350, loss : 1.4424, accuracy : 49.43\n",
      "Epoch :  12, training loss : 1.4404, training accuracy : 49.52, test loss : 1.0709, test accuracy : 64.35\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 1.3819, accuracy : 51.38\n",
      "iteration : 100, loss : 1.3876, accuracy : 51.37\n",
      "iteration : 150, loss : 1.3656, accuracy : 52.04\n",
      "iteration : 200, loss : 1.3917, accuracy : 51.23\n",
      "iteration : 250, loss : 1.3881, accuracy : 51.34\n",
      "iteration : 300, loss : 1.4010, accuracy : 50.89\n",
      "iteration : 350, loss : 1.4060, accuracy : 50.75\n",
      "Epoch :  13, training loss : 1.4076, training accuracy : 50.73, test loss : 1.1183, test accuracy : 62.07\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 1.4042, accuracy : 51.20\n",
      "iteration : 100, loss : 1.4117, accuracy : 50.55\n",
      "iteration : 150, loss : 1.4130, accuracy : 50.40\n",
      "iteration : 200, loss : 1.4052, accuracy : 50.67\n",
      "iteration : 250, loss : 1.4043, accuracy : 50.56\n",
      "iteration : 300, loss : 1.4055, accuracy : 50.59\n",
      "iteration : 350, loss : 1.4081, accuracy : 50.57\n",
      "Epoch :  14, training loss : 1.4094, training accuracy : 50.54, test loss : 0.8712, test accuracy : 69.63\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 1.4391, accuracy : 48.97\n",
      "iteration : 100, loss : 1.4069, accuracy : 50.42\n",
      "iteration : 150, loss : 1.3972, accuracy : 50.71\n",
      "iteration : 200, loss : 1.3941, accuracy : 51.00\n",
      "iteration : 250, loss : 1.3884, accuracy : 51.20\n",
      "iteration : 300, loss : 1.3893, accuracy : 51.12\n",
      "iteration : 350, loss : 1.4015, accuracy : 50.57\n",
      "Epoch :  15, training loss : 1.3937, training accuracy : 50.92, test loss : 0.8579, test accuracy : 70.04\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 1.3363, accuracy : 52.61\n",
      "iteration : 100, loss : 1.3260, accuracy : 52.91\n",
      "iteration : 150, loss : 1.3168, accuracy : 53.50\n",
      "iteration : 200, loss : 1.3377, accuracy : 53.00\n",
      "iteration : 250, loss : 1.3444, accuracy : 52.69\n",
      "iteration : 300, loss : 1.3432, accuracy : 52.71\n",
      "iteration : 350, loss : 1.3466, accuracy : 52.51\n",
      "Epoch :  16, training loss : 1.3467, training accuracy : 52.53, test loss : 0.9681, test accuracy : 66.53\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 1.3005, accuracy : 53.92\n",
      "iteration : 100, loss : 1.3325, accuracy : 53.09\n",
      "iteration : 150, loss : 1.3444, accuracy : 52.80\n",
      "iteration : 200, loss : 1.3258, accuracy : 53.44\n",
      "iteration : 250, loss : 1.3284, accuracy : 53.39\n",
      "iteration : 300, loss : 1.3331, accuracy : 53.20\n",
      "iteration : 350, loss : 1.3267, accuracy : 53.33\n",
      "Epoch :  17, training loss : 1.3248, training accuracy : 53.32, test loss : 0.9388, test accuracy : 66.73\n",
      "\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 1.2860, accuracy : 53.94\n",
      "iteration : 100, loss : 1.2886, accuracy : 54.48\n",
      "iteration : 150, loss : 1.3000, accuracy : 54.02\n",
      "iteration : 200, loss : 1.3015, accuracy : 53.91\n",
      "iteration : 250, loss : 1.3077, accuracy : 53.73\n",
      "iteration : 300, loss : 1.3162, accuracy : 53.54\n",
      "iteration : 350, loss : 1.3049, accuracy : 53.90\n",
      "Epoch :  18, training loss : 1.2966, training accuracy : 54.22, test loss : 0.7847, test accuracy : 72.49\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 1.2551, accuracy : 56.42\n",
      "iteration : 100, loss : 1.2501, accuracy : 56.44\n",
      "iteration : 150, loss : 1.2757, accuracy : 55.56\n",
      "iteration : 200, loss : 1.2799, accuracy : 55.31\n",
      "iteration : 250, loss : 1.2981, accuracy : 54.87\n",
      "iteration : 300, loss : 1.2853, accuracy : 55.20\n",
      "iteration : 350, loss : 1.2949, accuracy : 54.70\n",
      "Epoch :  19, training loss : 1.2917, training accuracy : 54.77, test loss : 0.8446, test accuracy : 70.63\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 1.3421, accuracy : 53.08\n",
      "iteration : 100, loss : 1.2887, accuracy : 55.19\n",
      "iteration : 150, loss : 1.2688, accuracy : 55.75\n",
      "iteration : 200, loss : 1.2865, accuracy : 54.96\n",
      "iteration : 250, loss : 1.2810, accuracy : 55.08\n",
      "iteration : 300, loss : 1.2836, accuracy : 54.96\n",
      "iteration : 350, loss : 1.2849, accuracy : 54.90\n",
      "Epoch :  20, training loss : 1.2764, training accuracy : 55.19, test loss : 0.7804, test accuracy : 72.96\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 1.1952, accuracy : 58.14\n",
      "iteration : 100, loss : 1.1972, accuracy : 57.91\n",
      "iteration : 150, loss : 1.2395, accuracy : 56.55\n",
      "iteration : 200, loss : 1.2441, accuracy : 56.20\n",
      "iteration : 250, loss : 1.2494, accuracy : 55.92\n",
      "iteration : 300, loss : 1.2539, accuracy : 55.66\n",
      "iteration : 350, loss : 1.2486, accuracy : 55.93\n",
      "Epoch :  21, training loss : 1.2418, training accuracy : 56.11, test loss : 0.7996, test accuracy : 72.12\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 1.3428, accuracy : 52.50\n",
      "iteration : 100, loss : 1.3111, accuracy : 53.83\n",
      "iteration : 150, loss : 1.2725, accuracy : 55.08\n",
      "iteration : 200, loss : 1.2661, accuracy : 55.16\n",
      "iteration : 250, loss : 1.2615, accuracy : 55.55\n",
      "iteration : 300, loss : 1.2597, accuracy : 55.70\n",
      "iteration : 350, loss : 1.2509, accuracy : 55.91\n",
      "Epoch :  22, training loss : 1.2500, training accuracy : 56.03, test loss : 0.7482, test accuracy : 73.61\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 1.2408, accuracy : 56.83\n",
      "iteration : 100, loss : 1.2605, accuracy : 56.05\n",
      "iteration : 150, loss : 1.2389, accuracy : 56.54\n",
      "iteration : 200, loss : 1.2381, accuracy : 56.71\n",
      "iteration : 250, loss : 1.2476, accuracy : 56.26\n",
      "iteration : 300, loss : 1.2491, accuracy : 56.09\n",
      "iteration : 350, loss : 1.2492, accuracy : 56.00\n",
      "Epoch :  23, training loss : 1.2483, training accuracy : 56.05, test loss : 0.8185, test accuracy : 71.56\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 1.2286, accuracy : 56.23\n",
      "iteration : 100, loss : 1.2607, accuracy : 55.45\n",
      "iteration : 150, loss : 1.2467, accuracy : 55.77\n",
      "iteration : 200, loss : 1.2366, accuracy : 56.18\n",
      "iteration : 250, loss : 1.2459, accuracy : 55.87\n",
      "iteration : 300, loss : 1.2357, accuracy : 56.31\n",
      "iteration : 350, loss : 1.2323, accuracy : 56.46\n",
      "Epoch :  24, training loss : 1.2309, training accuracy : 56.54, test loss : 0.7703, test accuracy : 73.80\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 1.1742, accuracy : 58.72\n",
      "iteration : 100, loss : 1.1940, accuracy : 57.77\n",
      "iteration : 150, loss : 1.1948, accuracy : 57.58\n",
      "iteration : 200, loss : 1.1800, accuracy : 58.28\n",
      "iteration : 250, loss : 1.1831, accuracy : 58.33\n",
      "iteration : 300, loss : 1.1878, accuracy : 58.14\n",
      "iteration : 350, loss : 1.1865, accuracy : 58.20\n",
      "Epoch :  25, training loss : 1.1817, training accuracy : 58.31, test loss : 0.8277, test accuracy : 71.30\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 1.2444, accuracy : 56.78\n",
      "iteration : 100, loss : 1.2328, accuracy : 56.86\n",
      "iteration : 150, loss : 1.2324, accuracy : 56.67\n",
      "iteration : 200, loss : 1.2175, accuracy : 57.00\n",
      "iteration : 250, loss : 1.1881, accuracy : 57.97\n",
      "iteration : 300, loss : 1.1847, accuracy : 58.15\n",
      "iteration : 350, loss : 1.1884, accuracy : 58.10\n",
      "Epoch :  26, training loss : 1.1902, training accuracy : 57.95, test loss : 0.7189, test accuracy : 75.04\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 1.1920, accuracy : 57.62\n",
      "iteration : 100, loss : 1.1297, accuracy : 59.79\n",
      "iteration : 150, loss : 1.1175, accuracy : 60.27\n",
      "iteration : 200, loss : 1.1379, accuracy : 59.69\n",
      "iteration : 250, loss : 1.1370, accuracy : 59.79\n",
      "iteration : 300, loss : 1.1366, accuracy : 59.72\n",
      "iteration : 350, loss : 1.1437, accuracy : 59.38\n",
      "Epoch :  27, training loss : 1.1400, training accuracy : 59.51, test loss : 0.7053, test accuracy : 75.53\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 1.1755, accuracy : 58.22\n",
      "iteration : 100, loss : 1.1632, accuracy : 58.43\n",
      "iteration : 150, loss : 1.1648, accuracy : 58.43\n",
      "iteration : 200, loss : 1.1830, accuracy : 57.75\n",
      "iteration : 250, loss : 1.1957, accuracy : 57.30\n",
      "iteration : 300, loss : 1.1799, accuracy : 57.90\n",
      "iteration : 350, loss : 1.1732, accuracy : 58.26\n",
      "Epoch :  28, training loss : 1.1633, training accuracy : 58.66, test loss : 0.7007, test accuracy : 75.42\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 1.1564, accuracy : 59.05\n",
      "iteration : 100, loss : 1.1696, accuracy : 58.79\n",
      "iteration : 150, loss : 1.1527, accuracy : 59.32\n",
      "iteration : 200, loss : 1.1435, accuracy : 59.62\n",
      "iteration : 250, loss : 1.1510, accuracy : 59.33\n",
      "iteration : 300, loss : 1.1438, accuracy : 59.50\n",
      "iteration : 350, loss : 1.1464, accuracy : 59.28\n",
      "Epoch :  29, training loss : 1.1519, training accuracy : 59.12, test loss : 0.7916, test accuracy : 72.38\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 1.0995, accuracy : 60.33\n",
      "iteration : 100, loss : 1.1307, accuracy : 59.49\n",
      "iteration : 150, loss : 1.1420, accuracy : 59.07\n",
      "iteration : 200, loss : 1.1338, accuracy : 59.40\n",
      "iteration : 250, loss : 1.1258, accuracy : 59.77\n",
      "iteration : 300, loss : 1.1359, accuracy : 59.44\n",
      "iteration : 350, loss : 1.1438, accuracy : 59.32\n",
      "Epoch :  30, training loss : 1.1577, training accuracy : 58.92, test loss : 0.8827, test accuracy : 69.75\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 1.0802, accuracy : 61.48\n",
      "iteration : 100, loss : 1.1052, accuracy : 60.70\n",
      "iteration : 150, loss : 1.1256, accuracy : 59.81\n",
      "iteration : 200, loss : 1.1037, accuracy : 60.84\n",
      "iteration : 250, loss : 1.1293, accuracy : 60.09\n",
      "iteration : 300, loss : 1.1333, accuracy : 59.95\n",
      "iteration : 350, loss : 1.1307, accuracy : 60.00\n",
      "Epoch :  31, training loss : 1.1264, training accuracy : 60.16, test loss : 0.6739, test accuracy : 76.78\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 1.0927, accuracy : 61.30\n",
      "iteration : 100, loss : 1.1136, accuracy : 60.73\n",
      "iteration : 150, loss : 1.1126, accuracy : 60.43\n",
      "iteration : 200, loss : 1.1194, accuracy : 60.21\n",
      "iteration : 250, loss : 1.1170, accuracy : 60.25\n",
      "iteration : 300, loss : 1.1210, accuracy : 60.12\n",
      "iteration : 350, loss : 1.1152, accuracy : 60.40\n",
      "Epoch :  32, training loss : 1.1150, training accuracy : 60.43, test loss : 0.7110, test accuracy : 75.17\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 1.1731, accuracy : 58.52\n",
      "iteration : 100, loss : 1.1260, accuracy : 60.34\n",
      "iteration : 150, loss : 1.1367, accuracy : 59.81\n",
      "iteration : 200, loss : 1.1399, accuracy : 59.64\n",
      "iteration : 250, loss : 1.1347, accuracy : 59.95\n",
      "iteration : 300, loss : 1.1301, accuracy : 60.05\n",
      "iteration : 350, loss : 1.1308, accuracy : 60.01\n",
      "Epoch :  33, training loss : 1.1291, training accuracy : 60.04, test loss : 0.7023, test accuracy : 75.52\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 1.0986, accuracy : 61.12\n",
      "iteration : 100, loss : 1.0710, accuracy : 62.26\n",
      "iteration : 150, loss : 1.0626, accuracy : 62.36\n",
      "iteration : 200, loss : 1.0687, accuracy : 62.27\n",
      "iteration : 250, loss : 1.0747, accuracy : 62.08\n",
      "iteration : 300, loss : 1.0735, accuracy : 62.09\n",
      "iteration : 350, loss : 1.0700, accuracy : 62.25\n",
      "Epoch :  34, training loss : 1.0696, training accuracy : 62.22, test loss : 0.6072, test accuracy : 78.95\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 1.0348, accuracy : 63.42\n",
      "iteration : 100, loss : 1.0515, accuracy : 62.50\n",
      "iteration : 150, loss : 1.0690, accuracy : 62.06\n",
      "iteration : 200, loss : 1.0689, accuracy : 62.10\n",
      "iteration : 250, loss : 1.0707, accuracy : 61.98\n",
      "iteration : 300, loss : 1.0628, accuracy : 62.17\n",
      "iteration : 350, loss : 1.0654, accuracy : 62.12\n",
      "Epoch :  35, training loss : 1.0638, training accuracy : 62.16, test loss : 0.6513, test accuracy : 76.81\n",
      "\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 1.0893, accuracy : 60.50\n",
      "iteration : 100, loss : 1.0818, accuracy : 61.24\n",
      "iteration : 150, loss : 1.0507, accuracy : 62.77\n",
      "iteration : 200, loss : 1.0498, accuracy : 62.84\n",
      "iteration : 250, loss : 1.0526, accuracy : 62.67\n",
      "iteration : 300, loss : 1.0793, accuracy : 61.76\n",
      "iteration : 350, loss : 1.0706, accuracy : 62.11\n",
      "Epoch :  36, training loss : 1.0708, training accuracy : 62.09, test loss : 0.7328, test accuracy : 74.72\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 1.0084, accuracy : 64.66\n",
      "iteration : 100, loss : 1.0112, accuracy : 64.39\n",
      "iteration : 150, loss : 1.0285, accuracy : 63.71\n",
      "iteration : 200, loss : 1.0421, accuracy : 63.15\n",
      "iteration : 250, loss : 1.0519, accuracy : 62.79\n",
      "iteration : 300, loss : 1.0587, accuracy : 62.55\n",
      "iteration : 350, loss : 1.0494, accuracy : 62.85\n",
      "Epoch :  37, training loss : 1.0559, training accuracy : 62.66, test loss : 0.7235, test accuracy : 75.09\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 1.1370, accuracy : 59.81\n",
      "iteration : 100, loss : 1.0672, accuracy : 62.02\n",
      "iteration : 150, loss : 1.1028, accuracy : 60.85\n",
      "iteration : 200, loss : 1.0826, accuracy : 61.52\n",
      "iteration : 250, loss : 1.0750, accuracy : 61.80\n",
      "iteration : 300, loss : 1.0832, accuracy : 61.55\n",
      "iteration : 350, loss : 1.0765, accuracy : 61.79\n",
      "Epoch :  38, training loss : 1.0740, training accuracy : 61.93, test loss : 0.7149, test accuracy : 74.78\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 1.0264, accuracy : 63.22\n",
      "iteration : 100, loss : 1.0388, accuracy : 62.92\n",
      "iteration : 150, loss : 1.0107, accuracy : 63.98\n",
      "iteration : 200, loss : 1.0343, accuracy : 63.32\n",
      "iteration : 250, loss : 1.0326, accuracy : 63.36\n",
      "iteration : 300, loss : 1.0265, accuracy : 63.52\n",
      "iteration : 350, loss : 1.0283, accuracy : 63.39\n",
      "Epoch :  39, training loss : 1.0316, training accuracy : 63.34, test loss : 0.6145, test accuracy : 79.15\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 1.0030, accuracy : 63.84\n",
      "iteration : 100, loss : 1.0332, accuracy : 63.05\n",
      "iteration : 150, loss : 1.0330, accuracy : 63.20\n",
      "iteration : 200, loss : 1.0327, accuracy : 63.13\n",
      "iteration : 250, loss : 1.0390, accuracy : 62.98\n",
      "iteration : 300, loss : 1.0418, accuracy : 62.96\n",
      "iteration : 350, loss : 1.0287, accuracy : 63.57\n",
      "Epoch :  40, training loss : 1.0318, training accuracy : 63.51, test loss : 0.6166, test accuracy : 78.24\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 1.1290, accuracy : 59.83\n",
      "iteration : 100, loss : 1.0724, accuracy : 62.14\n",
      "iteration : 150, loss : 1.0835, accuracy : 61.72\n",
      "iteration : 200, loss : 1.0809, accuracy : 61.79\n",
      "iteration : 250, loss : 1.0702, accuracy : 62.15\n",
      "iteration : 300, loss : 1.0634, accuracy : 62.42\n",
      "iteration : 350, loss : 1.0668, accuracy : 62.25\n",
      "Epoch :  41, training loss : 1.0593, training accuracy : 62.57, test loss : 0.5841, test accuracy : 80.08\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.9755, accuracy : 65.55\n",
      "iteration : 100, loss : 1.0226, accuracy : 63.88\n",
      "iteration : 150, loss : 1.0473, accuracy : 63.31\n",
      "iteration : 200, loss : 1.0581, accuracy : 62.91\n",
      "iteration : 250, loss : 1.0448, accuracy : 63.30\n",
      "iteration : 300, loss : 1.0468, accuracy : 63.24\n",
      "iteration : 350, loss : 1.0380, accuracy : 63.53\n",
      "Epoch :  42, training loss : 1.0426, training accuracy : 63.39, test loss : 0.6444, test accuracy : 77.24\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.9942, accuracy : 65.22\n",
      "iteration : 100, loss : 1.0213, accuracy : 64.01\n",
      "iteration : 150, loss : 1.0311, accuracy : 63.72\n",
      "iteration : 200, loss : 1.0212, accuracy : 63.98\n",
      "iteration : 250, loss : 1.0187, accuracy : 64.08\n",
      "iteration : 300, loss : 1.0155, accuracy : 64.12\n",
      "iteration : 350, loss : 1.0143, accuracy : 64.15\n",
      "Epoch :  43, training loss : 1.0091, training accuracy : 64.31, test loss : 0.7142, test accuracy : 75.40\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 1.0533, accuracy : 62.75\n",
      "iteration : 100, loss : 1.0385, accuracy : 62.94\n",
      "iteration : 150, loss : 1.0250, accuracy : 63.45\n",
      "iteration : 200, loss : 1.0266, accuracy : 63.54\n",
      "iteration : 250, loss : 1.0266, accuracy : 63.45\n",
      "iteration : 300, loss : 1.0186, accuracy : 63.79\n",
      "iteration : 350, loss : 1.0103, accuracy : 64.16\n",
      "Epoch :  44, training loss : 1.0140, training accuracy : 64.05, test loss : 0.6168, test accuracy : 78.73\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.9950, accuracy : 65.14\n",
      "iteration : 100, loss : 1.0528, accuracy : 62.60\n",
      "iteration : 150, loss : 1.0369, accuracy : 63.20\n",
      "iteration : 200, loss : 1.0419, accuracy : 63.14\n",
      "iteration : 250, loss : 1.0382, accuracy : 63.31\n",
      "iteration : 300, loss : 1.0279, accuracy : 63.64\n",
      "iteration : 350, loss : 1.0314, accuracy : 63.56\n",
      "Epoch :  45, training loss : 1.0343, training accuracy : 63.55, test loss : 0.6876, test accuracy : 76.06\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.9598, accuracy : 66.41\n",
      "iteration : 100, loss : 0.9544, accuracy : 66.30\n",
      "iteration : 150, loss : 0.9870, accuracy : 65.07\n",
      "iteration : 200, loss : 0.9932, accuracy : 64.78\n",
      "iteration : 250, loss : 1.0039, accuracy : 64.39\n",
      "iteration : 300, loss : 0.9935, accuracy : 64.76\n",
      "iteration : 350, loss : 0.9867, accuracy : 65.01\n",
      "Epoch :  46, training loss : 0.9891, training accuracy : 64.94, test loss : 0.6063, test accuracy : 78.71\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.9470, accuracy : 66.27\n",
      "iteration : 100, loss : 0.9533, accuracy : 66.01\n",
      "iteration : 150, loss : 0.9450, accuracy : 66.45\n",
      "iteration : 200, loss : 0.9690, accuracy : 65.81\n",
      "iteration : 250, loss : 0.9779, accuracy : 65.48\n",
      "iteration : 300, loss : 0.9881, accuracy : 65.05\n",
      "iteration : 350, loss : 0.9985, accuracy : 64.67\n",
      "Epoch :  47, training loss : 1.0025, training accuracy : 64.55, test loss : 0.5810, test accuracy : 79.86\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.9855, accuracy : 64.89\n",
      "iteration : 100, loss : 1.0077, accuracy : 64.45\n",
      "iteration : 150, loss : 1.0167, accuracy : 64.11\n",
      "iteration : 200, loss : 1.0009, accuracy : 64.68\n",
      "iteration : 250, loss : 0.9908, accuracy : 64.97\n",
      "iteration : 300, loss : 0.9830, accuracy : 65.22\n",
      "iteration : 350, loss : 0.9913, accuracy : 64.92\n",
      "Epoch :  48, training loss : 0.9970, training accuracy : 64.61, test loss : 0.6271, test accuracy : 78.42\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.9390, accuracy : 66.17\n",
      "iteration : 100, loss : 0.9788, accuracy : 65.00\n",
      "iteration : 150, loss : 0.9681, accuracy : 65.70\n",
      "iteration : 200, loss : 0.9766, accuracy : 65.45\n",
      "iteration : 250, loss : 0.9754, accuracy : 65.52\n",
      "iteration : 300, loss : 0.9732, accuracy : 65.67\n",
      "iteration : 350, loss : 0.9832, accuracy : 65.27\n",
      "Epoch :  49, training loss : 0.9867, training accuracy : 65.15, test loss : 0.6471, test accuracy : 76.91\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 1.0132, accuracy : 63.94\n",
      "iteration : 100, loss : 0.9911, accuracy : 64.94\n",
      "iteration : 150, loss : 0.9963, accuracy : 64.80\n",
      "iteration : 200, loss : 0.9979, accuracy : 64.75\n",
      "iteration : 250, loss : 1.0009, accuracy : 64.62\n",
      "iteration : 300, loss : 0.9930, accuracy : 64.90\n",
      "iteration : 350, loss : 0.9933, accuracy : 64.89\n",
      "Epoch :  50, training loss : 0.9855, training accuracy : 65.18, test loss : 0.5954, test accuracy : 79.37\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.9852, accuracy : 65.20\n",
      "iteration : 100, loss : 0.9823, accuracy : 65.28\n",
      "iteration : 150, loss : 0.9801, accuracy : 65.35\n",
      "iteration : 200, loss : 0.9617, accuracy : 66.07\n",
      "iteration : 250, loss : 0.9727, accuracy : 65.61\n",
      "iteration : 300, loss : 0.9666, accuracy : 65.82\n",
      "iteration : 350, loss : 0.9806, accuracy : 65.42\n",
      "Epoch :  51, training loss : 0.9814, training accuracy : 65.38, test loss : 0.6781, test accuracy : 75.57\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.9939, accuracy : 64.41\n",
      "iteration : 100, loss : 0.9703, accuracy : 65.37\n",
      "iteration : 150, loss : 0.9679, accuracy : 65.55\n",
      "iteration : 200, loss : 0.9629, accuracy : 65.75\n",
      "iteration : 250, loss : 0.9611, accuracy : 65.74\n",
      "iteration : 300, loss : 0.9720, accuracy : 65.55\n",
      "iteration : 350, loss : 0.9714, accuracy : 65.65\n",
      "Epoch :  52, training loss : 0.9688, training accuracy : 65.74, test loss : 0.6015, test accuracy : 78.78\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.9582, accuracy : 66.84\n",
      "iteration : 100, loss : 0.9973, accuracy : 65.01\n",
      "iteration : 150, loss : 0.9661, accuracy : 66.22\n",
      "iteration : 200, loss : 1.0021, accuracy : 64.91\n",
      "iteration : 250, loss : 0.9982, accuracy : 65.01\n",
      "iteration : 300, loss : 1.0001, accuracy : 64.89\n",
      "iteration : 350, loss : 0.9941, accuracy : 65.03\n",
      "Epoch :  53, training loss : 1.0015, training accuracy : 64.68, test loss : 0.5769, test accuracy : 80.19\n",
      "\n",
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.9288, accuracy : 67.66\n",
      "iteration : 100, loss : 1.0043, accuracy : 64.76\n",
      "iteration : 150, loss : 0.9734, accuracy : 65.84\n",
      "iteration : 200, loss : 0.9626, accuracy : 66.19\n",
      "iteration : 250, loss : 0.9557, accuracy : 66.47\n",
      "iteration : 300, loss : 0.9623, accuracy : 66.11\n",
      "iteration : 350, loss : 0.9713, accuracy : 65.79\n",
      "Epoch :  54, training loss : 0.9672, training accuracy : 65.95, test loss : 0.5889, test accuracy : 80.16\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.9019, accuracy : 67.45\n",
      "iteration : 100, loss : 0.9135, accuracy : 67.52\n",
      "iteration : 150, loss : 0.9571, accuracy : 65.97\n",
      "iteration : 200, loss : 0.9438, accuracy : 66.54\n",
      "iteration : 250, loss : 0.9444, accuracy : 66.60\n",
      "iteration : 300, loss : 0.9432, accuracy : 66.74\n",
      "iteration : 350, loss : 0.9487, accuracy : 66.58\n",
      "Epoch :  55, training loss : 0.9503, training accuracy : 66.53, test loss : 0.5318, test accuracy : 81.24\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.9891, accuracy : 64.50\n",
      "iteration : 100, loss : 0.9557, accuracy : 65.91\n",
      "iteration : 150, loss : 0.9480, accuracy : 66.20\n",
      "iteration : 200, loss : 0.9523, accuracy : 66.09\n",
      "iteration : 250, loss : 0.9461, accuracy : 66.58\n",
      "iteration : 300, loss : 0.9610, accuracy : 66.11\n",
      "iteration : 350, loss : 0.9576, accuracy : 66.21\n",
      "Epoch :  56, training loss : 0.9576, training accuracy : 66.17, test loss : 0.5435, test accuracy : 81.26\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.7944, accuracy : 71.94\n",
      "iteration : 100, loss : 0.9130, accuracy : 67.93\n",
      "iteration : 150, loss : 0.9000, accuracy : 68.27\n",
      "iteration : 200, loss : 0.9364, accuracy : 67.02\n",
      "iteration : 250, loss : 0.9369, accuracy : 66.89\n",
      "iteration : 300, loss : 0.9379, accuracy : 66.80\n",
      "iteration : 350, loss : 0.9351, accuracy : 66.82\n",
      "Epoch :  57, training loss : 0.9401, training accuracy : 66.70, test loss : 0.6533, test accuracy : 77.27\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.9014, accuracy : 67.73\n",
      "iteration : 100, loss : 0.9268, accuracy : 66.78\n",
      "iteration : 150, loss : 0.9415, accuracy : 66.38\n",
      "iteration : 200, loss : 0.9456, accuracy : 66.34\n",
      "iteration : 250, loss : 0.9367, accuracy : 66.73\n",
      "iteration : 300, loss : 0.9408, accuracy : 66.58\n",
      "iteration : 350, loss : 0.9457, accuracy : 66.40\n",
      "Epoch :  58, training loss : 0.9484, training accuracy : 66.35, test loss : 0.6227, test accuracy : 78.93\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.8968, accuracy : 68.47\n",
      "iteration : 100, loss : 0.9414, accuracy : 67.23\n",
      "iteration : 150, loss : 0.9437, accuracy : 66.89\n",
      "iteration : 200, loss : 0.9478, accuracy : 66.74\n",
      "iteration : 250, loss : 0.9365, accuracy : 67.15\n",
      "iteration : 300, loss : 0.9368, accuracy : 67.12\n",
      "iteration : 350, loss : 0.9398, accuracy : 66.91\n",
      "Epoch :  59, training loss : 0.9453, training accuracy : 66.76, test loss : 0.6885, test accuracy : 77.70\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.9635, accuracy : 65.66\n",
      "iteration : 100, loss : 0.9218, accuracy : 67.27\n",
      "iteration : 150, loss : 0.9182, accuracy : 67.42\n",
      "iteration : 200, loss : 0.9187, accuracy : 67.57\n",
      "iteration : 250, loss : 0.9258, accuracy : 67.42\n",
      "iteration : 300, loss : 0.9226, accuracy : 67.59\n",
      "iteration : 350, loss : 0.9121, accuracy : 67.94\n",
      "Epoch :  60, training loss : 0.9112, training accuracy : 68.00, test loss : 0.5194, test accuracy : 82.00\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.9644, accuracy : 66.17\n",
      "iteration : 100, loss : 0.9336, accuracy : 67.20\n",
      "iteration : 150, loss : 0.9339, accuracy : 67.32\n",
      "iteration : 200, loss : 0.9604, accuracy : 66.28\n",
      "iteration : 250, loss : 0.9499, accuracy : 66.61\n",
      "iteration : 300, loss : 0.9444, accuracy : 66.88\n",
      "iteration : 350, loss : 0.9314, accuracy : 67.34\n",
      "Epoch :  61, training loss : 0.9268, training accuracy : 67.49, test loss : 0.6010, test accuracy : 79.06\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.9105, accuracy : 67.84\n",
      "iteration : 100, loss : 0.9117, accuracy : 67.71\n",
      "iteration : 150, loss : 0.9080, accuracy : 67.94\n",
      "iteration : 200, loss : 0.9277, accuracy : 67.37\n",
      "iteration : 250, loss : 0.9286, accuracy : 67.25\n",
      "iteration : 300, loss : 0.9357, accuracy : 67.07\n",
      "iteration : 350, loss : 0.9412, accuracy : 66.83\n",
      "Epoch :  62, training loss : 0.9413, training accuracy : 66.84, test loss : 0.5798, test accuracy : 80.21\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.9328, accuracy : 66.58\n",
      "iteration : 100, loss : 0.9074, accuracy : 67.69\n",
      "iteration : 150, loss : 0.8907, accuracy : 68.40\n",
      "iteration : 200, loss : 0.8889, accuracy : 68.46\n",
      "iteration : 250, loss : 0.8946, accuracy : 68.28\n",
      "iteration : 300, loss : 0.9032, accuracy : 68.03\n",
      "iteration : 350, loss : 0.8985, accuracy : 68.30\n",
      "Epoch :  63, training loss : 0.9033, training accuracy : 68.18, test loss : 0.5437, test accuracy : 82.07\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.9674, accuracy : 65.77\n",
      "iteration : 100, loss : 0.9408, accuracy : 66.75\n",
      "iteration : 150, loss : 0.9588, accuracy : 66.16\n",
      "iteration : 200, loss : 0.9523, accuracy : 66.48\n",
      "iteration : 250, loss : 0.9390, accuracy : 66.84\n",
      "iteration : 300, loss : 0.9340, accuracy : 67.09\n",
      "iteration : 350, loss : 0.9378, accuracy : 66.97\n",
      "Epoch :  64, training loss : 0.9385, training accuracy : 66.89, test loss : 0.5830, test accuracy : 79.87\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.9296, accuracy : 67.31\n",
      "iteration : 100, loss : 0.9370, accuracy : 67.27\n",
      "iteration : 150, loss : 0.9275, accuracy : 67.49\n",
      "iteration : 200, loss : 0.9256, accuracy : 67.47\n",
      "iteration : 250, loss : 0.9306, accuracy : 67.30\n",
      "iteration : 300, loss : 0.9317, accuracy : 67.11\n",
      "iteration : 350, loss : 0.9259, accuracy : 67.31\n",
      "Epoch :  65, training loss : 0.9190, training accuracy : 67.58, test loss : 0.5131, test accuracy : 82.55\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.8189, accuracy : 71.64\n",
      "iteration : 100, loss : 0.8650, accuracy : 69.81\n",
      "iteration : 150, loss : 0.8561, accuracy : 69.89\n",
      "iteration : 200, loss : 0.8684, accuracy : 69.38\n",
      "iteration : 250, loss : 0.8721, accuracy : 69.17\n",
      "iteration : 300, loss : 0.8879, accuracy : 68.55\n",
      "iteration : 350, loss : 0.8873, accuracy : 68.60\n",
      "Epoch :  66, training loss : 0.8909, training accuracy : 68.55, test loss : 0.6687, test accuracy : 77.21\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.9406, accuracy : 66.50\n",
      "iteration : 100, loss : 0.9584, accuracy : 65.96\n",
      "iteration : 150, loss : 0.9595, accuracy : 66.07\n",
      "iteration : 200, loss : 0.9812, accuracy : 65.31\n",
      "iteration : 250, loss : 0.9614, accuracy : 66.03\n",
      "iteration : 300, loss : 0.9442, accuracy : 66.70\n",
      "iteration : 350, loss : 0.9428, accuracy : 66.73\n",
      "Epoch :  67, training loss : 0.9323, training accuracy : 67.15, test loss : 0.5596, test accuracy : 80.75\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.9582, accuracy : 66.00\n",
      "iteration : 100, loss : 0.9021, accuracy : 68.31\n",
      "iteration : 150, loss : 0.9090, accuracy : 68.11\n",
      "iteration : 200, loss : 0.9152, accuracy : 67.82\n",
      "iteration : 250, loss : 0.9406, accuracy : 66.82\n",
      "iteration : 300, loss : 0.9352, accuracy : 67.10\n",
      "iteration : 350, loss : 0.9284, accuracy : 67.35\n",
      "Epoch :  68, training loss : 0.9227, training accuracy : 67.50, test loss : 0.5666, test accuracy : 80.83\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.9097, accuracy : 67.55\n",
      "iteration : 100, loss : 0.8817, accuracy : 68.81\n",
      "iteration : 150, loss : 0.9187, accuracy : 67.65\n",
      "iteration : 200, loss : 0.9216, accuracy : 67.58\n",
      "iteration : 250, loss : 0.9187, accuracy : 67.62\n",
      "iteration : 300, loss : 0.9133, accuracy : 67.82\n",
      "iteration : 350, loss : 0.9099, accuracy : 67.90\n",
      "Epoch :  69, training loss : 0.9054, training accuracy : 68.02, test loss : 0.5152, test accuracy : 83.28\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.9179, accuracy : 67.39\n",
      "iteration : 100, loss : 0.9033, accuracy : 67.95\n",
      "iteration : 150, loss : 0.9144, accuracy : 67.40\n",
      "iteration : 200, loss : 0.9171, accuracy : 67.51\n",
      "iteration : 250, loss : 0.9188, accuracy : 67.51\n",
      "iteration : 300, loss : 0.9158, accuracy : 67.73\n",
      "iteration : 350, loss : 0.9103, accuracy : 67.91\n",
      "Epoch :  70, training loss : 0.9097, training accuracy : 67.88, test loss : 0.5359, test accuracy : 82.62\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.8703, accuracy : 68.84\n",
      "iteration : 100, loss : 0.9227, accuracy : 67.24\n",
      "iteration : 150, loss : 0.9270, accuracy : 67.32\n",
      "iteration : 200, loss : 0.9077, accuracy : 67.93\n",
      "iteration : 250, loss : 0.8965, accuracy : 68.53\n",
      "iteration : 300, loss : 0.8819, accuracy : 69.03\n",
      "iteration : 350, loss : 0.8914, accuracy : 68.61\n",
      "Epoch :  71, training loss : 0.8890, training accuracy : 68.71, test loss : 0.5453, test accuracy : 80.99\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.8810, accuracy : 68.72\n",
      "iteration : 100, loss : 0.9072, accuracy : 67.93\n",
      "iteration : 150, loss : 0.8863, accuracy : 68.65\n",
      "iteration : 200, loss : 0.8788, accuracy : 68.96\n",
      "iteration : 250, loss : 0.9012, accuracy : 68.14\n",
      "iteration : 300, loss : 0.8937, accuracy : 68.41\n",
      "iteration : 350, loss : 0.9001, accuracy : 68.28\n",
      "Epoch :  72, training loss : 0.9035, training accuracy : 68.21, test loss : 0.4925, test accuracy : 83.56\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.8461, accuracy : 70.06\n",
      "iteration : 100, loss : 0.9143, accuracy : 67.83\n",
      "iteration : 150, loss : 0.9072, accuracy : 67.98\n",
      "iteration : 200, loss : 0.9105, accuracy : 67.78\n",
      "iteration : 250, loss : 0.9018, accuracy : 68.09\n",
      "iteration : 300, loss : 0.8986, accuracy : 68.21\n",
      "iteration : 350, loss : 0.8855, accuracy : 68.69\n",
      "Epoch :  73, training loss : 0.8951, training accuracy : 68.39, test loss : 0.6112, test accuracy : 79.54\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.8291, accuracy : 70.61\n",
      "iteration : 100, loss : 0.8616, accuracy : 69.48\n",
      "iteration : 150, loss : 0.8860, accuracy : 68.55\n",
      "iteration : 200, loss : 0.8661, accuracy : 69.24\n",
      "iteration : 250, loss : 0.8746, accuracy : 68.86\n",
      "iteration : 300, loss : 0.8712, accuracy : 69.05\n",
      "iteration : 350, loss : 0.8668, accuracy : 69.40\n",
      "Epoch :  74, training loss : 0.8662, training accuracy : 69.47, test loss : 0.6226, test accuracy : 79.21\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.8860, accuracy : 68.86\n",
      "iteration : 100, loss : 0.8965, accuracy : 68.25\n",
      "iteration : 150, loss : 0.8613, accuracy : 69.67\n",
      "iteration : 200, loss : 0.8869, accuracy : 68.71\n",
      "iteration : 250, loss : 0.8850, accuracy : 68.68\n",
      "iteration : 300, loss : 0.8892, accuracy : 68.55\n",
      "iteration : 350, loss : 0.8885, accuracy : 68.65\n",
      "Epoch :  75, training loss : 0.8905, training accuracy : 68.64, test loss : 0.5284, test accuracy : 82.26\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.9487, accuracy : 67.06\n",
      "iteration : 100, loss : 0.8880, accuracy : 69.12\n",
      "iteration : 150, loss : 0.8573, accuracy : 70.00\n",
      "iteration : 200, loss : 0.8727, accuracy : 69.31\n",
      "iteration : 250, loss : 0.8943, accuracy : 68.49\n",
      "iteration : 300, loss : 0.8937, accuracy : 68.58\n",
      "iteration : 350, loss : 0.9109, accuracy : 68.00\n",
      "Epoch :  76, training loss : 0.9103, training accuracy : 68.06, test loss : 0.5584, test accuracy : 81.11\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.8771, accuracy : 68.81\n",
      "iteration : 100, loss : 0.8848, accuracy : 68.85\n",
      "iteration : 150, loss : 0.8766, accuracy : 69.24\n",
      "iteration : 200, loss : 0.8964, accuracy : 68.54\n",
      "iteration : 250, loss : 0.8898, accuracy : 68.74\n",
      "iteration : 300, loss : 0.8670, accuracy : 69.59\n",
      "iteration : 350, loss : 0.8630, accuracy : 69.65\n",
      "Epoch :  77, training loss : 0.8644, training accuracy : 69.60, test loss : 0.5213, test accuracy : 82.65\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.8311, accuracy : 71.09\n",
      "iteration : 100, loss : 0.8727, accuracy : 69.56\n",
      "iteration : 150, loss : 0.9065, accuracy : 68.44\n",
      "iteration : 200, loss : 0.9024, accuracy : 68.43\n",
      "iteration : 250, loss : 0.8921, accuracy : 68.86\n",
      "iteration : 300, loss : 0.8866, accuracy : 69.04\n",
      "iteration : 350, loss : 0.8783, accuracy : 69.29\n",
      "Epoch :  78, training loss : 0.8844, training accuracy : 69.11, test loss : 0.5404, test accuracy : 82.49\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.8536, accuracy : 70.50\n",
      "iteration : 100, loss : 0.8206, accuracy : 71.52\n",
      "iteration : 150, loss : 0.8353, accuracy : 70.81\n",
      "iteration : 200, loss : 0.8475, accuracy : 70.16\n",
      "iteration : 250, loss : 0.8541, accuracy : 69.95\n",
      "iteration : 300, loss : 0.8568, accuracy : 69.89\n",
      "iteration : 350, loss : 0.8505, accuracy : 70.12\n",
      "Epoch :  79, training loss : 0.8491, training accuracy : 70.16, test loss : 0.4946, test accuracy : 83.72\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.8923, accuracy : 68.16\n",
      "iteration : 100, loss : 0.8694, accuracy : 69.16\n",
      "iteration : 150, loss : 0.8760, accuracy : 68.83\n",
      "iteration : 200, loss : 0.8604, accuracy : 69.52\n",
      "iteration : 250, loss : 0.8584, accuracy : 69.58\n",
      "iteration : 300, loss : 0.8462, accuracy : 69.93\n",
      "iteration : 350, loss : 0.8538, accuracy : 69.75\n",
      "Epoch :  80, training loss : 0.8565, training accuracy : 69.65, test loss : 0.5181, test accuracy : 82.49\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.8763, accuracy : 69.44\n",
      "iteration : 100, loss : 0.8855, accuracy : 68.72\n",
      "iteration : 150, loss : 0.8835, accuracy : 68.76\n",
      "iteration : 200, loss : 0.8751, accuracy : 69.03\n",
      "iteration : 250, loss : 0.8664, accuracy : 69.41\n",
      "iteration : 300, loss : 0.8603, accuracy : 69.64\n",
      "iteration : 350, loss : 0.8658, accuracy : 69.44\n",
      "Epoch :  81, training loss : 0.8666, training accuracy : 69.32, test loss : 0.5507, test accuracy : 80.96\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.8719, accuracy : 69.38\n",
      "iteration : 100, loss : 0.8753, accuracy : 69.38\n",
      "iteration : 150, loss : 0.8636, accuracy : 69.73\n",
      "iteration : 200, loss : 0.8655, accuracy : 69.68\n",
      "iteration : 250, loss : 0.8675, accuracy : 69.58\n",
      "iteration : 300, loss : 0.8713, accuracy : 69.39\n",
      "iteration : 350, loss : 0.8629, accuracy : 69.73\n",
      "Epoch :  82, training loss : 0.8673, training accuracy : 69.59, test loss : 0.5510, test accuracy : 82.18\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.8601, accuracy : 69.77\n",
      "iteration : 100, loss : 0.8422, accuracy : 70.16\n",
      "iteration : 150, loss : 0.8434, accuracy : 70.08\n",
      "iteration : 200, loss : 0.8477, accuracy : 69.89\n",
      "iteration : 250, loss : 0.8455, accuracy : 70.09\n",
      "iteration : 300, loss : 0.8496, accuracy : 69.99\n",
      "iteration : 350, loss : 0.8411, accuracy : 70.34\n",
      "Epoch :  83, training loss : 0.8500, training accuracy : 70.03, test loss : 0.4806, test accuracy : 84.01\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.8374, accuracy : 70.03\n",
      "iteration : 100, loss : 0.8422, accuracy : 69.76\n",
      "iteration : 150, loss : 0.8556, accuracy : 69.56\n",
      "iteration : 200, loss : 0.8508, accuracy : 69.71\n",
      "iteration : 250, loss : 0.8625, accuracy : 69.36\n",
      "iteration : 300, loss : 0.8656, accuracy : 69.30\n",
      "iteration : 350, loss : 0.8665, accuracy : 69.36\n",
      "Epoch :  84, training loss : 0.8655, training accuracy : 69.44, test loss : 0.4676, test accuracy : 84.56\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.8825, accuracy : 68.00\n",
      "iteration : 100, loss : 0.8533, accuracy : 69.38\n",
      "iteration : 150, loss : 0.8632, accuracy : 69.10\n",
      "iteration : 200, loss : 0.8380, accuracy : 70.14\n",
      "iteration : 250, loss : 0.8547, accuracy : 69.64\n",
      "iteration : 300, loss : 0.8676, accuracy : 69.10\n",
      "iteration : 350, loss : 0.8613, accuracy : 69.36\n",
      "Epoch :  85, training loss : 0.8537, training accuracy : 69.73, test loss : 0.5319, test accuracy : 82.41\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.8921, accuracy : 68.89\n",
      "iteration : 100, loss : 0.8989, accuracy : 68.15\n",
      "iteration : 150, loss : 0.8547, accuracy : 69.66\n",
      "iteration : 200, loss : 0.8694, accuracy : 69.14\n",
      "iteration : 250, loss : 0.8684, accuracy : 69.13\n",
      "iteration : 300, loss : 0.8624, accuracy : 69.40\n",
      "iteration : 350, loss : 0.8757, accuracy : 68.94\n",
      "Epoch :  86, training loss : 0.8853, training accuracy : 68.61, test loss : 0.4844, test accuracy : 84.86\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.8410, accuracy : 70.22\n",
      "iteration : 100, loss : 0.8241, accuracy : 70.80\n",
      "iteration : 150, loss : 0.8375, accuracy : 70.34\n",
      "iteration : 200, loss : 0.8533, accuracy : 69.83\n",
      "iteration : 250, loss : 0.8464, accuracy : 70.16\n",
      "iteration : 300, loss : 0.8439, accuracy : 70.23\n",
      "iteration : 350, loss : 0.8383, accuracy : 70.43\n",
      "Epoch :  87, training loss : 0.8413, training accuracy : 70.27, test loss : 0.5453, test accuracy : 82.16\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.8164, accuracy : 71.61\n",
      "iteration : 100, loss : 0.8478, accuracy : 70.18\n",
      "iteration : 150, loss : 0.8620, accuracy : 69.54\n",
      "iteration : 200, loss : 0.8591, accuracy : 69.81\n",
      "iteration : 250, loss : 0.8454, accuracy : 70.28\n",
      "iteration : 300, loss : 0.8451, accuracy : 70.32\n",
      "iteration : 350, loss : 0.8416, accuracy : 70.34\n",
      "Epoch :  88, training loss : 0.8407, training accuracy : 70.41, test loss : 0.5424, test accuracy : 82.00\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.8219, accuracy : 71.00\n",
      "iteration : 100, loss : 0.7972, accuracy : 71.82\n",
      "iteration : 150, loss : 0.8193, accuracy : 71.19\n",
      "iteration : 200, loss : 0.8034, accuracy : 71.73\n",
      "iteration : 250, loss : 0.8083, accuracy : 71.62\n",
      "iteration : 300, loss : 0.8172, accuracy : 71.35\n",
      "iteration : 350, loss : 0.8209, accuracy : 71.23\n",
      "Epoch :  89, training loss : 0.8263, training accuracy : 71.02, test loss : 0.4655, test accuracy : 84.96\n",
      "\n",
      "Epoch: 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.8917, accuracy : 68.77\n",
      "iteration : 100, loss : 0.8938, accuracy : 68.80\n",
      "iteration : 150, loss : 0.8783, accuracy : 69.19\n",
      "iteration : 200, loss : 0.8733, accuracy : 69.20\n",
      "iteration : 250, loss : 0.8677, accuracy : 69.33\n",
      "iteration : 300, loss : 0.8616, accuracy : 69.51\n",
      "iteration : 350, loss : 0.8502, accuracy : 69.88\n",
      "Epoch :  90, training loss : 0.8458, training accuracy : 70.06, test loss : 0.5293, test accuracy : 82.58\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.8287, accuracy : 70.00\n",
      "iteration : 100, loss : 0.8212, accuracy : 70.66\n",
      "iteration : 150, loss : 0.7914, accuracy : 71.86\n",
      "iteration : 200, loss : 0.7921, accuracy : 71.96\n",
      "iteration : 250, loss : 0.8025, accuracy : 71.60\n",
      "iteration : 300, loss : 0.7934, accuracy : 71.93\n",
      "iteration : 350, loss : 0.7998, accuracy : 71.70\n",
      "Epoch :  91, training loss : 0.8026, training accuracy : 71.65, test loss : 0.5215, test accuracy : 83.12\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.8295, accuracy : 70.81\n",
      "iteration : 100, loss : 0.8184, accuracy : 71.17\n",
      "iteration : 150, loss : 0.8070, accuracy : 71.53\n",
      "iteration : 200, loss : 0.8116, accuracy : 71.30\n",
      "iteration : 250, loss : 0.8169, accuracy : 71.08\n",
      "iteration : 300, loss : 0.8164, accuracy : 71.12\n",
      "iteration : 350, loss : 0.8150, accuracy : 71.23\n",
      "Epoch :  92, training loss : 0.8148, training accuracy : 71.19, test loss : 0.4916, test accuracy : 83.29\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.8125, accuracy : 70.91\n",
      "iteration : 100, loss : 0.8065, accuracy : 71.27\n",
      "iteration : 150, loss : 0.8104, accuracy : 71.53\n",
      "iteration : 200, loss : 0.8059, accuracy : 71.73\n",
      "iteration : 250, loss : 0.8205, accuracy : 71.13\n",
      "iteration : 300, loss : 0.8193, accuracy : 71.14\n",
      "iteration : 350, loss : 0.8239, accuracy : 71.01\n",
      "Epoch :  93, training loss : 0.8205, training accuracy : 71.10, test loss : 0.4875, test accuracy : 84.35\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.7925, accuracy : 72.20\n",
      "iteration : 100, loss : 0.8333, accuracy : 70.63\n",
      "iteration : 150, loss : 0.8451, accuracy : 70.41\n",
      "iteration : 200, loss : 0.8347, accuracy : 70.80\n",
      "iteration : 250, loss : 0.8292, accuracy : 71.00\n",
      "iteration : 300, loss : 0.8299, accuracy : 70.95\n",
      "iteration : 350, loss : 0.8296, accuracy : 70.84\n",
      "Epoch :  94, training loss : 0.8257, training accuracy : 71.01, test loss : 0.4706, test accuracy : 84.99\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.8249, accuracy : 71.41\n",
      "iteration : 100, loss : 0.8562, accuracy : 70.17\n",
      "iteration : 150, loss : 0.8466, accuracy : 70.29\n",
      "iteration : 200, loss : 0.8520, accuracy : 70.05\n",
      "iteration : 250, loss : 0.8593, accuracy : 69.84\n",
      "iteration : 300, loss : 0.8602, accuracy : 69.73\n",
      "iteration : 350, loss : 0.8477, accuracy : 70.15\n",
      "Epoch :  95, training loss : 0.8551, training accuracy : 69.89, test loss : 0.6263, test accuracy : 79.84\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.8441, accuracy : 70.14\n",
      "iteration : 100, loss : 0.8169, accuracy : 71.20\n",
      "iteration : 150, loss : 0.8423, accuracy : 70.26\n",
      "iteration : 200, loss : 0.8315, accuracy : 70.59\n",
      "iteration : 250, loss : 0.8259, accuracy : 70.72\n",
      "iteration : 300, loss : 0.8289, accuracy : 70.66\n",
      "iteration : 350, loss : 0.8252, accuracy : 70.78\n",
      "Epoch :  96, training loss : 0.8277, training accuracy : 70.71, test loss : 0.4098, test accuracy : 86.95\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.8936, accuracy : 68.28\n",
      "iteration : 100, loss : 0.8491, accuracy : 69.98\n",
      "iteration : 150, loss : 0.8155, accuracy : 71.17\n",
      "iteration : 200, loss : 0.8106, accuracy : 71.34\n",
      "iteration : 250, loss : 0.8068, accuracy : 71.54\n",
      "iteration : 300, loss : 0.8017, accuracy : 71.78\n",
      "iteration : 350, loss : 0.8042, accuracy : 71.70\n",
      "Epoch :  97, training loss : 0.8034, training accuracy : 71.73, test loss : 0.4721, test accuracy : 84.72\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.7851, accuracy : 72.06\n",
      "iteration : 100, loss : 0.8125, accuracy : 71.16\n",
      "iteration : 150, loss : 0.8074, accuracy : 71.43\n",
      "iteration : 200, loss : 0.8213, accuracy : 71.02\n",
      "iteration : 250, loss : 0.8239, accuracy : 70.99\n",
      "iteration : 300, loss : 0.8244, accuracy : 70.96\n",
      "iteration : 350, loss : 0.8242, accuracy : 70.93\n",
      "Epoch :  98, training loss : 0.8205, training accuracy : 71.03, test loss : 0.4180, test accuracy : 86.30\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.8216, accuracy : 71.33\n",
      "iteration : 100, loss : 0.8341, accuracy : 70.79\n",
      "iteration : 150, loss : 0.8071, accuracy : 71.82\n",
      "iteration : 200, loss : 0.8115, accuracy : 71.63\n",
      "iteration : 250, loss : 0.8039, accuracy : 71.75\n",
      "iteration : 300, loss : 0.8142, accuracy : 71.39\n",
      "iteration : 350, loss : 0.8104, accuracy : 71.52\n",
      "Epoch :  99, training loss : 0.8099, training accuracy : 71.59, test loss : 0.4222, test accuracy : 85.54\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.7800, accuracy : 72.88\n",
      "iteration : 100, loss : 0.7921, accuracy : 72.36\n",
      "iteration : 150, loss : 0.8036, accuracy : 71.84\n",
      "iteration : 200, loss : 0.8331, accuracy : 70.83\n",
      "iteration : 250, loss : 0.8255, accuracy : 71.05\n",
      "iteration : 300, loss : 0.8189, accuracy : 71.33\n",
      "iteration : 350, loss : 0.8167, accuracy : 71.37\n",
      "Epoch : 100, training loss : 0.8193, training accuracy : 71.20, test loss : 0.4765, test accuracy : 84.49\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.7973, accuracy : 71.84\n",
      "iteration : 100, loss : 0.7999, accuracy : 71.63\n",
      "iteration : 150, loss : 0.7937, accuracy : 71.96\n",
      "iteration : 200, loss : 0.7809, accuracy : 72.53\n",
      "iteration : 250, loss : 0.7927, accuracy : 71.92\n",
      "iteration : 300, loss : 0.7926, accuracy : 71.89\n",
      "iteration : 350, loss : 0.7900, accuracy : 72.08\n",
      "Epoch : 101, training loss : 0.7904, training accuracy : 72.07, test loss : 0.4136, test accuracy : 86.59\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.7953, accuracy : 71.81\n",
      "iteration : 100, loss : 0.8009, accuracy : 71.68\n",
      "iteration : 150, loss : 0.7654, accuracy : 73.09\n",
      "iteration : 200, loss : 0.7845, accuracy : 72.52\n",
      "iteration : 250, loss : 0.8070, accuracy : 71.68\n",
      "iteration : 300, loss : 0.8039, accuracy : 71.77\n",
      "iteration : 350, loss : 0.8032, accuracy : 71.88\n",
      "Epoch : 102, training loss : 0.8001, training accuracy : 72.02, test loss : 0.4680, test accuracy : 84.96\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.8058, accuracy : 71.09\n",
      "iteration : 100, loss : 0.8270, accuracy : 70.59\n",
      "iteration : 150, loss : 0.8266, accuracy : 70.82\n",
      "iteration : 200, loss : 0.8360, accuracy : 70.52\n",
      "iteration : 250, loss : 0.8149, accuracy : 71.33\n",
      "iteration : 300, loss : 0.8156, accuracy : 71.18\n",
      "iteration : 350, loss : 0.8285, accuracy : 70.73\n",
      "Epoch : 103, training loss : 0.8435, training accuracy : 70.14, test loss : 0.5059, test accuracy : 84.14\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.8129, accuracy : 71.94\n",
      "iteration : 100, loss : 0.7781, accuracy : 72.71\n",
      "iteration : 150, loss : 0.7646, accuracy : 73.27\n",
      "iteration : 200, loss : 0.7582, accuracy : 73.64\n",
      "iteration : 250, loss : 0.7593, accuracy : 73.37\n",
      "iteration : 300, loss : 0.7730, accuracy : 72.86\n",
      "iteration : 350, loss : 0.7798, accuracy : 72.64\n",
      "Epoch : 104, training loss : 0.7840, training accuracy : 72.48, test loss : 0.4074, test accuracy : 86.70\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.8282, accuracy : 71.17\n",
      "iteration : 100, loss : 0.7961, accuracy : 72.43\n",
      "iteration : 150, loss : 0.7896, accuracy : 72.58\n",
      "iteration : 200, loss : 0.7906, accuracy : 72.50\n",
      "iteration : 250, loss : 0.7937, accuracy : 72.34\n",
      "iteration : 300, loss : 0.7922, accuracy : 72.37\n",
      "iteration : 350, loss : 0.7947, accuracy : 72.29\n",
      "Epoch : 105, training loss : 0.7926, training accuracy : 72.32, test loss : 0.4192, test accuracy : 86.24\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.8173, accuracy : 70.52\n",
      "iteration : 100, loss : 0.7841, accuracy : 72.30\n",
      "iteration : 150, loss : 0.7918, accuracy : 72.04\n",
      "iteration : 200, loss : 0.7687, accuracy : 72.83\n",
      "iteration : 250, loss : 0.7735, accuracy : 72.66\n",
      "iteration : 300, loss : 0.7838, accuracy : 72.39\n",
      "iteration : 350, loss : 0.7849, accuracy : 72.31\n",
      "Epoch : 106, training loss : 0.7912, training accuracy : 72.12, test loss : 0.4549, test accuracy : 85.72\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.9111, accuracy : 67.89\n",
      "iteration : 100, loss : 0.8199, accuracy : 71.03\n",
      "iteration : 150, loss : 0.8040, accuracy : 71.62\n",
      "iteration : 200, loss : 0.7812, accuracy : 72.42\n",
      "iteration : 250, loss : 0.7762, accuracy : 72.59\n",
      "iteration : 300, loss : 0.7717, accuracy : 72.82\n",
      "iteration : 350, loss : 0.7708, accuracy : 72.81\n",
      "Epoch : 107, training loss : 0.7779, training accuracy : 72.58, test loss : 0.4800, test accuracy : 84.93\n",
      "\n",
      "Epoch: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.8352, accuracy : 70.70\n",
      "iteration : 100, loss : 0.7907, accuracy : 71.92\n",
      "iteration : 150, loss : 0.7849, accuracy : 72.27\n",
      "iteration : 200, loss : 0.7914, accuracy : 72.17\n",
      "iteration : 250, loss : 0.7935, accuracy : 72.15\n",
      "iteration : 300, loss : 0.7885, accuracy : 72.45\n",
      "iteration : 350, loss : 0.7820, accuracy : 72.67\n",
      "Epoch : 108, training loss : 0.7784, training accuracy : 72.75, test loss : 0.4273, test accuracy : 85.49\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.8207, accuracy : 71.25\n",
      "iteration : 100, loss : 0.7979, accuracy : 72.05\n",
      "iteration : 150, loss : 0.7903, accuracy : 72.15\n",
      "iteration : 200, loss : 0.7862, accuracy : 72.22\n",
      "iteration : 250, loss : 0.7807, accuracy : 72.55\n",
      "iteration : 300, loss : 0.7815, accuracy : 72.58\n",
      "iteration : 350, loss : 0.7804, accuracy : 72.58\n",
      "Epoch : 109, training loss : 0.7702, training accuracy : 72.94, test loss : 0.4141, test accuracy : 85.94\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.8026, accuracy : 71.97\n",
      "iteration : 100, loss : 0.7657, accuracy : 73.23\n",
      "iteration : 150, loss : 0.7539, accuracy : 73.76\n",
      "iteration : 200, loss : 0.7491, accuracy : 73.80\n",
      "iteration : 250, loss : 0.7612, accuracy : 73.34\n",
      "iteration : 300, loss : 0.7487, accuracy : 73.79\n",
      "iteration : 350, loss : 0.7475, accuracy : 73.80\n",
      "Epoch : 110, training loss : 0.7533, training accuracy : 73.57, test loss : 0.4001, test accuracy : 86.53\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.7542, accuracy : 73.62\n",
      "iteration : 100, loss : 0.7378, accuracy : 73.99\n",
      "iteration : 150, loss : 0.7814, accuracy : 72.35\n",
      "iteration : 200, loss : 0.7902, accuracy : 72.12\n",
      "iteration : 250, loss : 0.7793, accuracy : 72.57\n",
      "iteration : 300, loss : 0.7701, accuracy : 72.91\n",
      "iteration : 350, loss : 0.7626, accuracy : 73.12\n",
      "Epoch : 111, training loss : 0.7590, training accuracy : 73.27, test loss : 0.3736, test accuracy : 87.61\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.7852, accuracy : 72.39\n",
      "iteration : 100, loss : 0.7925, accuracy : 72.04\n",
      "iteration : 150, loss : 0.7885, accuracy : 72.14\n",
      "iteration : 200, loss : 0.7591, accuracy : 73.08\n",
      "iteration : 250, loss : 0.7516, accuracy : 73.39\n",
      "iteration : 300, loss : 0.7688, accuracy : 72.76\n",
      "iteration : 350, loss : 0.7547, accuracy : 73.28\n",
      "Epoch : 112, training loss : 0.7544, training accuracy : 73.31, test loss : 0.4153, test accuracy : 86.99\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.7574, accuracy : 73.53\n",
      "iteration : 100, loss : 0.7316, accuracy : 74.02\n",
      "iteration : 150, loss : 0.7323, accuracy : 74.16\n",
      "iteration : 200, loss : 0.7546, accuracy : 73.61\n",
      "iteration : 250, loss : 0.7457, accuracy : 74.01\n",
      "iteration : 300, loss : 0.7696, accuracy : 73.12\n",
      "iteration : 350, loss : 0.7634, accuracy : 73.38\n",
      "Epoch : 113, training loss : 0.7614, training accuracy : 73.46, test loss : 0.4065, test accuracy : 86.08\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.7136, accuracy : 75.38\n",
      "iteration : 100, loss : 0.7114, accuracy : 75.05\n",
      "iteration : 150, loss : 0.7081, accuracy : 75.13\n",
      "iteration : 200, loss : 0.7220, accuracy : 74.70\n",
      "iteration : 250, loss : 0.7263, accuracy : 74.63\n",
      "iteration : 300, loss : 0.7313, accuracy : 74.37\n",
      "iteration : 350, loss : 0.7470, accuracy : 73.78\n",
      "Epoch : 114, training loss : 0.7465, training accuracy : 73.79, test loss : 0.3634, test accuracy : 87.48\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.8356, accuracy : 70.52\n",
      "iteration : 100, loss : 0.8151, accuracy : 71.41\n",
      "iteration : 150, loss : 0.8172, accuracy : 71.36\n",
      "iteration : 200, loss : 0.7946, accuracy : 72.07\n",
      "iteration : 250, loss : 0.7874, accuracy : 72.29\n",
      "iteration : 300, loss : 0.7838, accuracy : 72.49\n",
      "iteration : 350, loss : 0.7728, accuracy : 72.82\n",
      "Epoch : 115, training loss : 0.7776, training accuracy : 72.57, test loss : 0.4892, test accuracy : 84.77\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.7255, accuracy : 74.25\n",
      "iteration : 100, loss : 0.7162, accuracy : 74.45\n",
      "iteration : 150, loss : 0.7101, accuracy : 74.84\n",
      "iteration : 200, loss : 0.7258, accuracy : 74.30\n",
      "iteration : 250, loss : 0.7394, accuracy : 73.91\n",
      "iteration : 300, loss : 0.7386, accuracy : 73.93\n",
      "iteration : 350, loss : 0.7380, accuracy : 73.98\n",
      "Epoch : 116, training loss : 0.7447, training accuracy : 73.78, test loss : 0.5272, test accuracy : 83.65\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.8035, accuracy : 71.52\n",
      "iteration : 100, loss : 0.7669, accuracy : 72.78\n",
      "iteration : 150, loss : 0.7591, accuracy : 73.07\n",
      "iteration : 200, loss : 0.7547, accuracy : 73.25\n",
      "iteration : 250, loss : 0.7601, accuracy : 73.03\n",
      "iteration : 300, loss : 0.7454, accuracy : 73.61\n",
      "iteration : 350, loss : 0.7448, accuracy : 73.67\n",
      "Epoch : 117, training loss : 0.7501, training accuracy : 73.53, test loss : 0.4420, test accuracy : 86.01\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.7900, accuracy : 71.88\n",
      "iteration : 100, loss : 0.7431, accuracy : 73.82\n",
      "iteration : 150, loss : 0.7593, accuracy : 73.09\n",
      "iteration : 200, loss : 0.7656, accuracy : 72.88\n",
      "iteration : 250, loss : 0.7642, accuracy : 72.86\n",
      "iteration : 300, loss : 0.7682, accuracy : 72.67\n",
      "iteration : 350, loss : 0.7669, accuracy : 72.70\n",
      "Epoch : 118, training loss : 0.7717, training accuracy : 72.58, test loss : 0.4927, test accuracy : 85.41\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.7092, accuracy : 75.19\n",
      "iteration : 100, loss : 0.7500, accuracy : 73.53\n",
      "iteration : 150, loss : 0.7389, accuracy : 73.86\n",
      "iteration : 200, loss : 0.7458, accuracy : 73.60\n",
      "iteration : 250, loss : 0.7467, accuracy : 73.65\n",
      "iteration : 300, loss : 0.7497, accuracy : 73.66\n",
      "iteration : 350, loss : 0.7418, accuracy : 73.90\n",
      "Epoch : 119, training loss : 0.7400, training accuracy : 73.90, test loss : 0.4420, test accuracy : 85.59\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.7727, accuracy : 72.02\n",
      "iteration : 100, loss : 0.7274, accuracy : 73.87\n",
      "iteration : 150, loss : 0.7460, accuracy : 73.32\n",
      "iteration : 200, loss : 0.7408, accuracy : 73.48\n",
      "iteration : 250, loss : 0.7381, accuracy : 73.74\n",
      "iteration : 300, loss : 0.7408, accuracy : 73.66\n",
      "iteration : 350, loss : 0.7574, accuracy : 73.12\n",
      "Epoch : 120, training loss : 0.7603, training accuracy : 73.01, test loss : 0.3855, test accuracy : 87.49\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.7449, accuracy : 73.25\n",
      "iteration : 100, loss : 0.7076, accuracy : 74.84\n",
      "iteration : 150, loss : 0.6990, accuracy : 75.24\n",
      "iteration : 200, loss : 0.7161, accuracy : 74.65\n",
      "iteration : 250, loss : 0.7406, accuracy : 73.88\n",
      "iteration : 300, loss : 0.7309, accuracy : 74.25\n",
      "iteration : 350, loss : 0.7424, accuracy : 73.83\n",
      "Epoch : 121, training loss : 0.7403, training accuracy : 73.92, test loss : 0.3770, test accuracy : 87.30\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.7620, accuracy : 73.83\n",
      "iteration : 100, loss : 0.7298, accuracy : 74.54\n",
      "iteration : 150, loss : 0.7185, accuracy : 74.76\n",
      "iteration : 200, loss : 0.7306, accuracy : 74.32\n",
      "iteration : 250, loss : 0.7432, accuracy : 73.67\n",
      "iteration : 300, loss : 0.7378, accuracy : 73.87\n",
      "iteration : 350, loss : 0.7367, accuracy : 73.91\n",
      "Epoch : 122, training loss : 0.7368, training accuracy : 73.97, test loss : 0.4568, test accuracy : 85.17\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.7440, accuracy : 74.14\n",
      "iteration : 100, loss : 0.7287, accuracy : 74.48\n",
      "iteration : 150, loss : 0.7450, accuracy : 73.91\n",
      "iteration : 200, loss : 0.7412, accuracy : 73.94\n",
      "iteration : 250, loss : 0.7458, accuracy : 73.88\n",
      "iteration : 300, loss : 0.7417, accuracy : 74.01\n",
      "iteration : 350, loss : 0.7338, accuracy : 74.35\n",
      "Epoch : 123, training loss : 0.7258, training accuracy : 74.63, test loss : 0.3637, test accuracy : 88.13\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.7395, accuracy : 74.09\n",
      "iteration : 100, loss : 0.7108, accuracy : 75.01\n",
      "iteration : 150, loss : 0.7043, accuracy : 75.32\n",
      "iteration : 200, loss : 0.7051, accuracy : 75.26\n",
      "iteration : 250, loss : 0.7087, accuracy : 75.05\n",
      "iteration : 300, loss : 0.7149, accuracy : 74.80\n",
      "iteration : 350, loss : 0.7176, accuracy : 74.59\n",
      "Epoch : 124, training loss : 0.7203, training accuracy : 74.50, test loss : 0.4634, test accuracy : 84.41\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.6907, accuracy : 75.59\n",
      "iteration : 100, loss : 0.7012, accuracy : 75.52\n",
      "iteration : 150, loss : 0.7038, accuracy : 75.45\n",
      "iteration : 200, loss : 0.7147, accuracy : 74.88\n",
      "iteration : 250, loss : 0.7210, accuracy : 74.68\n",
      "iteration : 300, loss : 0.7155, accuracy : 74.84\n",
      "iteration : 350, loss : 0.7196, accuracy : 74.69\n",
      "Epoch : 125, training loss : 0.7125, training accuracy : 74.96, test loss : 0.3557, test accuracy : 88.48\n",
      "\n",
      "Epoch: 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.7332, accuracy : 74.11\n",
      "iteration : 100, loss : 0.7548, accuracy : 73.59\n",
      "iteration : 150, loss : 0.7445, accuracy : 73.94\n",
      "iteration : 200, loss : 0.7267, accuracy : 74.41\n",
      "iteration : 250, loss : 0.7319, accuracy : 74.20\n",
      "iteration : 300, loss : 0.7371, accuracy : 73.92\n",
      "iteration : 350, loss : 0.7321, accuracy : 73.99\n",
      "Epoch : 126, training loss : 0.7271, training accuracy : 74.18, test loss : 0.3817, test accuracy : 87.49\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.6612, accuracy : 76.83\n",
      "iteration : 100, loss : 0.7231, accuracy : 74.59\n",
      "iteration : 150, loss : 0.7207, accuracy : 74.55\n",
      "iteration : 200, loss : 0.7181, accuracy : 74.66\n",
      "iteration : 250, loss : 0.7331, accuracy : 74.08\n",
      "iteration : 300, loss : 0.7296, accuracy : 74.23\n",
      "iteration : 350, loss : 0.7294, accuracy : 74.19\n",
      "Epoch : 127, training loss : 0.7276, training accuracy : 74.28, test loss : 0.3311, test accuracy : 89.17\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.6479, accuracy : 77.25\n",
      "iteration : 100, loss : 0.6865, accuracy : 75.78\n",
      "iteration : 150, loss : 0.7241, accuracy : 74.48\n",
      "iteration : 200, loss : 0.7173, accuracy : 74.78\n",
      "iteration : 250, loss : 0.7050, accuracy : 75.11\n",
      "iteration : 300, loss : 0.7064, accuracy : 75.20\n",
      "iteration : 350, loss : 0.6961, accuracy : 75.50\n",
      "Epoch : 128, training loss : 0.7007, training accuracy : 75.27, test loss : 0.3350, test accuracy : 88.89\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.6478, accuracy : 76.94\n",
      "iteration : 100, loss : 0.6880, accuracy : 75.80\n",
      "iteration : 150, loss : 0.6879, accuracy : 75.62\n",
      "iteration : 200, loss : 0.6918, accuracy : 75.41\n",
      "iteration : 250, loss : 0.7001, accuracy : 75.28\n",
      "iteration : 300, loss : 0.6998, accuracy : 75.30\n",
      "iteration : 350, loss : 0.7016, accuracy : 75.29\n",
      "Epoch : 129, training loss : 0.7065, training accuracy : 75.07, test loss : 0.3578, test accuracy : 88.65\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.6484, accuracy : 77.17\n",
      "iteration : 100, loss : 0.6893, accuracy : 75.65\n",
      "iteration : 150, loss : 0.6841, accuracy : 75.85\n",
      "iteration : 200, loss : 0.6834, accuracy : 75.86\n",
      "iteration : 250, loss : 0.6874, accuracy : 75.66\n",
      "iteration : 300, loss : 0.6960, accuracy : 75.29\n",
      "iteration : 350, loss : 0.7083, accuracy : 74.84\n",
      "Epoch : 130, training loss : 0.7109, training accuracy : 74.77, test loss : 0.3813, test accuracy : 87.57\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.7012, accuracy : 75.12\n",
      "iteration : 100, loss : 0.7109, accuracy : 75.16\n",
      "iteration : 150, loss : 0.7021, accuracy : 75.28\n",
      "iteration : 200, loss : 0.7022, accuracy : 75.18\n",
      "iteration : 250, loss : 0.7022, accuracy : 75.21\n",
      "iteration : 300, loss : 0.7037, accuracy : 75.12\n",
      "iteration : 350, loss : 0.7127, accuracy : 74.81\n",
      "Epoch : 131, training loss : 0.7126, training accuracy : 74.83, test loss : 0.4369, test accuracy : 85.57\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.7983, accuracy : 71.39\n",
      "iteration : 100, loss : 0.7634, accuracy : 72.81\n",
      "iteration : 150, loss : 0.7482, accuracy : 73.27\n",
      "iteration : 200, loss : 0.7227, accuracy : 74.26\n",
      "iteration : 250, loss : 0.7113, accuracy : 74.69\n",
      "iteration : 300, loss : 0.7109, accuracy : 74.78\n",
      "iteration : 350, loss : 0.7105, accuracy : 74.84\n",
      "Epoch : 132, training loss : 0.7134, training accuracy : 74.71, test loss : 0.3696, test accuracy : 88.28\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.6055, accuracy : 78.88\n",
      "iteration : 100, loss : 0.6543, accuracy : 77.09\n",
      "iteration : 150, loss : 0.6453, accuracy : 77.32\n",
      "iteration : 200, loss : 0.6499, accuracy : 77.16\n",
      "iteration : 250, loss : 0.6633, accuracy : 76.67\n",
      "iteration : 300, loss : 0.6774, accuracy : 76.11\n",
      "iteration : 350, loss : 0.6780, accuracy : 76.09\n",
      "Epoch : 133, training loss : 0.6813, training accuracy : 76.00, test loss : 0.3411, test accuracy : 89.18\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.7007, accuracy : 75.47\n",
      "iteration : 100, loss : 0.6470, accuracy : 77.24\n",
      "iteration : 150, loss : 0.6635, accuracy : 76.41\n",
      "iteration : 200, loss : 0.6700, accuracy : 76.36\n",
      "iteration : 250, loss : 0.6793, accuracy : 76.07\n",
      "iteration : 300, loss : 0.6695, accuracy : 76.29\n",
      "iteration : 350, loss : 0.6833, accuracy : 75.83\n",
      "Epoch : 134, training loss : 0.6921, training accuracy : 75.54, test loss : 0.3363, test accuracy : 88.44\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.7085, accuracy : 75.34\n",
      "iteration : 100, loss : 0.6805, accuracy : 76.31\n",
      "iteration : 150, loss : 0.7345, accuracy : 74.39\n",
      "iteration : 200, loss : 0.7279, accuracy : 74.52\n",
      "iteration : 250, loss : 0.7176, accuracy : 74.83\n",
      "iteration : 300, loss : 0.7096, accuracy : 75.11\n",
      "iteration : 350, loss : 0.7028, accuracy : 75.32\n",
      "Epoch : 135, training loss : 0.7013, training accuracy : 75.37, test loss : 0.3487, test accuracy : 88.51\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.6734, accuracy : 76.50\n",
      "iteration : 100, loss : 0.6651, accuracy : 76.84\n",
      "iteration : 150, loss : 0.6837, accuracy : 76.38\n",
      "iteration : 200, loss : 0.6757, accuracy : 76.55\n",
      "iteration : 250, loss : 0.6935, accuracy : 75.88\n",
      "iteration : 300, loss : 0.6919, accuracy : 75.95\n",
      "iteration : 350, loss : 0.6919, accuracy : 75.88\n",
      "Epoch : 136, training loss : 0.6840, training accuracy : 76.11, test loss : 0.3303, test accuracy : 88.54\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.6338, accuracy : 77.78\n",
      "iteration : 100, loss : 0.6509, accuracy : 77.25\n",
      "iteration : 150, loss : 0.6725, accuracy : 76.39\n",
      "iteration : 200, loss : 0.6872, accuracy : 76.02\n",
      "iteration : 250, loss : 0.6979, accuracy : 75.66\n",
      "iteration : 300, loss : 0.6985, accuracy : 75.54\n",
      "iteration : 350, loss : 0.6985, accuracy : 75.53\n",
      "Epoch : 137, training loss : 0.6920, training accuracy : 75.77, test loss : 0.3369, test accuracy : 88.86\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.6772, accuracy : 76.30\n",
      "iteration : 100, loss : 0.6777, accuracy : 76.14\n",
      "iteration : 150, loss : 0.6663, accuracy : 76.52\n",
      "iteration : 200, loss : 0.6544, accuracy : 76.79\n",
      "iteration : 250, loss : 0.6545, accuracy : 76.85\n",
      "iteration : 300, loss : 0.6712, accuracy : 76.22\n",
      "iteration : 350, loss : 0.6679, accuracy : 76.41\n",
      "Epoch : 138, training loss : 0.6697, training accuracy : 76.37, test loss : 0.3234, test accuracy : 89.31\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.6694, accuracy : 76.30\n",
      "iteration : 100, loss : 0.6594, accuracy : 76.36\n",
      "iteration : 150, loss : 0.6904, accuracy : 75.52\n",
      "iteration : 200, loss : 0.6839, accuracy : 75.70\n",
      "iteration : 250, loss : 0.6867, accuracy : 75.72\n",
      "iteration : 300, loss : 0.6764, accuracy : 76.08\n",
      "iteration : 350, loss : 0.6815, accuracy : 75.85\n",
      "Epoch : 139, training loss : 0.6945, training accuracy : 75.38, test loss : 0.3512, test accuracy : 88.33\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.6744, accuracy : 76.89\n",
      "iteration : 100, loss : 0.6520, accuracy : 77.31\n",
      "iteration : 150, loss : 0.6296, accuracy : 77.95\n",
      "iteration : 200, loss : 0.6324, accuracy : 77.86\n",
      "iteration : 250, loss : 0.6322, accuracy : 77.84\n",
      "iteration : 300, loss : 0.6438, accuracy : 77.35\n",
      "iteration : 350, loss : 0.6432, accuracy : 77.34\n",
      "Epoch : 140, training loss : 0.6556, training accuracy : 77.00, test loss : 0.3502, test accuracy : 88.45\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.6597, accuracy : 76.12\n",
      "iteration : 100, loss : 0.6431, accuracy : 76.96\n",
      "iteration : 150, loss : 0.6580, accuracy : 76.60\n",
      "iteration : 200, loss : 0.6285, accuracy : 77.77\n",
      "iteration : 250, loss : 0.6287, accuracy : 77.74\n",
      "iteration : 300, loss : 0.6388, accuracy : 77.33\n",
      "iteration : 350, loss : 0.6463, accuracy : 77.04\n",
      "Epoch : 141, training loss : 0.6396, training accuracy : 77.34, test loss : 0.3178, test accuracy : 89.19\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.6875, accuracy : 75.81\n",
      "iteration : 100, loss : 0.6481, accuracy : 77.21\n",
      "iteration : 150, loss : 0.6388, accuracy : 77.58\n",
      "iteration : 200, loss : 0.6420, accuracy : 77.49\n",
      "iteration : 250, loss : 0.6452, accuracy : 77.32\n",
      "iteration : 300, loss : 0.6519, accuracy : 77.05\n",
      "iteration : 350, loss : 0.6500, accuracy : 77.09\n",
      "Epoch : 142, training loss : 0.6514, training accuracy : 77.08, test loss : 0.4135, test accuracy : 87.30\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.7128, accuracy : 75.31\n",
      "iteration : 100, loss : 0.6348, accuracy : 78.27\n",
      "iteration : 150, loss : 0.6362, accuracy : 78.09\n",
      "iteration : 200, loss : 0.6323, accuracy : 78.00\n",
      "iteration : 250, loss : 0.6396, accuracy : 77.62\n",
      "iteration : 300, loss : 0.6365, accuracy : 77.66\n",
      "iteration : 350, loss : 0.6397, accuracy : 77.53\n",
      "Epoch : 143, training loss : 0.6370, training accuracy : 77.63, test loss : 0.3273, test accuracy : 89.13\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.7109, accuracy : 74.86\n",
      "iteration : 100, loss : 0.7079, accuracy : 75.09\n",
      "iteration : 150, loss : 0.6771, accuracy : 76.29\n",
      "iteration : 200, loss : 0.6676, accuracy : 76.73\n",
      "iteration : 250, loss : 0.6607, accuracy : 76.92\n",
      "iteration : 300, loss : 0.6590, accuracy : 76.89\n",
      "iteration : 350, loss : 0.6506, accuracy : 77.21\n",
      "Epoch : 144, training loss : 0.6445, training accuracy : 77.35, test loss : 0.3512, test accuracy : 88.30\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.6544, accuracy : 76.77\n",
      "iteration : 100, loss : 0.6574, accuracy : 76.49\n",
      "iteration : 150, loss : 0.6540, accuracy : 76.68\n",
      "iteration : 200, loss : 0.6472, accuracy : 77.05\n",
      "iteration : 250, loss : 0.6423, accuracy : 77.16\n",
      "iteration : 300, loss : 0.6423, accuracy : 77.26\n",
      "iteration : 350, loss : 0.6442, accuracy : 77.17\n",
      "Epoch : 145, training loss : 0.6408, training accuracy : 77.23, test loss : 0.3255, test accuracy : 89.10\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.6250, accuracy : 78.25\n",
      "iteration : 100, loss : 0.6360, accuracy : 77.73\n",
      "iteration : 150, loss : 0.6144, accuracy : 78.48\n",
      "iteration : 200, loss : 0.6265, accuracy : 78.02\n",
      "iteration : 250, loss : 0.6423, accuracy : 77.38\n",
      "iteration : 300, loss : 0.6394, accuracy : 77.52\n",
      "iteration : 350, loss : 0.6487, accuracy : 77.17\n",
      "Epoch : 146, training loss : 0.6579, training accuracy : 76.88, test loss : 0.3801, test accuracy : 88.12\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.6425, accuracy : 77.42\n",
      "iteration : 100, loss : 0.6423, accuracy : 77.40\n",
      "iteration : 150, loss : 0.6316, accuracy : 77.74\n",
      "iteration : 200, loss : 0.6379, accuracy : 77.43\n",
      "iteration : 250, loss : 0.6428, accuracy : 77.38\n",
      "iteration : 300, loss : 0.6448, accuracy : 77.27\n",
      "iteration : 350, loss : 0.6452, accuracy : 77.31\n",
      "Epoch : 147, training loss : 0.6418, training accuracy : 77.42, test loss : 0.3063, test accuracy : 89.46\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.6116, accuracy : 78.27\n",
      "iteration : 100, loss : 0.6294, accuracy : 77.55\n",
      "iteration : 150, loss : 0.6830, accuracy : 75.71\n",
      "iteration : 200, loss : 0.6935, accuracy : 75.45\n",
      "iteration : 250, loss : 0.6797, accuracy : 75.96\n",
      "iteration : 300, loss : 0.6634, accuracy : 76.53\n",
      "iteration : 350, loss : 0.6627, accuracy : 76.53\n",
      "Epoch : 148, training loss : 0.6582, training accuracy : 76.71, test loss : 0.3168, test accuracy : 89.49\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.6108, accuracy : 78.55\n",
      "iteration : 100, loss : 0.6283, accuracy : 78.11\n",
      "iteration : 150, loss : 0.6244, accuracy : 78.16\n",
      "iteration : 200, loss : 0.6375, accuracy : 77.67\n",
      "iteration : 250, loss : 0.6476, accuracy : 77.21\n",
      "iteration : 300, loss : 0.6498, accuracy : 77.07\n",
      "iteration : 350, loss : 0.6437, accuracy : 77.23\n",
      "Epoch : 149, training loss : 0.6421, training accuracy : 77.29, test loss : 0.3035, test accuracy : 90.19\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.6785, accuracy : 76.27\n",
      "iteration : 100, loss : 0.6764, accuracy : 75.98\n",
      "iteration : 150, loss : 0.6603, accuracy : 76.49\n",
      "iteration : 200, loss : 0.6681, accuracy : 76.33\n",
      "iteration : 250, loss : 0.6638, accuracy : 76.52\n",
      "iteration : 300, loss : 0.6736, accuracy : 76.23\n",
      "iteration : 350, loss : 0.6607, accuracy : 76.73\n",
      "Epoch : 150, training loss : 0.6640, training accuracy : 76.59, test loss : 0.3354, test accuracy : 89.28\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.6220, accuracy : 77.97\n",
      "iteration : 100, loss : 0.5906, accuracy : 79.19\n",
      "iteration : 150, loss : 0.5698, accuracy : 79.90\n",
      "iteration : 200, loss : 0.5912, accuracy : 79.16\n",
      "iteration : 250, loss : 0.6096, accuracy : 78.49\n",
      "iteration : 300, loss : 0.6083, accuracy : 78.55\n",
      "iteration : 350, loss : 0.6198, accuracy : 78.18\n",
      "Epoch : 151, training loss : 0.6127, training accuracy : 78.48, test loss : 0.3497, test accuracy : 88.44\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.6163, accuracy : 77.89\n",
      "iteration : 100, loss : 0.6488, accuracy : 76.83\n",
      "iteration : 150, loss : 0.6425, accuracy : 77.16\n",
      "iteration : 200, loss : 0.6394, accuracy : 77.38\n",
      "iteration : 250, loss : 0.6249, accuracy : 77.95\n",
      "iteration : 300, loss : 0.6235, accuracy : 77.92\n",
      "iteration : 350, loss : 0.6191, accuracy : 78.06\n",
      "Epoch : 152, training loss : 0.6167, training accuracy : 78.13, test loss : 0.3136, test accuracy : 89.30\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.6565, accuracy : 76.23\n",
      "iteration : 100, loss : 0.6221, accuracy : 77.80\n",
      "iteration : 150, loss : 0.6539, accuracy : 76.72\n",
      "iteration : 200, loss : 0.6459, accuracy : 77.12\n",
      "iteration : 250, loss : 0.6482, accuracy : 77.11\n",
      "iteration : 300, loss : 0.6527, accuracy : 76.84\n",
      "iteration : 350, loss : 0.6448, accuracy : 77.15\n",
      "Epoch : 153, training loss : 0.6365, training accuracy : 77.45, test loss : 0.3028, test accuracy : 89.63\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.5367, accuracy : 80.75\n",
      "iteration : 100, loss : 0.5621, accuracy : 80.20\n",
      "iteration : 150, loss : 0.5854, accuracy : 79.42\n",
      "iteration : 200, loss : 0.5908, accuracy : 79.18\n",
      "iteration : 250, loss : 0.6011, accuracy : 78.86\n",
      "iteration : 300, loss : 0.6221, accuracy : 78.03\n",
      "iteration : 350, loss : 0.6134, accuracy : 78.34\n",
      "Epoch : 154, training loss : 0.6122, training accuracy : 78.39, test loss : 0.3411, test accuracy : 89.00\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.5790, accuracy : 79.38\n",
      "iteration : 100, loss : 0.6043, accuracy : 78.53\n",
      "iteration : 150, loss : 0.6078, accuracy : 78.44\n",
      "iteration : 200, loss : 0.5896, accuracy : 79.16\n",
      "iteration : 250, loss : 0.5955, accuracy : 79.02\n",
      "iteration : 300, loss : 0.6123, accuracy : 78.45\n",
      "iteration : 350, loss : 0.6171, accuracy : 78.22\n",
      "Epoch : 155, training loss : 0.6204, training accuracy : 78.13, test loss : 0.3117, test accuracy : 89.70\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.6452, accuracy : 77.00\n",
      "iteration : 100, loss : 0.6186, accuracy : 78.16\n",
      "iteration : 150, loss : 0.6293, accuracy : 77.83\n",
      "iteration : 200, loss : 0.6235, accuracy : 77.96\n",
      "iteration : 250, loss : 0.6201, accuracy : 78.03\n",
      "iteration : 300, loss : 0.6145, accuracy : 78.24\n",
      "iteration : 350, loss : 0.6119, accuracy : 78.26\n",
      "Epoch : 156, training loss : 0.6061, training accuracy : 78.51, test loss : 0.2868, test accuracy : 90.47\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.6238, accuracy : 78.16\n",
      "iteration : 100, loss : 0.6367, accuracy : 77.63\n",
      "iteration : 150, loss : 0.6213, accuracy : 78.08\n",
      "iteration : 200, loss : 0.6123, accuracy : 78.40\n",
      "iteration : 250, loss : 0.6092, accuracy : 78.53\n",
      "iteration : 300, loss : 0.6008, accuracy : 78.90\n",
      "iteration : 350, loss : 0.5953, accuracy : 79.08\n",
      "Epoch : 157, training loss : 0.6016, training accuracy : 78.81, test loss : 0.3137, test accuracy : 90.03\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.6181, accuracy : 78.36\n",
      "iteration : 100, loss : 0.6207, accuracy : 78.03\n",
      "iteration : 150, loss : 0.6222, accuracy : 77.97\n",
      "iteration : 200, loss : 0.6041, accuracy : 78.67\n",
      "iteration : 250, loss : 0.6173, accuracy : 78.10\n",
      "iteration : 300, loss : 0.6204, accuracy : 77.97\n",
      "iteration : 350, loss : 0.6194, accuracy : 78.02\n",
      "Epoch : 158, training loss : 0.6201, training accuracy : 77.96, test loss : 0.2994, test accuracy : 90.04\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.5985, accuracy : 78.84\n",
      "iteration : 100, loss : 0.6315, accuracy : 77.49\n",
      "iteration : 150, loss : 0.6302, accuracy : 77.55\n",
      "iteration : 200, loss : 0.6208, accuracy : 77.95\n",
      "iteration : 250, loss : 0.6319, accuracy : 77.61\n",
      "iteration : 300, loss : 0.6268, accuracy : 77.77\n",
      "iteration : 350, loss : 0.6114, accuracy : 78.34\n",
      "Epoch : 159, training loss : 0.6071, training accuracy : 78.50, test loss : 0.3110, test accuracy : 89.72\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.6141, accuracy : 78.67\n",
      "iteration : 100, loss : 0.6042, accuracy : 78.89\n",
      "iteration : 150, loss : 0.5949, accuracy : 79.35\n",
      "iteration : 200, loss : 0.5815, accuracy : 79.79\n",
      "iteration : 250, loss : 0.5903, accuracy : 79.37\n",
      "iteration : 300, loss : 0.5857, accuracy : 79.46\n",
      "iteration : 350, loss : 0.5809, accuracy : 79.67\n",
      "Epoch : 160, training loss : 0.5890, training accuracy : 79.35, test loss : 0.2987, test accuracy : 90.11\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.6308, accuracy : 77.78\n",
      "iteration : 100, loss : 0.5805, accuracy : 79.54\n",
      "iteration : 150, loss : 0.5767, accuracy : 79.70\n",
      "iteration : 200, loss : 0.6004, accuracy : 78.90\n",
      "iteration : 250, loss : 0.5955, accuracy : 79.07\n",
      "iteration : 300, loss : 0.5986, accuracy : 78.92\n",
      "iteration : 350, loss : 0.5979, accuracy : 78.91\n",
      "Epoch : 161, training loss : 0.6045, training accuracy : 78.64, test loss : 0.3053, test accuracy : 89.93\n",
      "\n",
      "Epoch: 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.6009, accuracy : 78.94\n",
      "iteration : 100, loss : 0.5804, accuracy : 79.55\n",
      "iteration : 150, loss : 0.5800, accuracy : 79.66\n",
      "iteration : 200, loss : 0.5900, accuracy : 79.36\n",
      "iteration : 250, loss : 0.5993, accuracy : 78.87\n",
      "iteration : 300, loss : 0.6050, accuracy : 78.59\n",
      "iteration : 350, loss : 0.6049, accuracy : 78.61\n",
      "Epoch : 162, training loss : 0.5985, training accuracy : 78.77, test loss : 0.3169, test accuracy : 89.48\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.5580, accuracy : 79.95\n",
      "iteration : 100, loss : 0.5932, accuracy : 79.08\n",
      "iteration : 150, loss : 0.5939, accuracy : 79.07\n",
      "iteration : 200, loss : 0.5877, accuracy : 79.41\n",
      "iteration : 250, loss : 0.5976, accuracy : 79.02\n",
      "iteration : 300, loss : 0.6140, accuracy : 78.36\n",
      "iteration : 350, loss : 0.6149, accuracy : 78.27\n",
      "Epoch : 163, training loss : 0.6107, training accuracy : 78.41, test loss : 0.2656, test accuracy : 91.22\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.5041, accuracy : 82.06\n",
      "iteration : 100, loss : 0.5456, accuracy : 80.81\n",
      "iteration : 150, loss : 0.5517, accuracy : 80.41\n",
      "iteration : 200, loss : 0.5527, accuracy : 80.57\n",
      "iteration : 250, loss : 0.5547, accuracy : 80.47\n",
      "iteration : 300, loss : 0.5725, accuracy : 79.87\n",
      "iteration : 350, loss : 0.5670, accuracy : 80.03\n",
      "Epoch : 164, training loss : 0.5658, training accuracy : 80.07, test loss : 0.2881, test accuracy : 90.57\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.5296, accuracy : 81.59\n",
      "iteration : 100, loss : 0.5470, accuracy : 81.05\n",
      "iteration : 150, loss : 0.5453, accuracy : 80.97\n",
      "iteration : 200, loss : 0.5264, accuracy : 81.64\n",
      "iteration : 250, loss : 0.5265, accuracy : 81.60\n",
      "iteration : 300, loss : 0.5421, accuracy : 80.96\n",
      "iteration : 350, loss : 0.5485, accuracy : 80.74\n",
      "Epoch : 165, training loss : 0.5643, training accuracy : 80.14, test loss : 0.3189, test accuracy : 89.39\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.5739, accuracy : 80.12\n",
      "iteration : 100, loss : 0.6014, accuracy : 78.95\n",
      "iteration : 150, loss : 0.6191, accuracy : 78.13\n",
      "iteration : 200, loss : 0.6214, accuracy : 78.12\n",
      "iteration : 250, loss : 0.6148, accuracy : 78.38\n",
      "iteration : 300, loss : 0.6116, accuracy : 78.59\n",
      "iteration : 350, loss : 0.6218, accuracy : 78.16\n",
      "Epoch : 166, training loss : 0.6111, training accuracy : 78.57, test loss : 0.3016, test accuracy : 90.22\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.5521, accuracy : 80.19\n",
      "iteration : 100, loss : 0.5776, accuracy : 79.40\n",
      "iteration : 150, loss : 0.5615, accuracy : 80.19\n",
      "iteration : 200, loss : 0.5567, accuracy : 80.36\n",
      "iteration : 250, loss : 0.5614, accuracy : 80.12\n",
      "iteration : 300, loss : 0.5791, accuracy : 79.45\n",
      "iteration : 350, loss : 0.5667, accuracy : 79.85\n",
      "Epoch : 167, training loss : 0.5652, training accuracy : 79.92, test loss : 0.2897, test accuracy : 90.13\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.5620, accuracy : 80.78\n",
      "iteration : 100, loss : 0.5326, accuracy : 81.56\n",
      "iteration : 150, loss : 0.5248, accuracy : 81.71\n",
      "iteration : 200, loss : 0.5471, accuracy : 80.86\n",
      "iteration : 250, loss : 0.5615, accuracy : 80.32\n",
      "iteration : 300, loss : 0.5637, accuracy : 80.24\n",
      "iteration : 350, loss : 0.5680, accuracy : 80.09\n",
      "Epoch : 168, training loss : 0.5645, training accuracy : 80.18, test loss : 0.2726, test accuracy : 90.88\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.5155, accuracy : 81.42\n",
      "iteration : 100, loss : 0.5705, accuracy : 79.56\n",
      "iteration : 150, loss : 0.5935, accuracy : 78.72\n",
      "iteration : 200, loss : 0.5826, accuracy : 79.34\n",
      "iteration : 250, loss : 0.5856, accuracy : 79.21\n",
      "iteration : 300, loss : 0.5864, accuracy : 79.15\n",
      "iteration : 350, loss : 0.5870, accuracy : 79.12\n",
      "Epoch : 169, training loss : 0.5880, training accuracy : 79.09, test loss : 0.2763, test accuracy : 90.84\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.5547, accuracy : 80.56\n",
      "iteration : 100, loss : 0.5387, accuracy : 81.03\n",
      "iteration : 150, loss : 0.5504, accuracy : 80.55\n",
      "iteration : 200, loss : 0.5711, accuracy : 79.75\n",
      "iteration : 250, loss : 0.5731, accuracy : 79.74\n",
      "iteration : 300, loss : 0.5671, accuracy : 80.01\n",
      "iteration : 350, loss : 0.5733, accuracy : 79.69\n",
      "Epoch : 170, training loss : 0.5728, training accuracy : 79.70, test loss : 0.2854, test accuracy : 90.75\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.6033, accuracy : 78.38\n",
      "iteration : 100, loss : 0.5807, accuracy : 79.38\n",
      "iteration : 150, loss : 0.5815, accuracy : 79.44\n",
      "iteration : 200, loss : 0.5699, accuracy : 79.99\n",
      "iteration : 250, loss : 0.5929, accuracy : 79.20\n",
      "iteration : 300, loss : 0.5955, accuracy : 79.08\n",
      "iteration : 350, loss : 0.5793, accuracy : 79.62\n",
      "Epoch : 171, training loss : 0.5771, training accuracy : 79.74, test loss : 0.2769, test accuracy : 90.66\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.5004, accuracy : 82.72\n",
      "iteration : 100, loss : 0.5557, accuracy : 80.56\n",
      "iteration : 150, loss : 0.5603, accuracy : 80.22\n",
      "iteration : 200, loss : 0.5709, accuracy : 79.93\n",
      "iteration : 250, loss : 0.5507, accuracy : 80.61\n",
      "iteration : 300, loss : 0.5571, accuracy : 80.33\n",
      "iteration : 350, loss : 0.5519, accuracy : 80.47\n",
      "Epoch : 172, training loss : 0.5535, training accuracy : 80.37, test loss : 0.2700, test accuracy : 90.89\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.5324, accuracy : 80.98\n",
      "iteration : 100, loss : 0.5442, accuracy : 80.93\n",
      "iteration : 150, loss : 0.5343, accuracy : 81.24\n",
      "iteration : 200, loss : 0.5295, accuracy : 81.35\n",
      "iteration : 250, loss : 0.5515, accuracy : 80.46\n",
      "iteration : 300, loss : 0.5654, accuracy : 79.84\n",
      "iteration : 350, loss : 0.5588, accuracy : 80.19\n",
      "Epoch : 173, training loss : 0.5597, training accuracy : 80.24, test loss : 0.2881, test accuracy : 90.70\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.4586, accuracy : 83.89\n",
      "iteration : 100, loss : 0.5176, accuracy : 81.82\n",
      "iteration : 150, loss : 0.5171, accuracy : 81.78\n",
      "iteration : 200, loss : 0.5372, accuracy : 81.16\n",
      "iteration : 250, loss : 0.5398, accuracy : 81.10\n",
      "iteration : 300, loss : 0.5463, accuracy : 80.86\n",
      "iteration : 350, loss : 0.5512, accuracy : 80.71\n",
      "Epoch : 174, training loss : 0.5525, training accuracy : 80.66, test loss : 0.2788, test accuracy : 90.84\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.5621, accuracy : 80.44\n",
      "iteration : 100, loss : 0.5524, accuracy : 80.54\n",
      "iteration : 150, loss : 0.5509, accuracy : 80.44\n",
      "iteration : 200, loss : 0.5486, accuracy : 80.60\n",
      "iteration : 250, loss : 0.5528, accuracy : 80.49\n",
      "iteration : 300, loss : 0.5429, accuracy : 80.82\n",
      "iteration : 350, loss : 0.5501, accuracy : 80.56\n",
      "Epoch : 175, training loss : 0.5621, training accuracy : 80.12, test loss : 0.2755, test accuracy : 91.02\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.5324, accuracy : 81.25\n",
      "iteration : 100, loss : 0.5207, accuracy : 81.41\n",
      "iteration : 150, loss : 0.5502, accuracy : 80.47\n",
      "iteration : 200, loss : 0.5672, accuracy : 79.85\n",
      "iteration : 250, loss : 0.5761, accuracy : 79.68\n",
      "iteration : 300, loss : 0.5738, accuracy : 79.74\n",
      "iteration : 350, loss : 0.5769, accuracy : 79.68\n",
      "Epoch : 176, training loss : 0.5747, training accuracy : 79.75, test loss : 0.2997, test accuracy : 90.38\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.5710, accuracy : 80.05\n",
      "iteration : 100, loss : 0.5223, accuracy : 81.91\n",
      "iteration : 150, loss : 0.5165, accuracy : 82.04\n",
      "iteration : 200, loss : 0.5224, accuracy : 81.90\n",
      "iteration : 250, loss : 0.5294, accuracy : 81.63\n",
      "iteration : 300, loss : 0.5382, accuracy : 81.31\n",
      "iteration : 350, loss : 0.5393, accuracy : 81.16\n",
      "Epoch : 177, training loss : 0.5288, training accuracy : 81.49, test loss : 0.2630, test accuracy : 91.04\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.5157, accuracy : 82.00\n",
      "iteration : 100, loss : 0.5157, accuracy : 82.09\n",
      "iteration : 150, loss : 0.5535, accuracy : 80.71\n",
      "iteration : 200, loss : 0.5588, accuracy : 80.29\n",
      "iteration : 250, loss : 0.5692, accuracy : 79.91\n",
      "iteration : 300, loss : 0.5641, accuracy : 80.06\n",
      "iteration : 350, loss : 0.5660, accuracy : 79.96\n",
      "Epoch : 178, training loss : 0.5644, training accuracy : 80.00, test loss : 0.2890, test accuracy : 90.27\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.5646, accuracy : 80.08\n",
      "iteration : 100, loss : 0.5839, accuracy : 79.51\n",
      "iteration : 150, loss : 0.5702, accuracy : 80.01\n",
      "iteration : 200, loss : 0.5722, accuracy : 79.93\n",
      "iteration : 250, loss : 0.5661, accuracy : 80.20\n",
      "iteration : 300, loss : 0.5618, accuracy : 80.30\n",
      "iteration : 350, loss : 0.5574, accuracy : 80.47\n",
      "Epoch : 179, training loss : 0.5526, training accuracy : 80.67, test loss : 0.2644, test accuracy : 91.07\n",
      "\n",
      "Epoch: 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.5004, accuracy : 82.19\n",
      "iteration : 100, loss : 0.5429, accuracy : 81.10\n",
      "iteration : 150, loss : 0.5467, accuracy : 80.83\n",
      "iteration : 200, loss : 0.5457, accuracy : 80.91\n",
      "iteration : 250, loss : 0.5336, accuracy : 81.28\n",
      "iteration : 300, loss : 0.5366, accuracy : 81.20\n",
      "iteration : 350, loss : 0.5374, accuracy : 81.16\n",
      "Epoch : 180, training loss : 0.5323, training accuracy : 81.31, test loss : 0.2597, test accuracy : 91.26\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.5335, accuracy : 81.41\n",
      "iteration : 100, loss : 0.5237, accuracy : 81.73\n",
      "iteration : 150, loss : 0.5120, accuracy : 82.17\n",
      "iteration : 200, loss : 0.5069, accuracy : 82.25\n",
      "iteration : 250, loss : 0.5239, accuracy : 81.61\n",
      "iteration : 300, loss : 0.5310, accuracy : 81.36\n",
      "iteration : 350, loss : 0.5318, accuracy : 81.28\n",
      "Epoch : 181, training loss : 0.5288, training accuracy : 81.35, test loss : 0.2505, test accuracy : 91.58\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.5398, accuracy : 81.34\n",
      "iteration : 100, loss : 0.5566, accuracy : 80.65\n",
      "iteration : 150, loss : 0.5614, accuracy : 80.47\n",
      "iteration : 200, loss : 0.5462, accuracy : 80.89\n",
      "iteration : 250, loss : 0.5229, accuracy : 81.78\n",
      "iteration : 300, loss : 0.5335, accuracy : 81.35\n",
      "iteration : 350, loss : 0.5232, accuracy : 81.68\n",
      "Epoch : 182, training loss : 0.5335, training accuracy : 81.29, test loss : 0.2816, test accuracy : 90.68\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.5251, accuracy : 82.03\n",
      "iteration : 100, loss : 0.5225, accuracy : 82.00\n",
      "iteration : 150, loss : 0.5145, accuracy : 82.31\n",
      "iteration : 200, loss : 0.5380, accuracy : 81.25\n",
      "iteration : 250, loss : 0.5486, accuracy : 80.84\n",
      "iteration : 300, loss : 0.5436, accuracy : 81.07\n",
      "iteration : 350, loss : 0.5519, accuracy : 80.70\n",
      "Epoch : 183, training loss : 0.5460, training accuracy : 80.87, test loss : 0.2567, test accuracy : 91.41\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.5053, accuracy : 82.53\n",
      "iteration : 100, loss : 0.5377, accuracy : 80.96\n",
      "iteration : 150, loss : 0.5503, accuracy : 80.52\n",
      "iteration : 200, loss : 0.5455, accuracy : 80.59\n",
      "iteration : 250, loss : 0.5338, accuracy : 81.08\n",
      "iteration : 300, loss : 0.5284, accuracy : 81.22\n",
      "iteration : 350, loss : 0.5339, accuracy : 81.08\n",
      "Epoch : 184, training loss : 0.5355, training accuracy : 81.01, test loss : 0.2531, test accuracy : 91.58\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.5559, accuracy : 80.28\n",
      "iteration : 100, loss : 0.5654, accuracy : 80.03\n",
      "iteration : 150, loss : 0.5277, accuracy : 81.36\n",
      "iteration : 200, loss : 0.5326, accuracy : 81.27\n",
      "iteration : 250, loss : 0.5483, accuracy : 80.80\n",
      "iteration : 300, loss : 0.5371, accuracy : 81.20\n",
      "iteration : 350, loss : 0.5430, accuracy : 81.01\n",
      "Epoch : 185, training loss : 0.5360, training accuracy : 81.25, test loss : 0.2491, test accuracy : 91.75\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.5742, accuracy : 79.75\n",
      "iteration : 100, loss : 0.5583, accuracy : 80.37\n",
      "iteration : 150, loss : 0.5556, accuracy : 80.43\n",
      "iteration : 200, loss : 0.5467, accuracy : 80.61\n",
      "iteration : 250, loss : 0.5455, accuracy : 80.57\n",
      "iteration : 300, loss : 0.5451, accuracy : 80.51\n",
      "iteration : 350, loss : 0.5430, accuracy : 80.65\n",
      "Epoch : 186, training loss : 0.5376, training accuracy : 80.85, test loss : 0.2893, test accuracy : 90.81\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.5222, accuracy : 81.06\n",
      "iteration : 100, loss : 0.5294, accuracy : 80.67\n",
      "iteration : 150, loss : 0.5322, accuracy : 80.79\n",
      "iteration : 200, loss : 0.5367, accuracy : 80.73\n",
      "iteration : 250, loss : 0.5283, accuracy : 81.04\n",
      "iteration : 300, loss : 0.5280, accuracy : 81.13\n",
      "iteration : 350, loss : 0.5313, accuracy : 80.99\n",
      "Epoch : 187, training loss : 0.5363, training accuracy : 80.86, test loss : 0.2549, test accuracy : 91.61\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.5410, accuracy : 80.95\n",
      "iteration : 100, loss : 0.5590, accuracy : 80.19\n",
      "iteration : 150, loss : 0.5602, accuracy : 80.30\n",
      "iteration : 200, loss : 0.5325, accuracy : 81.36\n",
      "iteration : 250, loss : 0.5426, accuracy : 80.92\n",
      "iteration : 300, loss : 0.5332, accuracy : 81.23\n",
      "iteration : 350, loss : 0.5386, accuracy : 80.98\n",
      "Epoch : 188, training loss : 0.5406, training accuracy : 80.95, test loss : 0.2863, test accuracy : 90.90\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.5944, accuracy : 79.06\n",
      "iteration : 100, loss : 0.5828, accuracy : 79.55\n",
      "iteration : 150, loss : 0.5842, accuracy : 79.41\n",
      "iteration : 200, loss : 0.5552, accuracy : 80.44\n",
      "iteration : 250, loss : 0.5332, accuracy : 81.23\n",
      "iteration : 300, loss : 0.5258, accuracy : 81.51\n",
      "iteration : 350, loss : 0.5297, accuracy : 81.39\n",
      "Epoch : 189, training loss : 0.5472, training accuracy : 80.70, test loss : 0.2499, test accuracy : 91.74\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.5548, accuracy : 79.86\n",
      "iteration : 100, loss : 0.5714, accuracy : 79.36\n",
      "iteration : 150, loss : 0.5755, accuracy : 79.35\n",
      "iteration : 200, loss : 0.5569, accuracy : 79.95\n",
      "iteration : 250, loss : 0.5592, accuracy : 79.93\n",
      "iteration : 300, loss : 0.5672, accuracy : 79.63\n",
      "iteration : 350, loss : 0.5536, accuracy : 80.21\n",
      "Epoch : 190, training loss : 0.5592, training accuracy : 79.99, test loss : 0.2687, test accuracy : 91.24\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.5035, accuracy : 82.67\n",
      "iteration : 100, loss : 0.5737, accuracy : 79.87\n",
      "iteration : 150, loss : 0.5428, accuracy : 80.89\n",
      "iteration : 200, loss : 0.5397, accuracy : 80.93\n",
      "iteration : 250, loss : 0.5367, accuracy : 81.01\n",
      "iteration : 300, loss : 0.5303, accuracy : 81.23\n",
      "iteration : 350, loss : 0.5408, accuracy : 80.85\n",
      "Epoch : 191, training loss : 0.5419, training accuracy : 80.81, test loss : 0.2829, test accuracy : 90.74\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.6143, accuracy : 78.22\n",
      "iteration : 100, loss : 0.5353, accuracy : 81.14\n",
      "iteration : 150, loss : 0.5362, accuracy : 81.15\n",
      "iteration : 200, loss : 0.5569, accuracy : 80.45\n",
      "iteration : 250, loss : 0.5508, accuracy : 80.56\n",
      "iteration : 300, loss : 0.5383, accuracy : 80.95\n",
      "iteration : 350, loss : 0.5414, accuracy : 80.87\n",
      "Epoch : 192, training loss : 0.5422, training accuracy : 80.85, test loss : 0.2583, test accuracy : 91.29\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.5767, accuracy : 79.98\n",
      "iteration : 100, loss : 0.5525, accuracy : 80.56\n",
      "iteration : 150, loss : 0.5605, accuracy : 80.30\n",
      "iteration : 200, loss : 0.5527, accuracy : 80.63\n",
      "iteration : 250, loss : 0.5457, accuracy : 80.90\n",
      "iteration : 300, loss : 0.5568, accuracy : 80.47\n",
      "iteration : 350, loss : 0.5563, accuracy : 80.50\n",
      "Epoch : 193, training loss : 0.5485, training accuracy : 80.77, test loss : 0.2460, test accuracy : 91.78\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.5247, accuracy : 81.38\n",
      "iteration : 100, loss : 0.5195, accuracy : 81.48\n",
      "iteration : 150, loss : 0.5305, accuracy : 81.29\n",
      "iteration : 200, loss : 0.5252, accuracy : 81.48\n",
      "iteration : 250, loss : 0.5338, accuracy : 81.14\n",
      "iteration : 300, loss : 0.5409, accuracy : 80.90\n",
      "iteration : 350, loss : 0.5505, accuracy : 80.55\n",
      "Epoch : 194, training loss : 0.5486, training accuracy : 80.61, test loss : 0.2637, test accuracy : 91.13\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.4775, accuracy : 82.72\n",
      "iteration : 100, loss : 0.4973, accuracy : 82.36\n",
      "iteration : 150, loss : 0.5035, accuracy : 82.18\n",
      "iteration : 200, loss : 0.5138, accuracy : 81.74\n",
      "iteration : 250, loss : 0.5240, accuracy : 81.38\n",
      "iteration : 300, loss : 0.5105, accuracy : 81.86\n",
      "iteration : 350, loss : 0.5078, accuracy : 81.92\n",
      "Epoch : 195, training loss : 0.5096, training accuracy : 81.84, test loss : 0.2517, test accuracy : 91.62\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.5187, accuracy : 82.03\n",
      "iteration : 100, loss : 0.5192, accuracy : 81.65\n",
      "iteration : 150, loss : 0.5374, accuracy : 81.14\n",
      "iteration : 200, loss : 0.5416, accuracy : 80.91\n",
      "iteration : 250, loss : 0.5461, accuracy : 80.68\n",
      "iteration : 300, loss : 0.5439, accuracy : 80.82\n",
      "iteration : 350, loss : 0.5463, accuracy : 80.71\n",
      "Epoch : 196, training loss : 0.5502, training accuracy : 80.56, test loss : 0.2609, test accuracy : 91.24\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.5461, accuracy : 80.58\n",
      "iteration : 100, loss : 0.5424, accuracy : 80.77\n",
      "iteration : 150, loss : 0.5451, accuracy : 80.69\n",
      "iteration : 200, loss : 0.5601, accuracy : 80.32\n",
      "iteration : 250, loss : 0.5580, accuracy : 80.34\n",
      "iteration : 300, loss : 0.5559, accuracy : 80.46\n",
      "iteration : 350, loss : 0.5599, accuracy : 80.31\n",
      "Epoch : 197, training loss : 0.5588, training accuracy : 80.34, test loss : 0.2534, test accuracy : 91.49\n",
      "\n",
      "Epoch: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.4989, accuracy : 82.31\n",
      "iteration : 100, loss : 0.5689, accuracy : 79.82\n",
      "iteration : 150, loss : 0.5642, accuracy : 80.11\n",
      "iteration : 200, loss : 0.5541, accuracy : 80.34\n",
      "iteration : 250, loss : 0.5565, accuracy : 80.24\n",
      "iteration : 300, loss : 0.5468, accuracy : 80.61\n",
      "iteration : 350, loss : 0.5441, accuracy : 80.71\n",
      "Epoch : 198, training loss : 0.5504, training accuracy : 80.49, test loss : 0.2710, test accuracy : 91.16\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.5578, accuracy : 80.44\n",
      "iteration : 100, loss : 0.5557, accuracy : 80.58\n",
      "iteration : 150, loss : 0.5527, accuracy : 80.44\n",
      "iteration : 200, loss : 0.5615, accuracy : 80.08\n",
      "iteration : 250, loss : 0.5444, accuracy : 80.80\n",
      "iteration : 300, loss : 0.5372, accuracy : 81.07\n",
      "iteration : 350, loss : 0.5306, accuracy : 81.29\n",
      "Epoch : 199, training loss : 0.5316, training accuracy : 81.23, test loss : 0.2897, test accuracy : 90.99\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list  = []\n",
    "test_loss_list  = []\n",
    "test_acc_list  = []\n",
    "\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "#print(scheduler)\n",
    "for epoch in range(0, 200):\n",
    "    # normal - train, jaco- jaco_train, mma - mma_train\n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader,scheduler)\n",
    "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
    "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
    "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
    "\n",
    "plt.xlabel(\"N0. of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs No. of epochs\")\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
    "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
    "plt.xlabel(\"NO. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\" Accuracy vs N0. of epochs\")\n",
    "plt.legend(['train', 'test']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
