{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfGrai_Qt7Ny"
   },
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from swd_optim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgAiImV0uURP",
    "outputId": "948f3caa-995f-4d1d-a258-f324158afaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hldipDVsv-Jt"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, criterion, trainloader,scheduler):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "            print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VkooK-hQu4a6"
   },
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #inputs.requires_grad = True\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss= criterion(outputs, targets)\n",
    "            #R = reg(inputs, outputs)   # Jacobian regularization\n",
    "            #loss = loss_super + lambda_JR*R # full loss\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jEj8J7xqwAxD"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vlCAjBEWwXNo"
   },
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "ArgupDVRwB8i",
    "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "iteration :  50, loss : 1.9694, accuracy : 28.11\n",
      "iteration : 100, loss : 1.8049, accuracy : 33.30\n",
      "iteration : 150, loss : 1.7243, accuracy : 36.07\n",
      "iteration : 200, loss : 1.6477, accuracy : 39.20\n",
      "iteration : 250, loss : 1.5856, accuracy : 41.75\n",
      "iteration : 300, loss : 1.5312, accuracy : 43.76\n",
      "iteration : 350, loss : 1.4833, accuracy : 45.62\n",
      "Epoch :   0, training loss : 1.4470, training accuracy : 46.97, test loss : 1.1887, test accuracy : 57.38\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.0623, accuracy : 62.53\n",
      "iteration : 100, loss : 1.0465, accuracy : 62.69\n",
      "iteration : 150, loss : 1.0371, accuracy : 63.26\n",
      "iteration : 200, loss : 1.0186, accuracy : 63.93\n",
      "iteration : 250, loss : 1.0093, accuracy : 64.03\n",
      "iteration : 300, loss : 0.9968, accuracy : 64.54\n",
      "iteration : 350, loss : 0.9828, accuracy : 65.08\n",
      "Epoch :   1, training loss : 0.9731, training accuracy : 65.45, test loss : 1.1821, test accuracy : 60.65\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 0.8229, accuracy : 71.33\n",
      "iteration : 100, loss : 0.8219, accuracy : 71.19\n",
      "iteration : 150, loss : 0.8200, accuracy : 71.18\n",
      "iteration : 200, loss : 0.8162, accuracy : 71.33\n",
      "iteration : 250, loss : 0.8078, accuracy : 71.62\n",
      "iteration : 300, loss : 0.7978, accuracy : 71.88\n",
      "iteration : 350, loss : 0.7910, accuracy : 72.08\n",
      "Epoch :   2, training loss : 0.7858, training accuracy : 72.31, test loss : 0.9965, test accuracy : 67.75\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 0.7124, accuracy : 75.27\n",
      "iteration : 100, loss : 0.6958, accuracy : 75.84\n",
      "iteration : 150, loss : 0.6954, accuracy : 75.94\n",
      "iteration : 200, loss : 0.6922, accuracy : 76.02\n",
      "iteration : 250, loss : 0.6852, accuracy : 76.28\n",
      "iteration : 300, loss : 0.6742, accuracy : 76.71\n",
      "iteration : 350, loss : 0.6691, accuracy : 76.85\n",
      "Epoch :   3, training loss : 0.6635, training accuracy : 77.07, test loss : 0.7495, test accuracy : 74.51\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 0.6054, accuracy : 79.30\n",
      "iteration : 100, loss : 0.6119, accuracy : 78.83\n",
      "iteration : 150, loss : 0.6038, accuracy : 79.24\n",
      "iteration : 200, loss : 0.6006, accuracy : 79.28\n",
      "iteration : 250, loss : 0.5971, accuracy : 79.46\n",
      "iteration : 300, loss : 0.5967, accuracy : 79.53\n",
      "iteration : 350, loss : 0.5962, accuracy : 79.63\n",
      "Epoch :   4, training loss : 0.5975, training accuracy : 79.56, test loss : 0.7319, test accuracy : 75.21\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 0.5497, accuracy : 81.33\n",
      "iteration : 100, loss : 0.5576, accuracy : 80.77\n",
      "iteration : 150, loss : 0.5548, accuracy : 80.88\n",
      "iteration : 200, loss : 0.5536, accuracy : 80.82\n",
      "iteration : 250, loss : 0.5523, accuracy : 80.88\n",
      "iteration : 300, loss : 0.5495, accuracy : 80.95\n",
      "iteration : 350, loss : 0.5484, accuracy : 81.00\n",
      "Epoch :   5, training loss : 0.5501, training accuracy : 80.98, test loss : 0.6184, test accuracy : 78.57\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 0.4802, accuracy : 83.80\n",
      "iteration : 100, loss : 0.5062, accuracy : 82.93\n",
      "iteration : 150, loss : 0.5087, accuracy : 82.83\n",
      "iteration : 200, loss : 0.5082, accuracy : 82.91\n",
      "iteration : 250, loss : 0.5155, accuracy : 82.66\n",
      "iteration : 300, loss : 0.5146, accuracy : 82.64\n",
      "iteration : 350, loss : 0.5139, accuracy : 82.67\n",
      "Epoch :   6, training loss : 0.5161, training accuracy : 82.57, test loss : 0.5288, test accuracy : 81.86\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 0.4771, accuracy : 83.66\n",
      "iteration : 100, loss : 0.4886, accuracy : 83.38\n",
      "iteration : 150, loss : 0.4893, accuracy : 83.35\n",
      "iteration : 200, loss : 0.4889, accuracy : 83.49\n",
      "iteration : 250, loss : 0.4883, accuracy : 83.48\n",
      "iteration : 300, loss : 0.4891, accuracy : 83.43\n",
      "iteration : 350, loss : 0.4906, accuracy : 83.37\n",
      "Epoch :   7, training loss : 0.4895, training accuracy : 83.43, test loss : 0.5977, test accuracy : 79.65\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 0.4632, accuracy : 84.47\n",
      "iteration : 100, loss : 0.4717, accuracy : 84.26\n",
      "iteration : 150, loss : 0.4682, accuracy : 84.45\n",
      "iteration : 200, loss : 0.4677, accuracy : 84.42\n",
      "iteration : 250, loss : 0.4699, accuracy : 84.36\n",
      "iteration : 300, loss : 0.4684, accuracy : 84.34\n",
      "iteration : 350, loss : 0.4679, accuracy : 84.35\n",
      "Epoch :   8, training loss : 0.4695, training accuracy : 84.27, test loss : 0.5352, test accuracy : 81.78\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 0.4243, accuracy : 85.67\n",
      "iteration : 100, loss : 0.4306, accuracy : 85.46\n",
      "iteration : 150, loss : 0.4353, accuracy : 85.25\n",
      "iteration : 200, loss : 0.4445, accuracy : 85.12\n",
      "iteration : 250, loss : 0.4437, accuracy : 85.11\n",
      "iteration : 300, loss : 0.4460, accuracy : 84.96\n",
      "iteration : 350, loss : 0.4505, accuracy : 84.79\n",
      "Epoch :   9, training loss : 0.4523, training accuracy : 84.75, test loss : 0.4948, test accuracy : 83.49\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 0.4260, accuracy : 85.64\n",
      "iteration : 100, loss : 0.4195, accuracy : 85.63\n",
      "iteration : 150, loss : 0.4245, accuracy : 85.49\n",
      "iteration : 200, loss : 0.4241, accuracy : 85.57\n",
      "iteration : 250, loss : 0.4289, accuracy : 85.48\n",
      "iteration : 300, loss : 0.4275, accuracy : 85.50\n",
      "iteration : 350, loss : 0.4289, accuracy : 85.51\n",
      "Epoch :  10, training loss : 0.4316, training accuracy : 85.41, test loss : 0.6716, test accuracy : 77.94\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 0.4068, accuracy : 85.86\n",
      "iteration : 100, loss : 0.4220, accuracy : 85.55\n",
      "iteration : 150, loss : 0.4183, accuracy : 85.65\n",
      "iteration : 200, loss : 0.4112, accuracy : 85.95\n",
      "iteration : 250, loss : 0.4164, accuracy : 85.88\n",
      "iteration : 300, loss : 0.4190, accuracy : 85.79\n",
      "iteration : 350, loss : 0.4227, accuracy : 85.63\n",
      "Epoch :  11, training loss : 0.4239, training accuracy : 85.57, test loss : 0.7041, test accuracy : 77.89\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 0.3886, accuracy : 87.06\n",
      "iteration : 100, loss : 0.4068, accuracy : 86.47\n",
      "iteration : 150, loss : 0.4118, accuracy : 86.27\n",
      "iteration : 200, loss : 0.4105, accuracy : 86.25\n",
      "iteration : 250, loss : 0.4082, accuracy : 86.27\n",
      "iteration : 300, loss : 0.4074, accuracy : 86.29\n",
      "iteration : 350, loss : 0.4049, accuracy : 86.36\n",
      "Epoch :  12, training loss : 0.4074, training accuracy : 86.23, test loss : 0.5386, test accuracy : 81.40\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 0.3957, accuracy : 86.47\n",
      "iteration : 100, loss : 0.3927, accuracy : 86.80\n",
      "iteration : 150, loss : 0.3897, accuracy : 86.83\n",
      "iteration : 200, loss : 0.3895, accuracy : 86.80\n",
      "iteration : 250, loss : 0.3950, accuracy : 86.66\n",
      "iteration : 300, loss : 0.3940, accuracy : 86.63\n",
      "iteration : 350, loss : 0.3959, accuracy : 86.58\n",
      "Epoch :  13, training loss : 0.3950, training accuracy : 86.64, test loss : 0.5790, test accuracy : 81.07\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 0.3623, accuracy : 87.48\n",
      "iteration : 100, loss : 0.3826, accuracy : 86.98\n",
      "iteration : 150, loss : 0.3750, accuracy : 87.16\n",
      "iteration : 200, loss : 0.3766, accuracy : 87.07\n",
      "iteration : 250, loss : 0.3808, accuracy : 87.06\n",
      "iteration : 300, loss : 0.3846, accuracy : 86.85\n",
      "iteration : 350, loss : 0.3861, accuracy : 86.85\n",
      "Epoch :  14, training loss : 0.3875, training accuracy : 86.81, test loss : 0.4940, test accuracy : 83.56\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 0.3563, accuracy : 88.03\n",
      "iteration : 100, loss : 0.3563, accuracy : 87.88\n",
      "iteration : 150, loss : 0.3639, accuracy : 87.44\n",
      "iteration : 200, loss : 0.3701, accuracy : 87.22\n",
      "iteration : 250, loss : 0.3710, accuracy : 87.28\n",
      "iteration : 300, loss : 0.3702, accuracy : 87.29\n",
      "iteration : 350, loss : 0.3703, accuracy : 87.28\n",
      "Epoch :  15, training loss : 0.3714, training accuracy : 87.26, test loss : 0.5383, test accuracy : 82.12\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 0.3665, accuracy : 87.42\n",
      "iteration : 100, loss : 0.3631, accuracy : 87.48\n",
      "iteration : 150, loss : 0.3616, accuracy : 87.72\n",
      "iteration : 200, loss : 0.3622, accuracy : 87.77\n",
      "iteration : 250, loss : 0.3650, accuracy : 87.66\n",
      "iteration : 300, loss : 0.3641, accuracy : 87.67\n",
      "iteration : 350, loss : 0.3615, accuracy : 87.78\n",
      "Epoch :  16, training loss : 0.3606, training accuracy : 87.83, test loss : 0.5356, test accuracy : 82.34\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 0.3346, accuracy : 88.64\n",
      "iteration : 100, loss : 0.3416, accuracy : 88.41\n",
      "iteration : 150, loss : 0.3429, accuracy : 88.37\n",
      "iteration : 200, loss : 0.3431, accuracy : 88.38\n",
      "iteration : 250, loss : 0.3476, accuracy : 88.22\n",
      "iteration : 300, loss : 0.3513, accuracy : 88.09\n",
      "iteration : 350, loss : 0.3533, accuracy : 88.04\n",
      "Epoch :  17, training loss : 0.3521, training accuracy : 88.10, test loss : 0.5059, test accuracy : 83.08\n",
      "\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.3464, accuracy : 88.11\n",
      "iteration : 100, loss : 0.3369, accuracy : 88.46\n",
      "iteration : 150, loss : 0.3458, accuracy : 88.16\n",
      "iteration : 200, loss : 0.3450, accuracy : 88.25\n",
      "iteration : 250, loss : 0.3424, accuracy : 88.36\n",
      "iteration : 300, loss : 0.3394, accuracy : 88.46\n",
      "iteration : 350, loss : 0.3401, accuracy : 88.44\n",
      "Epoch :  18, training loss : 0.3432, training accuracy : 88.36, test loss : 0.5363, test accuracy : 82.95\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 0.3148, accuracy : 89.73\n",
      "iteration : 100, loss : 0.3019, accuracy : 90.01\n",
      "iteration : 150, loss : 0.3099, accuracy : 89.58\n",
      "iteration : 200, loss : 0.3200, accuracy : 89.27\n",
      "iteration : 250, loss : 0.3299, accuracy : 88.94\n",
      "iteration : 300, loss : 0.3308, accuracy : 88.89\n",
      "iteration : 350, loss : 0.3321, accuracy : 88.84\n",
      "Epoch :  19, training loss : 0.3314, training accuracy : 88.82, test loss : 0.5241, test accuracy : 82.99\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 0.3203, accuracy : 89.47\n",
      "iteration : 100, loss : 0.3211, accuracy : 89.27\n",
      "iteration : 150, loss : 0.3208, accuracy : 89.28\n",
      "iteration : 200, loss : 0.3212, accuracy : 89.34\n",
      "iteration : 250, loss : 0.3204, accuracy : 89.38\n",
      "iteration : 300, loss : 0.3252, accuracy : 89.17\n",
      "iteration : 350, loss : 0.3250, accuracy : 89.19\n",
      "Epoch :  20, training loss : 0.3237, training accuracy : 89.20, test loss : 0.5226, test accuracy : 82.90\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 0.3283, accuracy : 88.69\n",
      "iteration : 100, loss : 0.3211, accuracy : 89.11\n",
      "iteration : 150, loss : 0.3174, accuracy : 89.22\n",
      "iteration : 200, loss : 0.3176, accuracy : 89.27\n",
      "iteration : 250, loss : 0.3185, accuracy : 89.17\n",
      "iteration : 300, loss : 0.3229, accuracy : 88.98\n",
      "iteration : 350, loss : 0.3221, accuracy : 88.95\n",
      "Epoch :  21, training loss : 0.3212, training accuracy : 88.98, test loss : 0.3987, test accuracy : 87.18\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 0.2909, accuracy : 90.27\n",
      "iteration : 100, loss : 0.2919, accuracy : 90.12\n",
      "iteration : 150, loss : 0.3051, accuracy : 89.63\n",
      "iteration : 200, loss : 0.3094, accuracy : 89.52\n",
      "iteration : 250, loss : 0.3124, accuracy : 89.42\n",
      "iteration : 300, loss : 0.3140, accuracy : 89.35\n",
      "iteration : 350, loss : 0.3107, accuracy : 89.46\n",
      "Epoch :  22, training loss : 0.3130, training accuracy : 89.39, test loss : 0.5678, test accuracy : 82.42\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 0.2974, accuracy : 89.66\n",
      "iteration : 100, loss : 0.3031, accuracy : 89.58\n",
      "iteration : 150, loss : 0.3044, accuracy : 89.59\n",
      "iteration : 200, loss : 0.3002, accuracy : 89.71\n",
      "iteration : 250, loss : 0.3036, accuracy : 89.59\n",
      "iteration : 300, loss : 0.3050, accuracy : 89.60\n",
      "iteration : 350, loss : 0.3082, accuracy : 89.49\n",
      "Epoch :  23, training loss : 0.3076, training accuracy : 89.52, test loss : 0.4877, test accuracy : 84.27\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 0.2697, accuracy : 90.89\n",
      "iteration : 100, loss : 0.2816, accuracy : 90.35\n",
      "iteration : 150, loss : 0.2890, accuracy : 90.03\n",
      "iteration : 200, loss : 0.2898, accuracy : 90.14\n",
      "iteration : 250, loss : 0.2924, accuracy : 90.07\n",
      "iteration : 300, loss : 0.2932, accuracy : 90.04\n",
      "iteration : 350, loss : 0.2976, accuracy : 89.88\n",
      "Epoch :  24, training loss : 0.2988, training accuracy : 89.82, test loss : 0.4012, test accuracy : 86.30\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 0.2816, accuracy : 90.61\n",
      "iteration : 100, loss : 0.2902, accuracy : 90.21\n",
      "iteration : 150, loss : 0.2930, accuracy : 90.04\n",
      "iteration : 200, loss : 0.2952, accuracy : 89.84\n",
      "iteration : 250, loss : 0.2973, accuracy : 89.82\n",
      "iteration : 300, loss : 0.3004, accuracy : 89.67\n",
      "iteration : 350, loss : 0.2998, accuracy : 89.67\n",
      "Epoch :  25, training loss : 0.2993, training accuracy : 89.71, test loss : 0.4421, test accuracy : 85.86\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 0.2640, accuracy : 91.19\n",
      "iteration : 100, loss : 0.2722, accuracy : 90.78\n",
      "iteration : 150, loss : 0.2780, accuracy : 90.51\n",
      "iteration : 200, loss : 0.2822, accuracy : 90.34\n",
      "iteration : 250, loss : 0.2835, accuracy : 90.32\n",
      "iteration : 300, loss : 0.2858, accuracy : 90.21\n",
      "iteration : 350, loss : 0.2883, accuracy : 90.12\n",
      "Epoch :  26, training loss : 0.2861, training accuracy : 90.23, test loss : 0.4603, test accuracy : 84.89\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.2667, accuracy : 91.00\n",
      "iteration : 100, loss : 0.2679, accuracy : 90.80\n",
      "iteration : 150, loss : 0.2730, accuracy : 90.64\n",
      "iteration : 200, loss : 0.2829, accuracy : 90.34\n",
      "iteration : 250, loss : 0.2876, accuracy : 90.16\n",
      "iteration : 300, loss : 0.2890, accuracy : 90.09\n",
      "iteration : 350, loss : 0.2893, accuracy : 90.12\n",
      "Epoch :  27, training loss : 0.2902, training accuracy : 90.08, test loss : 0.3775, test accuracy : 87.68\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 0.2634, accuracy : 91.27\n",
      "iteration : 100, loss : 0.2683, accuracy : 90.88\n",
      "iteration : 150, loss : 0.2704, accuracy : 90.84\n",
      "iteration : 200, loss : 0.2783, accuracy : 90.48\n",
      "iteration : 250, loss : 0.2787, accuracy : 90.55\n",
      "iteration : 300, loss : 0.2801, accuracy : 90.43\n",
      "iteration : 350, loss : 0.2794, accuracy : 90.46\n",
      "Epoch :  28, training loss : 0.2834, training accuracy : 90.32, test loss : 0.4115, test accuracy : 85.84\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 0.2597, accuracy : 91.08\n",
      "iteration : 100, loss : 0.2615, accuracy : 91.07\n",
      "iteration : 150, loss : 0.2740, accuracy : 90.59\n",
      "iteration : 200, loss : 0.2739, accuracy : 90.61\n",
      "iteration : 250, loss : 0.2743, accuracy : 90.62\n",
      "iteration : 300, loss : 0.2749, accuracy : 90.59\n",
      "iteration : 350, loss : 0.2761, accuracy : 90.54\n",
      "Epoch :  29, training loss : 0.2773, training accuracy : 90.52, test loss : 0.5149, test accuracy : 83.21\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 0.2567, accuracy : 91.52\n",
      "iteration : 100, loss : 0.2547, accuracy : 91.48\n",
      "iteration : 150, loss : 0.2574, accuracy : 91.33\n",
      "iteration : 200, loss : 0.2595, accuracy : 91.22\n",
      "iteration : 250, loss : 0.2678, accuracy : 90.93\n",
      "iteration : 300, loss : 0.2697, accuracy : 90.86\n",
      "iteration : 350, loss : 0.2717, accuracy : 90.81\n",
      "Epoch :  30, training loss : 0.2740, training accuracy : 90.75, test loss : 0.3833, test accuracy : 87.15\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.2481, accuracy : 91.52\n",
      "iteration : 100, loss : 0.2545, accuracy : 91.22\n",
      "iteration : 150, loss : 0.2520, accuracy : 91.32\n",
      "iteration : 200, loss : 0.2599, accuracy : 91.04\n",
      "iteration : 250, loss : 0.2634, accuracy : 90.89\n",
      "iteration : 300, loss : 0.2660, accuracy : 90.78\n",
      "iteration : 350, loss : 0.2643, accuracy : 90.85\n",
      "Epoch :  31, training loss : 0.2657, training accuracy : 90.82, test loss : 0.6004, test accuracy : 82.35\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 0.2569, accuracy : 91.27\n",
      "iteration : 100, loss : 0.2627, accuracy : 91.09\n",
      "iteration : 150, loss : 0.2662, accuracy : 90.91\n",
      "iteration : 200, loss : 0.2692, accuracy : 90.82\n",
      "iteration : 250, loss : 0.2669, accuracy : 90.90\n",
      "iteration : 300, loss : 0.2665, accuracy : 90.89\n",
      "iteration : 350, loss : 0.2669, accuracy : 90.85\n",
      "Epoch :  32, training loss : 0.2685, training accuracy : 90.81, test loss : 0.3696, test accuracy : 87.48\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 0.2377, accuracy : 91.59\n",
      "iteration : 100, loss : 0.2424, accuracy : 91.41\n",
      "iteration : 150, loss : 0.2484, accuracy : 91.20\n",
      "iteration : 200, loss : 0.2512, accuracy : 91.17\n",
      "iteration : 250, loss : 0.2546, accuracy : 91.13\n",
      "iteration : 300, loss : 0.2582, accuracy : 91.03\n",
      "iteration : 350, loss : 0.2580, accuracy : 91.11\n",
      "Epoch :  33, training loss : 0.2614, training accuracy : 90.97, test loss : 0.3808, test accuracy : 87.01\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.2123, accuracy : 92.72\n",
      "iteration : 100, loss : 0.2317, accuracy : 92.09\n",
      "iteration : 150, loss : 0.2451, accuracy : 91.74\n",
      "iteration : 200, loss : 0.2467, accuracy : 91.62\n",
      "iteration : 250, loss : 0.2519, accuracy : 91.53\n",
      "iteration : 300, loss : 0.2547, accuracy : 91.35\n",
      "iteration : 350, loss : 0.2572, accuracy : 91.24\n",
      "Epoch :  34, training loss : 0.2590, training accuracy : 91.17, test loss : 0.4072, test accuracy : 87.15\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 0.2373, accuracy : 92.02\n",
      "iteration : 100, loss : 0.2391, accuracy : 91.84\n",
      "iteration : 150, loss : 0.2517, accuracy : 91.31\n",
      "iteration : 200, loss : 0.2561, accuracy : 91.17\n",
      "iteration : 250, loss : 0.2597, accuracy : 91.01\n",
      "iteration : 300, loss : 0.2580, accuracy : 91.11\n",
      "iteration : 350, loss : 0.2595, accuracy : 91.04\n",
      "Epoch :  35, training loss : 0.2606, training accuracy : 91.00, test loss : 0.5103, test accuracy : 83.05\n",
      "\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.2403, accuracy : 91.72\n",
      "iteration : 100, loss : 0.2425, accuracy : 91.66\n",
      "iteration : 150, loss : 0.2501, accuracy : 91.44\n",
      "iteration : 200, loss : 0.2505, accuracy : 91.37\n",
      "iteration : 250, loss : 0.2499, accuracy : 91.44\n",
      "iteration : 300, loss : 0.2510, accuracy : 91.38\n",
      "iteration : 350, loss : 0.2552, accuracy : 91.25\n",
      "Epoch :  36, training loss : 0.2560, training accuracy : 91.25, test loss : 0.5953, test accuracy : 82.02\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.2534, accuracy : 91.64\n",
      "iteration : 100, loss : 0.2498, accuracy : 91.80\n",
      "iteration : 150, loss : 0.2483, accuracy : 91.79\n",
      "iteration : 200, loss : 0.2465, accuracy : 91.74\n",
      "iteration : 250, loss : 0.2497, accuracy : 91.63\n",
      "iteration : 300, loss : 0.2532, accuracy : 91.46\n",
      "iteration : 350, loss : 0.2556, accuracy : 91.38\n",
      "Epoch :  37, training loss : 0.2557, training accuracy : 91.35, test loss : 0.4576, test accuracy : 84.84\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 0.2238, accuracy : 92.17\n",
      "iteration : 100, loss : 0.2306, accuracy : 92.13\n",
      "iteration : 150, loss : 0.2420, accuracy : 91.75\n",
      "iteration : 200, loss : 0.2478, accuracy : 91.54\n",
      "iteration : 250, loss : 0.2453, accuracy : 91.59\n",
      "iteration : 300, loss : 0.2469, accuracy : 91.54\n",
      "iteration : 350, loss : 0.2496, accuracy : 91.50\n",
      "Epoch :  38, training loss : 0.2496, training accuracy : 91.50, test loss : 0.4203, test accuracy : 86.19\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.2240, accuracy : 91.98\n",
      "iteration : 100, loss : 0.2340, accuracy : 91.70\n",
      "iteration : 150, loss : 0.2416, accuracy : 91.50\n",
      "iteration : 200, loss : 0.2419, accuracy : 91.54\n",
      "iteration : 250, loss : 0.2467, accuracy : 91.37\n",
      "iteration : 300, loss : 0.2451, accuracy : 91.40\n",
      "iteration : 350, loss : 0.2461, accuracy : 91.38\n",
      "Epoch :  39, training loss : 0.2479, training accuracy : 91.36, test loss : 0.3839, test accuracy : 86.99\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.2161, accuracy : 92.77\n",
      "iteration : 100, loss : 0.2260, accuracy : 92.23\n",
      "iteration : 150, loss : 0.2329, accuracy : 91.98\n",
      "iteration : 200, loss : 0.2322, accuracy : 92.10\n",
      "iteration : 250, loss : 0.2375, accuracy : 91.84\n",
      "iteration : 300, loss : 0.2423, accuracy : 91.71\n",
      "iteration : 350, loss : 0.2412, accuracy : 91.74\n",
      "Epoch :  40, training loss : 0.2429, training accuracy : 91.69, test loss : 0.4006, test accuracy : 87.18\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.2350, accuracy : 92.27\n",
      "iteration : 100, loss : 0.2297, accuracy : 92.41\n",
      "iteration : 150, loss : 0.2367, accuracy : 92.11\n",
      "iteration : 200, loss : 0.2364, accuracy : 92.10\n",
      "iteration : 250, loss : 0.2351, accuracy : 92.07\n",
      "iteration : 300, loss : 0.2390, accuracy : 91.88\n",
      "iteration : 350, loss : 0.2383, accuracy : 91.87\n",
      "Epoch :  41, training loss : 0.2399, training accuracy : 91.81, test loss : 0.5688, test accuracy : 83.55\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.2384, accuracy : 91.25\n",
      "iteration : 100, loss : 0.2304, accuracy : 91.97\n",
      "iteration : 150, loss : 0.2323, accuracy : 91.91\n",
      "iteration : 200, loss : 0.2323, accuracy : 91.95\n",
      "iteration : 250, loss : 0.2345, accuracy : 91.91\n",
      "iteration : 300, loss : 0.2336, accuracy : 91.94\n",
      "iteration : 350, loss : 0.2388, accuracy : 91.80\n",
      "Epoch :  42, training loss : 0.2425, training accuracy : 91.63, test loss : 0.4136, test accuracy : 86.49\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.2213, accuracy : 92.12\n",
      "iteration : 100, loss : 0.2156, accuracy : 92.35\n",
      "iteration : 150, loss : 0.2247, accuracy : 92.11\n",
      "iteration : 200, loss : 0.2253, accuracy : 92.11\n",
      "iteration : 250, loss : 0.2311, accuracy : 91.92\n",
      "iteration : 300, loss : 0.2355, accuracy : 91.76\n",
      "iteration : 350, loss : 0.2378, accuracy : 91.69\n",
      "Epoch :  43, training loss : 0.2366, training accuracy : 91.73, test loss : 0.3622, test accuracy : 87.86\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.2045, accuracy : 93.02\n",
      "iteration : 100, loss : 0.2148, accuracy : 92.64\n",
      "iteration : 150, loss : 0.2176, accuracy : 92.63\n",
      "iteration : 200, loss : 0.2266, accuracy : 92.34\n",
      "iteration : 250, loss : 0.2308, accuracy : 92.23\n",
      "iteration : 300, loss : 0.2301, accuracy : 92.23\n",
      "iteration : 350, loss : 0.2341, accuracy : 92.01\n",
      "Epoch :  44, training loss : 0.2363, training accuracy : 91.91, test loss : 0.4856, test accuracy : 85.69\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.2227, accuracy : 92.75\n",
      "iteration : 100, loss : 0.2217, accuracy : 92.59\n",
      "iteration : 150, loss : 0.2240, accuracy : 92.35\n",
      "iteration : 200, loss : 0.2219, accuracy : 92.48\n",
      "iteration : 250, loss : 0.2225, accuracy : 92.46\n",
      "iteration : 300, loss : 0.2269, accuracy : 92.34\n",
      "iteration : 350, loss : 0.2321, accuracy : 92.13\n",
      "Epoch :  45, training loss : 0.2313, training accuracy : 92.18, test loss : 0.4485, test accuracy : 85.52\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.1862, accuracy : 93.50\n",
      "iteration : 100, loss : 0.2075, accuracy : 92.76\n",
      "iteration : 150, loss : 0.2216, accuracy : 92.25\n",
      "iteration : 200, loss : 0.2308, accuracy : 91.91\n",
      "iteration : 250, loss : 0.2308, accuracy : 91.97\n",
      "iteration : 300, loss : 0.2353, accuracy : 91.89\n",
      "iteration : 350, loss : 0.2322, accuracy : 92.03\n",
      "Epoch :  46, training loss : 0.2339, training accuracy : 91.97, test loss : 0.5111, test accuracy : 84.10\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.2125, accuracy : 92.56\n",
      "iteration : 100, loss : 0.2222, accuracy : 92.30\n",
      "iteration : 150, loss : 0.2211, accuracy : 92.36\n",
      "iteration : 200, loss : 0.2196, accuracy : 92.45\n",
      "iteration : 250, loss : 0.2220, accuracy : 92.36\n",
      "iteration : 300, loss : 0.2254, accuracy : 92.24\n",
      "iteration : 350, loss : 0.2281, accuracy : 92.12\n",
      "Epoch :  47, training loss : 0.2266, training accuracy : 92.17, test loss : 0.4941, test accuracy : 85.05\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.2071, accuracy : 92.75\n",
      "iteration : 100, loss : 0.2124, accuracy : 92.49\n",
      "iteration : 150, loss : 0.2176, accuracy : 92.45\n",
      "iteration : 200, loss : 0.2214, accuracy : 92.39\n",
      "iteration : 250, loss : 0.2223, accuracy : 92.39\n",
      "iteration : 300, loss : 0.2245, accuracy : 92.29\n",
      "iteration : 350, loss : 0.2272, accuracy : 92.23\n",
      "Epoch :  48, training loss : 0.2266, training accuracy : 92.25, test loss : 0.3303, test accuracy : 88.89\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.2088, accuracy : 92.94\n",
      "iteration : 100, loss : 0.2056, accuracy : 93.14\n",
      "iteration : 150, loss : 0.2144, accuracy : 92.77\n",
      "iteration : 200, loss : 0.2168, accuracy : 92.64\n",
      "iteration : 250, loss : 0.2189, accuracy : 92.58\n",
      "iteration : 300, loss : 0.2204, accuracy : 92.46\n",
      "iteration : 350, loss : 0.2227, accuracy : 92.40\n",
      "Epoch :  49, training loss : 0.2265, training accuracy : 92.25, test loss : 0.3706, test accuracy : 87.79\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.1941, accuracy : 93.47\n",
      "iteration : 100, loss : 0.2025, accuracy : 93.16\n",
      "iteration : 150, loss : 0.2112, accuracy : 92.81\n",
      "iteration : 200, loss : 0.2133, accuracy : 92.61\n",
      "iteration : 250, loss : 0.2175, accuracy : 92.45\n",
      "iteration : 300, loss : 0.2230, accuracy : 92.29\n",
      "iteration : 350, loss : 0.2252, accuracy : 92.23\n",
      "Epoch :  50, training loss : 0.2261, training accuracy : 92.15, test loss : 0.3387, test accuracy : 88.50\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.2020, accuracy : 93.17\n",
      "iteration : 100, loss : 0.2095, accuracy : 92.80\n",
      "iteration : 150, loss : 0.2153, accuracy : 92.53\n",
      "iteration : 200, loss : 0.2175, accuracy : 92.52\n",
      "iteration : 250, loss : 0.2180, accuracy : 92.48\n",
      "iteration : 300, loss : 0.2174, accuracy : 92.55\n",
      "iteration : 350, loss : 0.2168, accuracy : 92.57\n",
      "Epoch :  51, training loss : 0.2197, training accuracy : 92.51, test loss : 0.4110, test accuracy : 87.06\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.1943, accuracy : 93.38\n",
      "iteration : 100, loss : 0.1988, accuracy : 93.20\n",
      "iteration : 150, loss : 0.2040, accuracy : 93.08\n",
      "iteration : 200, loss : 0.2080, accuracy : 92.96\n",
      "iteration : 250, loss : 0.2128, accuracy : 92.81\n",
      "iteration : 300, loss : 0.2159, accuracy : 92.68\n",
      "iteration : 350, loss : 0.2178, accuracy : 92.62\n",
      "Epoch :  52, training loss : 0.2186, training accuracy : 92.60, test loss : 0.3805, test accuracy : 87.92\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.1931, accuracy : 93.45\n",
      "iteration : 100, loss : 0.1941, accuracy : 93.34\n",
      "iteration : 150, loss : 0.1957, accuracy : 93.32\n",
      "iteration : 200, loss : 0.2010, accuracy : 93.12\n",
      "iteration : 250, loss : 0.2064, accuracy : 92.92\n",
      "iteration : 300, loss : 0.2074, accuracy : 92.89\n",
      "iteration : 350, loss : 0.2094, accuracy : 92.87\n",
      "Epoch :  53, training loss : 0.2115, training accuracy : 92.81, test loss : 0.3709, test accuracy : 87.77\n",
      "\n",
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.2046, accuracy : 92.84\n",
      "iteration : 100, loss : 0.1951, accuracy : 93.28\n",
      "iteration : 150, loss : 0.1970, accuracy : 93.31\n",
      "iteration : 200, loss : 0.2024, accuracy : 93.09\n",
      "iteration : 250, loss : 0.2096, accuracy : 92.80\n",
      "iteration : 300, loss : 0.2117, accuracy : 92.72\n",
      "iteration : 350, loss : 0.2122, accuracy : 92.71\n",
      "Epoch :  54, training loss : 0.2111, training accuracy : 92.74, test loss : 0.3639, test accuracy : 87.96\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.2055, accuracy : 93.20\n",
      "iteration : 100, loss : 0.2063, accuracy : 93.05\n",
      "iteration : 150, loss : 0.2103, accuracy : 92.86\n",
      "iteration : 200, loss : 0.2123, accuracy : 92.77\n",
      "iteration : 250, loss : 0.2149, accuracy : 92.65\n",
      "iteration : 300, loss : 0.2162, accuracy : 92.63\n",
      "iteration : 350, loss : 0.2206, accuracy : 92.46\n",
      "Epoch :  55, training loss : 0.2203, training accuracy : 92.48, test loss : 0.3026, test accuracy : 89.95\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.1633, accuracy : 94.47\n",
      "iteration : 100, loss : 0.1771, accuracy : 94.08\n",
      "iteration : 150, loss : 0.1838, accuracy : 93.81\n",
      "iteration : 200, loss : 0.1913, accuracy : 93.43\n",
      "iteration : 250, loss : 0.1956, accuracy : 93.30\n",
      "iteration : 300, loss : 0.2004, accuracy : 93.11\n",
      "iteration : 350, loss : 0.2018, accuracy : 93.05\n",
      "Epoch :  56, training loss : 0.2039, training accuracy : 92.95, test loss : 0.4720, test accuracy : 85.68\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.1875, accuracy : 93.64\n",
      "iteration : 100, loss : 0.1913, accuracy : 93.41\n",
      "iteration : 150, loss : 0.1948, accuracy : 93.40\n",
      "iteration : 200, loss : 0.1958, accuracy : 93.34\n",
      "iteration : 250, loss : 0.2034, accuracy : 93.11\n",
      "iteration : 300, loss : 0.2022, accuracy : 93.18\n",
      "iteration : 350, loss : 0.2038, accuracy : 93.10\n",
      "Epoch :  57, training loss : 0.2080, training accuracy : 92.97, test loss : 0.3789, test accuracy : 87.69\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.1778, accuracy : 93.94\n",
      "iteration : 100, loss : 0.1782, accuracy : 93.95\n",
      "iteration : 150, loss : 0.1873, accuracy : 93.59\n",
      "iteration : 200, loss : 0.1943, accuracy : 93.34\n",
      "iteration : 250, loss : 0.2001, accuracy : 93.20\n",
      "iteration : 300, loss : 0.2043, accuracy : 93.05\n",
      "iteration : 350, loss : 0.2073, accuracy : 92.94\n",
      "Epoch :  58, training loss : 0.2073, training accuracy : 92.95, test loss : 0.3875, test accuracy : 86.97\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.1798, accuracy : 93.78\n",
      "iteration : 100, loss : 0.1874, accuracy : 93.56\n",
      "iteration : 150, loss : 0.1884, accuracy : 93.56\n",
      "iteration : 200, loss : 0.1935, accuracy : 93.43\n",
      "iteration : 250, loss : 0.1947, accuracy : 93.40\n",
      "iteration : 300, loss : 0.1973, accuracy : 93.28\n",
      "iteration : 350, loss : 0.1993, accuracy : 93.20\n",
      "Epoch :  59, training loss : 0.1998, training accuracy : 93.12, test loss : 0.6690, test accuracy : 80.61\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.1871, accuracy : 93.41\n",
      "iteration : 100, loss : 0.1866, accuracy : 93.49\n",
      "iteration : 150, loss : 0.1936, accuracy : 93.22\n",
      "iteration : 200, loss : 0.1946, accuracy : 93.27\n",
      "iteration : 250, loss : 0.1939, accuracy : 93.32\n",
      "iteration : 300, loss : 0.1963, accuracy : 93.19\n",
      "iteration : 350, loss : 0.1969, accuracy : 93.19\n",
      "Epoch :  60, training loss : 0.1978, training accuracy : 93.16, test loss : 0.4802, test accuracy : 85.95\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.1749, accuracy : 94.02\n",
      "iteration : 100, loss : 0.1806, accuracy : 93.80\n",
      "iteration : 150, loss : 0.1855, accuracy : 93.60\n",
      "iteration : 200, loss : 0.1895, accuracy : 93.42\n",
      "iteration : 250, loss : 0.1930, accuracy : 93.28\n",
      "iteration : 300, loss : 0.1922, accuracy : 93.27\n",
      "iteration : 350, loss : 0.1952, accuracy : 93.20\n",
      "Epoch :  61, training loss : 0.1976, training accuracy : 93.10, test loss : 0.4116, test accuracy : 87.45\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.1841, accuracy : 93.48\n",
      "iteration : 100, loss : 0.1877, accuracy : 93.44\n",
      "iteration : 150, loss : 0.1911, accuracy : 93.34\n",
      "iteration : 200, loss : 0.1907, accuracy : 93.37\n",
      "iteration : 250, loss : 0.1941, accuracy : 93.28\n",
      "iteration : 300, loss : 0.1962, accuracy : 93.26\n",
      "iteration : 350, loss : 0.1953, accuracy : 93.32\n",
      "Epoch :  62, training loss : 0.1958, training accuracy : 93.32, test loss : 0.3603, test accuracy : 88.14\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.1747, accuracy : 94.09\n",
      "iteration : 100, loss : 0.1783, accuracy : 93.86\n",
      "iteration : 150, loss : 0.1844, accuracy : 93.67\n",
      "iteration : 200, loss : 0.1894, accuracy : 93.40\n",
      "iteration : 250, loss : 0.1929, accuracy : 93.35\n",
      "iteration : 300, loss : 0.1918, accuracy : 93.38\n",
      "iteration : 350, loss : 0.1948, accuracy : 93.29\n",
      "Epoch :  63, training loss : 0.1952, training accuracy : 93.28, test loss : 0.3915, test accuracy : 87.36\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.1725, accuracy : 93.97\n",
      "iteration : 100, loss : 0.1764, accuracy : 93.84\n",
      "iteration : 150, loss : 0.1784, accuracy : 93.74\n",
      "iteration : 200, loss : 0.1825, accuracy : 93.64\n",
      "iteration : 250, loss : 0.1842, accuracy : 93.58\n",
      "iteration : 300, loss : 0.1869, accuracy : 93.52\n",
      "iteration : 350, loss : 0.1877, accuracy : 93.54\n",
      "Epoch :  64, training loss : 0.1898, training accuracy : 93.50, test loss : 0.3687, test accuracy : 88.30\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.1644, accuracy : 94.45\n",
      "iteration : 100, loss : 0.1707, accuracy : 94.20\n",
      "iteration : 150, loss : 0.1825, accuracy : 93.78\n",
      "iteration : 200, loss : 0.1834, accuracy : 93.69\n",
      "iteration : 250, loss : 0.1849, accuracy : 93.57\n",
      "iteration : 300, loss : 0.1885, accuracy : 93.45\n",
      "iteration : 350, loss : 0.1897, accuracy : 93.40\n",
      "Epoch :  65, training loss : 0.1937, training accuracy : 93.25, test loss : 0.3452, test accuracy : 88.80\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.1636, accuracy : 94.34\n",
      "iteration : 100, loss : 0.1736, accuracy : 93.91\n",
      "iteration : 150, loss : 0.1722, accuracy : 94.12\n",
      "iteration : 200, loss : 0.1722, accuracy : 94.08\n",
      "iteration : 250, loss : 0.1740, accuracy : 94.08\n",
      "iteration : 300, loss : 0.1786, accuracy : 93.89\n",
      "iteration : 350, loss : 0.1819, accuracy : 93.75\n",
      "Epoch :  66, training loss : 0.1859, training accuracy : 93.62, test loss : 0.3990, test accuracy : 87.23\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.1641, accuracy : 94.44\n",
      "iteration : 100, loss : 0.1623, accuracy : 94.49\n",
      "iteration : 150, loss : 0.1735, accuracy : 93.97\n",
      "iteration : 200, loss : 0.1830, accuracy : 93.64\n",
      "iteration : 250, loss : 0.1858, accuracy : 93.56\n",
      "iteration : 300, loss : 0.1833, accuracy : 93.67\n",
      "iteration : 350, loss : 0.1869, accuracy : 93.56\n",
      "Epoch :  67, training loss : 0.1881, training accuracy : 93.53, test loss : 0.5192, test accuracy : 84.64\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.1736, accuracy : 93.94\n",
      "iteration : 100, loss : 0.1794, accuracy : 93.89\n",
      "iteration : 150, loss : 0.1723, accuracy : 94.20\n",
      "iteration : 200, loss : 0.1757, accuracy : 94.05\n",
      "iteration : 250, loss : 0.1755, accuracy : 94.03\n",
      "iteration : 300, loss : 0.1752, accuracy : 94.04\n",
      "iteration : 350, loss : 0.1768, accuracy : 94.01\n",
      "Epoch :  68, training loss : 0.1785, training accuracy : 93.98, test loss : 0.3502, test accuracy : 88.73\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.1618, accuracy : 94.53\n",
      "iteration : 100, loss : 0.1687, accuracy : 94.37\n",
      "iteration : 150, loss : 0.1703, accuracy : 94.32\n",
      "iteration : 200, loss : 0.1695, accuracy : 94.36\n",
      "iteration : 250, loss : 0.1722, accuracy : 94.19\n",
      "iteration : 300, loss : 0.1761, accuracy : 94.03\n",
      "iteration : 350, loss : 0.1801, accuracy : 93.88\n",
      "Epoch :  69, training loss : 0.1816, training accuracy : 93.84, test loss : 0.3295, test accuracy : 89.39\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.1625, accuracy : 94.27\n",
      "iteration : 100, loss : 0.1644, accuracy : 94.31\n",
      "iteration : 150, loss : 0.1651, accuracy : 94.28\n",
      "iteration : 200, loss : 0.1659, accuracy : 94.31\n",
      "iteration : 250, loss : 0.1694, accuracy : 94.19\n",
      "iteration : 300, loss : 0.1719, accuracy : 94.08\n",
      "iteration : 350, loss : 0.1726, accuracy : 94.01\n",
      "Epoch :  70, training loss : 0.1756, training accuracy : 93.87, test loss : 0.3689, test accuracy : 88.37\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.1610, accuracy : 94.56\n",
      "iteration : 100, loss : 0.1648, accuracy : 94.47\n",
      "iteration : 150, loss : 0.1675, accuracy : 94.34\n",
      "iteration : 200, loss : 0.1703, accuracy : 94.18\n",
      "iteration : 250, loss : 0.1700, accuracy : 94.14\n",
      "iteration : 300, loss : 0.1753, accuracy : 93.97\n",
      "iteration : 350, loss : 0.1769, accuracy : 93.90\n",
      "Epoch :  71, training loss : 0.1789, training accuracy : 93.85, test loss : 0.3600, test accuracy : 88.38\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.1691, accuracy : 94.19\n",
      "iteration : 100, loss : 0.1656, accuracy : 94.41\n",
      "iteration : 150, loss : 0.1624, accuracy : 94.57\n",
      "iteration : 200, loss : 0.1634, accuracy : 94.45\n",
      "iteration : 250, loss : 0.1686, accuracy : 94.23\n",
      "iteration : 300, loss : 0.1720, accuracy : 94.12\n",
      "iteration : 350, loss : 0.1745, accuracy : 94.02\n",
      "Epoch :  72, training loss : 0.1737, training accuracy : 94.06, test loss : 0.4247, test accuracy : 87.61\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.1384, accuracy : 95.34\n",
      "iteration : 100, loss : 0.1442, accuracy : 95.10\n",
      "iteration : 150, loss : 0.1493, accuracy : 94.85\n",
      "iteration : 200, loss : 0.1591, accuracy : 94.57\n",
      "iteration : 250, loss : 0.1624, accuracy : 94.46\n",
      "iteration : 300, loss : 0.1638, accuracy : 94.43\n",
      "iteration : 350, loss : 0.1646, accuracy : 94.38\n",
      "Epoch :  73, training loss : 0.1666, training accuracy : 94.32, test loss : 0.3793, test accuracy : 88.29\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.1698, accuracy : 94.12\n",
      "iteration : 100, loss : 0.1611, accuracy : 94.44\n",
      "iteration : 150, loss : 0.1667, accuracy : 94.31\n",
      "iteration : 200, loss : 0.1726, accuracy : 94.02\n",
      "iteration : 250, loss : 0.1714, accuracy : 94.05\n",
      "iteration : 300, loss : 0.1727, accuracy : 94.01\n",
      "iteration : 350, loss : 0.1762, accuracy : 93.87\n",
      "Epoch :  74, training loss : 0.1760, training accuracy : 93.89, test loss : 0.3033, test accuracy : 90.21\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.1418, accuracy : 95.23\n",
      "iteration : 100, loss : 0.1436, accuracy : 95.05\n",
      "iteration : 150, loss : 0.1490, accuracy : 94.86\n",
      "iteration : 200, loss : 0.1547, accuracy : 94.61\n",
      "iteration : 250, loss : 0.1542, accuracy : 94.59\n",
      "iteration : 300, loss : 0.1573, accuracy : 94.54\n",
      "iteration : 350, loss : 0.1602, accuracy : 94.45\n",
      "Epoch :  75, training loss : 0.1643, training accuracy : 94.34, test loss : 0.4049, test accuracy : 87.96\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.1556, accuracy : 94.97\n",
      "iteration : 100, loss : 0.1536, accuracy : 94.88\n",
      "iteration : 150, loss : 0.1563, accuracy : 94.73\n",
      "iteration : 200, loss : 0.1567, accuracy : 94.66\n",
      "iteration : 250, loss : 0.1598, accuracy : 94.55\n",
      "iteration : 300, loss : 0.1634, accuracy : 94.36\n",
      "iteration : 350, loss : 0.1657, accuracy : 94.25\n",
      "Epoch :  76, training loss : 0.1677, training accuracy : 94.16, test loss : 0.3308, test accuracy : 89.36\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.1407, accuracy : 95.30\n",
      "iteration : 100, loss : 0.1469, accuracy : 95.03\n",
      "iteration : 150, loss : 0.1541, accuracy : 94.72\n",
      "iteration : 200, loss : 0.1526, accuracy : 94.75\n",
      "iteration : 250, loss : 0.1522, accuracy : 94.79\n",
      "iteration : 300, loss : 0.1561, accuracy : 94.66\n",
      "iteration : 350, loss : 0.1590, accuracy : 94.55\n",
      "Epoch :  77, training loss : 0.1607, training accuracy : 94.49, test loss : 0.3535, test accuracy : 88.89\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.1515, accuracy : 94.81\n",
      "iteration : 100, loss : 0.1514, accuracy : 94.76\n",
      "iteration : 150, loss : 0.1491, accuracy : 94.83\n",
      "iteration : 200, loss : 0.1504, accuracy : 94.85\n",
      "iteration : 250, loss : 0.1514, accuracy : 94.83\n",
      "iteration : 300, loss : 0.1529, accuracy : 94.71\n",
      "iteration : 350, loss : 0.1549, accuracy : 94.62\n",
      "Epoch :  78, training loss : 0.1566, training accuracy : 94.59, test loss : 0.3156, test accuracy : 90.15\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.1455, accuracy : 95.09\n",
      "iteration : 100, loss : 0.1361, accuracy : 95.42\n",
      "iteration : 150, loss : 0.1444, accuracy : 95.09\n",
      "iteration : 200, loss : 0.1478, accuracy : 94.92\n",
      "iteration : 250, loss : 0.1527, accuracy : 94.79\n",
      "iteration : 300, loss : 0.1553, accuracy : 94.72\n",
      "iteration : 350, loss : 0.1562, accuracy : 94.67\n",
      "Epoch :  79, training loss : 0.1581, training accuracy : 94.63, test loss : 0.3347, test accuracy : 89.47\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.1345, accuracy : 95.61\n",
      "iteration : 100, loss : 0.1451, accuracy : 95.23\n",
      "iteration : 150, loss : 0.1531, accuracy : 94.89\n",
      "iteration : 200, loss : 0.1551, accuracy : 94.72\n",
      "iteration : 250, loss : 0.1518, accuracy : 94.84\n",
      "iteration : 300, loss : 0.1541, accuracy : 94.78\n",
      "iteration : 350, loss : 0.1579, accuracy : 94.62\n",
      "Epoch :  80, training loss : 0.1585, training accuracy : 94.60, test loss : 0.3185, test accuracy : 90.19\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.1392, accuracy : 95.12\n",
      "iteration : 100, loss : 0.1418, accuracy : 95.10\n",
      "iteration : 150, loss : 0.1451, accuracy : 94.92\n",
      "iteration : 200, loss : 0.1476, accuracy : 94.83\n",
      "iteration : 250, loss : 0.1501, accuracy : 94.79\n",
      "iteration : 300, loss : 0.1510, accuracy : 94.77\n",
      "iteration : 350, loss : 0.1535, accuracy : 94.72\n",
      "Epoch :  81, training loss : 0.1541, training accuracy : 94.69, test loss : 0.3280, test accuracy : 89.42\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.1437, accuracy : 95.17\n",
      "iteration : 100, loss : 0.1334, accuracy : 95.45\n",
      "iteration : 150, loss : 0.1350, accuracy : 95.35\n",
      "iteration : 200, loss : 0.1357, accuracy : 95.27\n",
      "iteration : 250, loss : 0.1368, accuracy : 95.24\n",
      "iteration : 300, loss : 0.1417, accuracy : 95.09\n",
      "iteration : 350, loss : 0.1452, accuracy : 94.95\n",
      "Epoch :  82, training loss : 0.1473, training accuracy : 94.87, test loss : 0.3851, test accuracy : 88.81\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.1340, accuracy : 95.47\n",
      "iteration : 100, loss : 0.1345, accuracy : 95.27\n",
      "iteration : 150, loss : 0.1389, accuracy : 95.15\n",
      "iteration : 200, loss : 0.1415, accuracy : 95.13\n",
      "iteration : 250, loss : 0.1420, accuracy : 95.16\n",
      "iteration : 300, loss : 0.1437, accuracy : 95.09\n",
      "iteration : 350, loss : 0.1458, accuracy : 95.03\n",
      "Epoch :  83, training loss : 0.1467, training accuracy : 94.98, test loss : 0.3265, test accuracy : 90.05\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.1264, accuracy : 95.52\n",
      "iteration : 100, loss : 0.1311, accuracy : 95.58\n",
      "iteration : 150, loss : 0.1372, accuracy : 95.40\n",
      "iteration : 200, loss : 0.1395, accuracy : 95.27\n",
      "iteration : 250, loss : 0.1437, accuracy : 95.12\n",
      "iteration : 300, loss : 0.1461, accuracy : 95.02\n",
      "iteration : 350, loss : 0.1481, accuracy : 94.94\n",
      "Epoch :  84, training loss : 0.1476, training accuracy : 94.98, test loss : 0.3202, test accuracy : 90.21\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.1272, accuracy : 95.52\n",
      "iteration : 100, loss : 0.1250, accuracy : 95.55\n",
      "iteration : 150, loss : 0.1346, accuracy : 95.21\n",
      "iteration : 200, loss : 0.1397, accuracy : 95.06\n",
      "iteration : 250, loss : 0.1417, accuracy : 95.01\n",
      "iteration : 300, loss : 0.1438, accuracy : 94.96\n",
      "iteration : 350, loss : 0.1450, accuracy : 94.94\n",
      "Epoch :  85, training loss : 0.1446, training accuracy : 94.91, test loss : 0.2967, test accuracy : 90.81\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.1422, accuracy : 95.25\n",
      "iteration : 100, loss : 0.1332, accuracy : 95.36\n",
      "iteration : 150, loss : 0.1358, accuracy : 95.35\n",
      "iteration : 200, loss : 0.1343, accuracy : 95.42\n",
      "iteration : 250, loss : 0.1330, accuracy : 95.46\n",
      "iteration : 300, loss : 0.1340, accuracy : 95.42\n",
      "iteration : 350, loss : 0.1349, accuracy : 95.36\n",
      "Epoch :  86, training loss : 0.1405, training accuracy : 95.17, test loss : 0.4043, test accuracy : 88.10\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.1396, accuracy : 95.25\n",
      "iteration : 100, loss : 0.1322, accuracy : 95.42\n",
      "iteration : 150, loss : 0.1385, accuracy : 95.26\n",
      "iteration : 200, loss : 0.1393, accuracy : 95.21\n",
      "iteration : 250, loss : 0.1379, accuracy : 95.28\n",
      "iteration : 300, loss : 0.1378, accuracy : 95.29\n",
      "iteration : 350, loss : 0.1382, accuracy : 95.25\n",
      "Epoch :  87, training loss : 0.1415, training accuracy : 95.12, test loss : 0.3223, test accuracy : 90.27\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.1313, accuracy : 95.62\n",
      "iteration : 100, loss : 0.1226, accuracy : 95.86\n",
      "iteration : 150, loss : 0.1200, accuracy : 95.95\n",
      "iteration : 200, loss : 0.1228, accuracy : 95.81\n",
      "iteration : 250, loss : 0.1289, accuracy : 95.62\n",
      "iteration : 300, loss : 0.1328, accuracy : 95.45\n",
      "iteration : 350, loss : 0.1342, accuracy : 95.40\n",
      "Epoch :  88, training loss : 0.1359, training accuracy : 95.33, test loss : 0.2892, test accuracy : 91.03\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.1132, accuracy : 96.27\n",
      "iteration : 100, loss : 0.1137, accuracy : 96.11\n",
      "iteration : 150, loss : 0.1222, accuracy : 95.78\n",
      "iteration : 200, loss : 0.1276, accuracy : 95.60\n",
      "iteration : 250, loss : 0.1315, accuracy : 95.47\n",
      "iteration : 300, loss : 0.1340, accuracy : 95.37\n",
      "iteration : 350, loss : 0.1370, accuracy : 95.29\n",
      "Epoch :  89, training loss : 0.1408, training accuracy : 95.18, test loss : 0.3810, test accuracy : 88.80\n",
      "\n",
      "Epoch: 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.1230, accuracy : 95.81\n",
      "iteration : 100, loss : 0.1186, accuracy : 95.93\n",
      "iteration : 150, loss : 0.1202, accuracy : 95.81\n",
      "iteration : 200, loss : 0.1201, accuracy : 95.86\n",
      "iteration : 250, loss : 0.1240, accuracy : 95.68\n",
      "iteration : 300, loss : 0.1262, accuracy : 95.61\n",
      "iteration : 350, loss : 0.1279, accuracy : 95.55\n",
      "Epoch :  90, training loss : 0.1300, training accuracy : 95.50, test loss : 0.3205, test accuracy : 90.62\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.1296, accuracy : 95.72\n",
      "iteration : 100, loss : 0.1292, accuracy : 95.70\n",
      "iteration : 150, loss : 0.1311, accuracy : 95.64\n",
      "iteration : 200, loss : 0.1310, accuracy : 95.55\n",
      "iteration : 250, loss : 0.1302, accuracy : 95.59\n",
      "iteration : 300, loss : 0.1289, accuracy : 95.60\n",
      "iteration : 350, loss : 0.1300, accuracy : 95.54\n",
      "Epoch :  91, training loss : 0.1308, training accuracy : 95.50, test loss : 0.3767, test accuracy : 89.44\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.1187, accuracy : 96.09\n",
      "iteration : 100, loss : 0.1199, accuracy : 96.13\n",
      "iteration : 150, loss : 0.1240, accuracy : 95.86\n",
      "iteration : 200, loss : 0.1234, accuracy : 95.84\n",
      "iteration : 250, loss : 0.1254, accuracy : 95.75\n",
      "iteration : 300, loss : 0.1264, accuracy : 95.71\n",
      "iteration : 350, loss : 0.1301, accuracy : 95.57\n",
      "Epoch :  92, training loss : 0.1308, training accuracy : 95.58, test loss : 0.2996, test accuracy : 90.78\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.1002, accuracy : 96.72\n",
      "iteration : 100, loss : 0.1069, accuracy : 96.38\n",
      "iteration : 150, loss : 0.1088, accuracy : 96.35\n",
      "iteration : 200, loss : 0.1131, accuracy : 96.17\n",
      "iteration : 250, loss : 0.1167, accuracy : 96.06\n",
      "iteration : 300, loss : 0.1187, accuracy : 96.01\n",
      "iteration : 350, loss : 0.1204, accuracy : 95.96\n",
      "Epoch :  93, training loss : 0.1233, training accuracy : 95.87, test loss : 0.4503, test accuracy : 87.28\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.1243, accuracy : 95.56\n",
      "iteration : 100, loss : 0.1155, accuracy : 95.91\n",
      "iteration : 150, loss : 0.1152, accuracy : 96.03\n",
      "iteration : 200, loss : 0.1178, accuracy : 96.02\n",
      "iteration : 250, loss : 0.1216, accuracy : 95.91\n",
      "iteration : 300, loss : 0.1238, accuracy : 95.82\n",
      "iteration : 350, loss : 0.1259, accuracy : 95.72\n",
      "Epoch :  94, training loss : 0.1275, training accuracy : 95.64, test loss : 0.3187, test accuracy : 90.17\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.1147, accuracy : 96.09\n",
      "iteration : 100, loss : 0.1124, accuracy : 96.18\n",
      "iteration : 150, loss : 0.1137, accuracy : 96.14\n",
      "iteration : 200, loss : 0.1174, accuracy : 96.02\n",
      "iteration : 250, loss : 0.1195, accuracy : 95.92\n",
      "iteration : 300, loss : 0.1224, accuracy : 95.79\n",
      "iteration : 350, loss : 0.1239, accuracy : 95.74\n",
      "Epoch :  95, training loss : 0.1233, training accuracy : 95.79, test loss : 0.3155, test accuracy : 90.13\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.1018, accuracy : 96.41\n",
      "iteration : 100, loss : 0.1050, accuracy : 96.27\n",
      "iteration : 150, loss : 0.1065, accuracy : 96.37\n",
      "iteration : 200, loss : 0.1074, accuracy : 96.30\n",
      "iteration : 250, loss : 0.1121, accuracy : 96.14\n",
      "iteration : 300, loss : 0.1167, accuracy : 96.01\n",
      "iteration : 350, loss : 0.1184, accuracy : 95.93\n",
      "Epoch :  96, training loss : 0.1195, training accuracy : 95.89, test loss : 0.3158, test accuracy : 90.25\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.1113, accuracy : 96.31\n",
      "iteration : 100, loss : 0.1056, accuracy : 96.48\n",
      "iteration : 150, loss : 0.1072, accuracy : 96.42\n",
      "iteration : 200, loss : 0.1104, accuracy : 96.25\n",
      "iteration : 250, loss : 0.1143, accuracy : 96.10\n",
      "iteration : 300, loss : 0.1169, accuracy : 95.96\n",
      "iteration : 350, loss : 0.1168, accuracy : 95.94\n",
      "Epoch :  97, training loss : 0.1186, training accuracy : 95.92, test loss : 0.3442, test accuracy : 90.06\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.1043, accuracy : 96.44\n",
      "iteration : 100, loss : 0.1074, accuracy : 96.35\n",
      "iteration : 150, loss : 0.1050, accuracy : 96.39\n",
      "iteration : 200, loss : 0.1040, accuracy : 96.45\n",
      "iteration : 250, loss : 0.1061, accuracy : 96.36\n",
      "iteration : 300, loss : 0.1104, accuracy : 96.21\n",
      "iteration : 350, loss : 0.1152, accuracy : 96.02\n",
      "Epoch :  98, training loss : 0.1171, training accuracy : 95.92, test loss : 0.3062, test accuracy : 91.02\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.0989, accuracy : 96.67\n",
      "iteration : 100, loss : 0.0961, accuracy : 96.73\n",
      "iteration : 150, loss : 0.0954, accuracy : 96.76\n",
      "iteration : 200, loss : 0.1044, accuracy : 96.47\n",
      "iteration : 250, loss : 0.1089, accuracy : 96.34\n",
      "iteration : 300, loss : 0.1074, accuracy : 96.35\n",
      "iteration : 350, loss : 0.1108, accuracy : 96.23\n",
      "Epoch :  99, training loss : 0.1133, training accuracy : 96.14, test loss : 0.3367, test accuracy : 90.40\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.0898, accuracy : 96.94\n",
      "iteration : 100, loss : 0.0925, accuracy : 96.80\n",
      "iteration : 150, loss : 0.0957, accuracy : 96.66\n",
      "iteration : 200, loss : 0.0962, accuracy : 96.62\n",
      "iteration : 250, loss : 0.0985, accuracy : 96.53\n",
      "iteration : 300, loss : 0.1025, accuracy : 96.39\n",
      "iteration : 350, loss : 0.1051, accuracy : 96.33\n",
      "Epoch : 100, training loss : 0.1075, training accuracy : 96.30, test loss : 0.3307, test accuracy : 90.06\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.1040, accuracy : 96.50\n",
      "iteration : 100, loss : 0.1002, accuracy : 96.63\n",
      "iteration : 150, loss : 0.1011, accuracy : 96.60\n",
      "iteration : 200, loss : 0.1042, accuracy : 96.44\n",
      "iteration : 250, loss : 0.1030, accuracy : 96.50\n",
      "iteration : 300, loss : 0.1028, accuracy : 96.48\n",
      "iteration : 350, loss : 0.1057, accuracy : 96.33\n",
      "Epoch : 101, training loss : 0.1083, training accuracy : 96.24, test loss : 0.2796, test accuracy : 91.53\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.0897, accuracy : 97.02\n",
      "iteration : 100, loss : 0.0866, accuracy : 97.09\n",
      "iteration : 150, loss : 0.0897, accuracy : 96.91\n",
      "iteration : 200, loss : 0.0933, accuracy : 96.74\n",
      "iteration : 250, loss : 0.0950, accuracy : 96.67\n",
      "iteration : 300, loss : 0.0993, accuracy : 96.54\n",
      "iteration : 350, loss : 0.1015, accuracy : 96.45\n",
      "Epoch : 102, training loss : 0.1052, training accuracy : 96.32, test loss : 0.3128, test accuracy : 90.33\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.0973, accuracy : 96.77\n",
      "iteration : 100, loss : 0.0978, accuracy : 96.66\n",
      "iteration : 150, loss : 0.0977, accuracy : 96.65\n",
      "iteration : 200, loss : 0.1024, accuracy : 96.47\n",
      "iteration : 250, loss : 0.1019, accuracy : 96.48\n",
      "iteration : 300, loss : 0.1051, accuracy : 96.40\n",
      "iteration : 350, loss : 0.1054, accuracy : 96.41\n",
      "Epoch : 103, training loss : 0.1056, training accuracy : 96.40, test loss : 0.3129, test accuracy : 90.70\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.0910, accuracy : 96.97\n",
      "iteration : 100, loss : 0.0874, accuracy : 97.04\n",
      "iteration : 150, loss : 0.0930, accuracy : 96.84\n",
      "iteration : 200, loss : 0.0954, accuracy : 96.72\n",
      "iteration : 250, loss : 0.0953, accuracy : 96.73\n",
      "iteration : 300, loss : 0.0977, accuracy : 96.66\n",
      "iteration : 350, loss : 0.0991, accuracy : 96.59\n",
      "Epoch : 104, training loss : 0.1016, training accuracy : 96.50, test loss : 0.3725, test accuracy : 89.31\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.0974, accuracy : 96.48\n",
      "iteration : 100, loss : 0.0940, accuracy : 96.71\n",
      "iteration : 150, loss : 0.0965, accuracy : 96.61\n",
      "iteration : 200, loss : 0.0950, accuracy : 96.65\n",
      "iteration : 250, loss : 0.0964, accuracy : 96.62\n",
      "iteration : 300, loss : 0.0943, accuracy : 96.70\n",
      "iteration : 350, loss : 0.0959, accuracy : 96.65\n",
      "Epoch : 105, training loss : 0.0973, training accuracy : 96.61, test loss : 0.3115, test accuracy : 90.65\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.0978, accuracy : 96.55\n",
      "iteration : 100, loss : 0.0942, accuracy : 96.66\n",
      "iteration : 150, loss : 0.0941, accuracy : 96.71\n",
      "iteration : 200, loss : 0.0979, accuracy : 96.60\n",
      "iteration : 250, loss : 0.0979, accuracy : 96.63\n",
      "iteration : 300, loss : 0.0975, accuracy : 96.64\n",
      "iteration : 350, loss : 0.1006, accuracy : 96.54\n",
      "Epoch : 106, training loss : 0.1002, training accuracy : 96.54, test loss : 0.3087, test accuracy : 91.12\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.0833, accuracy : 97.27\n",
      "iteration : 100, loss : 0.0838, accuracy : 97.20\n",
      "iteration : 150, loss : 0.0836, accuracy : 97.19\n",
      "iteration : 200, loss : 0.0852, accuracy : 97.07\n",
      "iteration : 250, loss : 0.0870, accuracy : 97.02\n",
      "iteration : 300, loss : 0.0891, accuracy : 96.91\n",
      "iteration : 350, loss : 0.0904, accuracy : 96.84\n",
      "Epoch : 107, training loss : 0.0925, training accuracy : 96.77, test loss : 0.3097, test accuracy : 91.05\n",
      "\n",
      "Epoch: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0772, accuracy : 97.50\n",
      "iteration : 100, loss : 0.0806, accuracy : 97.24\n",
      "iteration : 150, loss : 0.0848, accuracy : 97.09\n",
      "iteration : 200, loss : 0.0867, accuracy : 96.94\n",
      "iteration : 250, loss : 0.0885, accuracy : 96.86\n",
      "iteration : 300, loss : 0.0907, accuracy : 96.76\n",
      "iteration : 350, loss : 0.0943, accuracy : 96.69\n",
      "Epoch : 108, training loss : 0.0936, training accuracy : 96.71, test loss : 0.2902, test accuracy : 91.27\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.0748, accuracy : 97.27\n",
      "iteration : 100, loss : 0.0753, accuracy : 97.47\n",
      "iteration : 150, loss : 0.0802, accuracy : 97.29\n",
      "iteration : 200, loss : 0.0837, accuracy : 97.17\n",
      "iteration : 250, loss : 0.0855, accuracy : 97.10\n",
      "iteration : 300, loss : 0.0864, accuracy : 97.09\n",
      "iteration : 350, loss : 0.0867, accuracy : 97.08\n",
      "Epoch : 109, training loss : 0.0865, training accuracy : 97.06, test loss : 0.2967, test accuracy : 91.42\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.0900, accuracy : 96.91\n",
      "iteration : 100, loss : 0.0942, accuracy : 96.75\n",
      "iteration : 150, loss : 0.0938, accuracy : 96.72\n",
      "iteration : 200, loss : 0.0934, accuracy : 96.77\n",
      "iteration : 250, loss : 0.0896, accuracy : 96.92\n",
      "iteration : 300, loss : 0.0887, accuracy : 96.90\n",
      "iteration : 350, loss : 0.0917, accuracy : 96.83\n",
      "Epoch : 110, training loss : 0.0918, training accuracy : 96.82, test loss : 0.2863, test accuracy : 91.49\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.0807, accuracy : 97.27\n",
      "iteration : 100, loss : 0.0747, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0763, accuracy : 97.34\n",
      "iteration : 200, loss : 0.0768, accuracy : 97.34\n",
      "iteration : 250, loss : 0.0799, accuracy : 97.27\n",
      "iteration : 300, loss : 0.0814, accuracy : 97.23\n",
      "iteration : 350, loss : 0.0821, accuracy : 97.19\n",
      "Epoch : 111, training loss : 0.0824, training accuracy : 97.18, test loss : 0.3181, test accuracy : 91.45\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.0820, accuracy : 96.84\n",
      "iteration : 100, loss : 0.0804, accuracy : 97.05\n",
      "iteration : 150, loss : 0.0841, accuracy : 96.92\n",
      "iteration : 200, loss : 0.0849, accuracy : 96.88\n",
      "iteration : 250, loss : 0.0853, accuracy : 96.88\n",
      "iteration : 300, loss : 0.0882, accuracy : 96.84\n",
      "iteration : 350, loss : 0.0884, accuracy : 96.84\n",
      "Epoch : 112, training loss : 0.0888, training accuracy : 96.83, test loss : 0.3534, test accuracy : 90.21\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.0756, accuracy : 97.22\n",
      "iteration : 100, loss : 0.0776, accuracy : 97.24\n",
      "iteration : 150, loss : 0.0776, accuracy : 97.28\n",
      "iteration : 200, loss : 0.0770, accuracy : 97.29\n",
      "iteration : 250, loss : 0.0746, accuracy : 97.36\n",
      "iteration : 300, loss : 0.0776, accuracy : 97.29\n",
      "iteration : 350, loss : 0.0802, accuracy : 97.23\n",
      "Epoch : 113, training loss : 0.0823, training accuracy : 97.16, test loss : 0.2889, test accuracy : 91.51\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.0677, accuracy : 97.78\n",
      "iteration : 100, loss : 0.0714, accuracy : 97.70\n",
      "iteration : 150, loss : 0.0740, accuracy : 97.54\n",
      "iteration : 200, loss : 0.0757, accuracy : 97.48\n",
      "iteration : 250, loss : 0.0760, accuracy : 97.39\n",
      "iteration : 300, loss : 0.0768, accuracy : 97.38\n",
      "iteration : 350, loss : 0.0782, accuracy : 97.34\n",
      "Epoch : 114, training loss : 0.0798, training accuracy : 97.29, test loss : 0.3091, test accuracy : 91.43\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.0646, accuracy : 97.70\n",
      "iteration : 100, loss : 0.0619, accuracy : 97.83\n",
      "iteration : 150, loss : 0.0664, accuracy : 97.77\n",
      "iteration : 200, loss : 0.0687, accuracy : 97.64\n",
      "iteration : 250, loss : 0.0725, accuracy : 97.50\n",
      "iteration : 300, loss : 0.0754, accuracy : 97.39\n",
      "iteration : 350, loss : 0.0762, accuracy : 97.37\n",
      "Epoch : 115, training loss : 0.0775, training accuracy : 97.35, test loss : 0.2978, test accuracy : 90.98\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.0758, accuracy : 97.34\n",
      "iteration : 100, loss : 0.0689, accuracy : 97.59\n",
      "iteration : 150, loss : 0.0713, accuracy : 97.59\n",
      "iteration : 200, loss : 0.0720, accuracy : 97.54\n",
      "iteration : 250, loss : 0.0736, accuracy : 97.49\n",
      "iteration : 300, loss : 0.0741, accuracy : 97.46\n",
      "iteration : 350, loss : 0.0766, accuracy : 97.35\n",
      "Epoch : 116, training loss : 0.0785, training accuracy : 97.29, test loss : 0.2939, test accuracy : 91.74\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.0677, accuracy : 97.84\n",
      "iteration : 100, loss : 0.0671, accuracy : 97.78\n",
      "iteration : 150, loss : 0.0699, accuracy : 97.62\n",
      "iteration : 200, loss : 0.0704, accuracy : 97.58\n",
      "iteration : 250, loss : 0.0702, accuracy : 97.59\n",
      "iteration : 300, loss : 0.0703, accuracy : 97.57\n",
      "iteration : 350, loss : 0.0729, accuracy : 97.48\n",
      "Epoch : 117, training loss : 0.0728, training accuracy : 97.48, test loss : 0.3030, test accuracy : 92.03\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.0816, accuracy : 97.11\n",
      "iteration : 100, loss : 0.0747, accuracy : 97.38\n",
      "iteration : 150, loss : 0.0727, accuracy : 97.43\n",
      "iteration : 200, loss : 0.0694, accuracy : 97.60\n",
      "iteration : 250, loss : 0.0680, accuracy : 97.70\n",
      "iteration : 300, loss : 0.0678, accuracy : 97.70\n",
      "iteration : 350, loss : 0.0677, accuracy : 97.72\n",
      "Epoch : 118, training loss : 0.0685, training accuracy : 97.69, test loss : 0.3444, test accuracy : 90.68\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.0694, accuracy : 97.66\n",
      "iteration : 100, loss : 0.0701, accuracy : 97.52\n",
      "iteration : 150, loss : 0.0692, accuracy : 97.60\n",
      "iteration : 200, loss : 0.0698, accuracy : 97.55\n",
      "iteration : 250, loss : 0.0725, accuracy : 97.48\n",
      "iteration : 300, loss : 0.0739, accuracy : 97.43\n",
      "iteration : 350, loss : 0.0751, accuracy : 97.39\n",
      "Epoch : 119, training loss : 0.0743, training accuracy : 97.41, test loss : 0.2629, test accuracy : 92.38\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.0510, accuracy : 98.30\n",
      "iteration : 100, loss : 0.0526, accuracy : 98.15\n",
      "iteration : 150, loss : 0.0549, accuracy : 98.08\n",
      "iteration : 200, loss : 0.0574, accuracy : 98.02\n",
      "iteration : 250, loss : 0.0614, accuracy : 97.88\n",
      "iteration : 300, loss : 0.0633, accuracy : 97.82\n",
      "iteration : 350, loss : 0.0648, accuracy : 97.76\n",
      "Epoch : 120, training loss : 0.0663, training accuracy : 97.70, test loss : 0.3145, test accuracy : 91.22\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.0573, accuracy : 98.20\n",
      "iteration : 100, loss : 0.0569, accuracy : 98.05\n",
      "iteration : 150, loss : 0.0545, accuracy : 98.19\n",
      "iteration : 200, loss : 0.0542, accuracy : 98.19\n",
      "iteration : 250, loss : 0.0581, accuracy : 98.04\n",
      "iteration : 300, loss : 0.0621, accuracy : 97.91\n",
      "iteration : 350, loss : 0.0653, accuracy : 97.83\n",
      "Epoch : 121, training loss : 0.0668, training accuracy : 97.76, test loss : 0.3135, test accuracy : 91.65\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.0518, accuracy : 98.19\n",
      "iteration : 100, loss : 0.0523, accuracy : 98.23\n",
      "iteration : 150, loss : 0.0544, accuracy : 98.16\n",
      "iteration : 200, loss : 0.0552, accuracy : 98.13\n",
      "iteration : 250, loss : 0.0564, accuracy : 98.06\n",
      "iteration : 300, loss : 0.0587, accuracy : 98.00\n",
      "iteration : 350, loss : 0.0601, accuracy : 97.96\n",
      "Epoch : 122, training loss : 0.0626, training accuracy : 97.88, test loss : 0.2920, test accuracy : 91.59\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.0619, accuracy : 97.95\n",
      "iteration : 100, loss : 0.0561, accuracy : 98.11\n",
      "iteration : 150, loss : 0.0551, accuracy : 98.14\n",
      "iteration : 200, loss : 0.0584, accuracy : 98.01\n",
      "iteration : 250, loss : 0.0601, accuracy : 97.94\n",
      "iteration : 300, loss : 0.0594, accuracy : 97.95\n",
      "iteration : 350, loss : 0.0605, accuracy : 97.90\n",
      "Epoch : 123, training loss : 0.0623, training accuracy : 97.85, test loss : 0.3737, test accuracy : 90.63\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.0618, accuracy : 97.92\n",
      "iteration : 100, loss : 0.0547, accuracy : 98.18\n",
      "iteration : 150, loss : 0.0516, accuracy : 98.26\n",
      "iteration : 200, loss : 0.0534, accuracy : 98.18\n",
      "iteration : 250, loss : 0.0552, accuracy : 98.15\n",
      "iteration : 300, loss : 0.0560, accuracy : 98.11\n",
      "iteration : 350, loss : 0.0580, accuracy : 98.02\n",
      "Epoch : 124, training loss : 0.0578, training accuracy : 98.02, test loss : 0.3122, test accuracy : 91.64\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.0571, accuracy : 98.00\n",
      "iteration : 100, loss : 0.0595, accuracy : 97.93\n",
      "iteration : 150, loss : 0.0575, accuracy : 98.00\n",
      "iteration : 200, loss : 0.0596, accuracy : 97.91\n",
      "iteration : 250, loss : 0.0593, accuracy : 97.93\n",
      "iteration : 300, loss : 0.0581, accuracy : 97.99\n",
      "iteration : 350, loss : 0.0589, accuracy : 97.97\n",
      "Epoch : 125, training loss : 0.0590, training accuracy : 97.97, test loss : 0.3058, test accuracy : 92.03\n",
      "\n",
      "Epoch: 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0474, accuracy : 98.44\n",
      "iteration : 100, loss : 0.0472, accuracy : 98.36\n",
      "iteration : 150, loss : 0.0504, accuracy : 98.24\n",
      "iteration : 200, loss : 0.0519, accuracy : 98.21\n",
      "iteration : 250, loss : 0.0515, accuracy : 98.24\n",
      "iteration : 300, loss : 0.0522, accuracy : 98.21\n",
      "iteration : 350, loss : 0.0534, accuracy : 98.15\n",
      "Epoch : 126, training loss : 0.0542, training accuracy : 98.12, test loss : 0.3098, test accuracy : 91.87\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.0419, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0431, accuracy : 98.66\n",
      "iteration : 150, loss : 0.0475, accuracy : 98.49\n",
      "iteration : 200, loss : 0.0499, accuracy : 98.33\n",
      "iteration : 250, loss : 0.0518, accuracy : 98.25\n",
      "iteration : 300, loss : 0.0547, accuracy : 98.14\n",
      "iteration : 350, loss : 0.0565, accuracy : 98.09\n",
      "Epoch : 127, training loss : 0.0570, training accuracy : 98.07, test loss : 0.2912, test accuracy : 92.02\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.0499, accuracy : 98.22\n",
      "iteration : 100, loss : 0.0520, accuracy : 98.09\n",
      "iteration : 150, loss : 0.0537, accuracy : 98.03\n",
      "iteration : 200, loss : 0.0524, accuracy : 98.07\n",
      "iteration : 250, loss : 0.0533, accuracy : 98.09\n",
      "iteration : 300, loss : 0.0542, accuracy : 98.08\n",
      "iteration : 350, loss : 0.0552, accuracy : 98.04\n",
      "Epoch : 128, training loss : 0.0564, training accuracy : 98.01, test loss : 0.3037, test accuracy : 91.76\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.0496, accuracy : 98.38\n",
      "iteration : 100, loss : 0.0495, accuracy : 98.36\n",
      "iteration : 150, loss : 0.0508, accuracy : 98.33\n",
      "iteration : 200, loss : 0.0511, accuracy : 98.30\n",
      "iteration : 250, loss : 0.0505, accuracy : 98.31\n",
      "iteration : 300, loss : 0.0516, accuracy : 98.25\n",
      "iteration : 350, loss : 0.0527, accuracy : 98.21\n",
      "Epoch : 129, training loss : 0.0532, training accuracy : 98.19, test loss : 0.3250, test accuracy : 91.49\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.0387, accuracy : 98.78\n",
      "iteration : 100, loss : 0.0375, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0380, accuracy : 98.71\n",
      "iteration : 200, loss : 0.0386, accuracy : 98.66\n",
      "iteration : 250, loss : 0.0405, accuracy : 98.58\n",
      "iteration : 300, loss : 0.0427, accuracy : 98.53\n",
      "iteration : 350, loss : 0.0451, accuracy : 98.43\n",
      "Epoch : 130, training loss : 0.0452, training accuracy : 98.45, test loss : 0.2686, test accuracy : 92.83\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.0387, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0372, accuracy : 98.78\n",
      "iteration : 150, loss : 0.0382, accuracy : 98.72\n",
      "iteration : 200, loss : 0.0413, accuracy : 98.59\n",
      "iteration : 250, loss : 0.0425, accuracy : 98.57\n",
      "iteration : 300, loss : 0.0427, accuracy : 98.55\n",
      "iteration : 350, loss : 0.0431, accuracy : 98.51\n",
      "Epoch : 131, training loss : 0.0441, training accuracy : 98.48, test loss : 0.3341, test accuracy : 91.33\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.0388, accuracy : 98.64\n",
      "iteration : 100, loss : 0.0416, accuracy : 98.52\n",
      "iteration : 150, loss : 0.0437, accuracy : 98.43\n",
      "iteration : 200, loss : 0.0445, accuracy : 98.38\n",
      "iteration : 250, loss : 0.0458, accuracy : 98.36\n",
      "iteration : 300, loss : 0.0484, accuracy : 98.30\n",
      "iteration : 350, loss : 0.0483, accuracy : 98.31\n",
      "Epoch : 132, training loss : 0.0496, training accuracy : 98.26, test loss : 0.3182, test accuracy : 91.52\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.0427, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0384, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0362, accuracy : 98.81\n",
      "iteration : 200, loss : 0.0360, accuracy : 98.81\n",
      "iteration : 250, loss : 0.0368, accuracy : 98.78\n",
      "iteration : 300, loss : 0.0379, accuracy : 98.72\n",
      "iteration : 350, loss : 0.0399, accuracy : 98.65\n",
      "Epoch : 133, training loss : 0.0409, training accuracy : 98.62, test loss : 0.2676, test accuracy : 92.41\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.0433, accuracy : 98.55\n",
      "iteration : 100, loss : 0.0397, accuracy : 98.66\n",
      "iteration : 150, loss : 0.0380, accuracy : 98.71\n",
      "iteration : 200, loss : 0.0391, accuracy : 98.67\n",
      "iteration : 250, loss : 0.0414, accuracy : 98.59\n",
      "iteration : 300, loss : 0.0426, accuracy : 98.56\n",
      "iteration : 350, loss : 0.0437, accuracy : 98.51\n",
      "Epoch : 134, training loss : 0.0450, training accuracy : 98.47, test loss : 0.3216, test accuracy : 91.66\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.0384, accuracy : 98.83\n",
      "iteration : 100, loss : 0.0380, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0355, accuracy : 98.84\n",
      "iteration : 200, loss : 0.0350, accuracy : 98.91\n",
      "iteration : 250, loss : 0.0350, accuracy : 98.92\n",
      "iteration : 300, loss : 0.0357, accuracy : 98.86\n",
      "iteration : 350, loss : 0.0386, accuracy : 98.75\n",
      "Epoch : 135, training loss : 0.0403, training accuracy : 98.67, test loss : 0.2821, test accuracy : 92.58\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.0414, accuracy : 98.42\n",
      "iteration : 100, loss : 0.0427, accuracy : 98.50\n",
      "iteration : 150, loss : 0.0408, accuracy : 98.58\n",
      "iteration : 200, loss : 0.0398, accuracy : 98.61\n",
      "iteration : 250, loss : 0.0366, accuracy : 98.71\n",
      "iteration : 300, loss : 0.0368, accuracy : 98.73\n",
      "iteration : 350, loss : 0.0374, accuracy : 98.69\n",
      "Epoch : 136, training loss : 0.0387, training accuracy : 98.65, test loss : 0.3056, test accuracy : 92.03\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.0438, accuracy : 98.55\n",
      "iteration : 100, loss : 0.0404, accuracy : 98.65\n",
      "iteration : 150, loss : 0.0386, accuracy : 98.73\n",
      "iteration : 200, loss : 0.0378, accuracy : 98.72\n",
      "iteration : 250, loss : 0.0382, accuracy : 98.71\n",
      "iteration : 300, loss : 0.0397, accuracy : 98.66\n",
      "iteration : 350, loss : 0.0399, accuracy : 98.66\n",
      "Epoch : 137, training loss : 0.0395, training accuracy : 98.67, test loss : 0.3004, test accuracy : 92.33\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.0335, accuracy : 98.98\n",
      "iteration : 100, loss : 0.0332, accuracy : 98.99\n",
      "iteration : 150, loss : 0.0327, accuracy : 98.96\n",
      "iteration : 200, loss : 0.0341, accuracy : 98.90\n",
      "iteration : 250, loss : 0.0367, accuracy : 98.81\n",
      "iteration : 300, loss : 0.0381, accuracy : 98.76\n",
      "iteration : 350, loss : 0.0381, accuracy : 98.76\n",
      "Epoch : 138, training loss : 0.0388, training accuracy : 98.72, test loss : 0.2465, test accuracy : 93.35\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.0314, accuracy : 99.00\n",
      "iteration : 100, loss : 0.0326, accuracy : 98.95\n",
      "iteration : 150, loss : 0.0319, accuracy : 98.95\n",
      "iteration : 200, loss : 0.0305, accuracy : 99.00\n",
      "iteration : 250, loss : 0.0308, accuracy : 98.99\n",
      "iteration : 300, loss : 0.0319, accuracy : 98.96\n",
      "iteration : 350, loss : 0.0322, accuracy : 98.95\n",
      "Epoch : 139, training loss : 0.0326, training accuracy : 98.92, test loss : 0.2717, test accuracy : 92.60\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.0274, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0292, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0300, accuracy : 99.01\n",
      "iteration : 200, loss : 0.0293, accuracy : 99.04\n",
      "iteration : 250, loss : 0.0296, accuracy : 99.03\n",
      "iteration : 300, loss : 0.0301, accuracy : 99.03\n",
      "iteration : 350, loss : 0.0306, accuracy : 99.00\n",
      "Epoch : 140, training loss : 0.0312, training accuracy : 98.98, test loss : 0.3260, test accuracy : 91.94\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.0291, accuracy : 98.95\n",
      "iteration : 100, loss : 0.0305, accuracy : 98.88\n",
      "iteration : 150, loss : 0.0301, accuracy : 98.92\n",
      "iteration : 200, loss : 0.0300, accuracy : 98.95\n",
      "iteration : 250, loss : 0.0303, accuracy : 98.95\n",
      "iteration : 300, loss : 0.0318, accuracy : 98.93\n",
      "iteration : 350, loss : 0.0318, accuracy : 98.94\n",
      "Epoch : 141, training loss : 0.0320, training accuracy : 98.93, test loss : 0.2676, test accuracy : 92.73\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.0300, accuracy : 98.88\n",
      "iteration : 100, loss : 0.0314, accuracy : 98.88\n",
      "iteration : 150, loss : 0.0317, accuracy : 98.87\n",
      "iteration : 200, loss : 0.0305, accuracy : 98.90\n",
      "iteration : 250, loss : 0.0296, accuracy : 98.94\n",
      "iteration : 300, loss : 0.0296, accuracy : 98.94\n",
      "iteration : 350, loss : 0.0310, accuracy : 98.91\n",
      "Epoch : 142, training loss : 0.0320, training accuracy : 98.89, test loss : 0.2864, test accuracy : 92.50\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.0277, accuracy : 99.08\n",
      "iteration : 100, loss : 0.0277, accuracy : 99.07\n",
      "iteration : 150, loss : 0.0269, accuracy : 99.10\n",
      "iteration : 200, loss : 0.0258, accuracy : 99.14\n",
      "iteration : 250, loss : 0.0259, accuracy : 99.13\n",
      "iteration : 300, loss : 0.0263, accuracy : 99.13\n",
      "iteration : 350, loss : 0.0264, accuracy : 99.13\n",
      "Epoch : 143, training loss : 0.0264, training accuracy : 99.12, test loss : 0.2733, test accuracy : 93.20\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0274, accuracy : 99.11\n",
      "iteration : 100, loss : 0.0283, accuracy : 99.07\n",
      "iteration : 150, loss : 0.0277, accuracy : 99.11\n",
      "iteration : 200, loss : 0.0278, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0288, accuracy : 99.08\n",
      "iteration : 300, loss : 0.0300, accuracy : 99.03\n",
      "iteration : 350, loss : 0.0301, accuracy : 99.01\n",
      "Epoch : 144, training loss : 0.0298, training accuracy : 99.03, test loss : 0.2768, test accuracy : 92.83\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.0228, accuracy : 99.14\n",
      "iteration : 100, loss : 0.0254, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0258, accuracy : 99.09\n",
      "iteration : 200, loss : 0.0253, accuracy : 99.12\n",
      "iteration : 250, loss : 0.0247, accuracy : 99.14\n",
      "iteration : 300, loss : 0.0244, accuracy : 99.18\n",
      "iteration : 350, loss : 0.0248, accuracy : 99.17\n",
      "Epoch : 145, training loss : 0.0251, training accuracy : 99.16, test loss : 0.2941, test accuracy : 92.66\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.0211, accuracy : 99.38\n",
      "iteration : 100, loss : 0.0221, accuracy : 99.32\n",
      "iteration : 150, loss : 0.0224, accuracy : 99.32\n",
      "iteration : 200, loss : 0.0232, accuracy : 99.28\n",
      "iteration : 250, loss : 0.0233, accuracy : 99.26\n",
      "iteration : 300, loss : 0.0244, accuracy : 99.21\n",
      "iteration : 350, loss : 0.0252, accuracy : 99.17\n",
      "Epoch : 146, training loss : 0.0255, training accuracy : 99.17, test loss : 0.2766, test accuracy : 92.97\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.0282, accuracy : 99.09\n",
      "iteration : 100, loss : 0.0299, accuracy : 98.99\n",
      "iteration : 150, loss : 0.0272, accuracy : 99.06\n",
      "iteration : 200, loss : 0.0256, accuracy : 99.11\n",
      "iteration : 250, loss : 0.0249, accuracy : 99.12\n",
      "iteration : 300, loss : 0.0257, accuracy : 99.11\n",
      "iteration : 350, loss : 0.0255, accuracy : 99.13\n",
      "Epoch : 147, training loss : 0.0246, training accuracy : 99.16, test loss : 0.2516, test accuracy : 93.57\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.0248, accuracy : 99.16\n",
      "iteration : 100, loss : 0.0231, accuracy : 99.21\n",
      "iteration : 150, loss : 0.0235, accuracy : 99.18\n",
      "iteration : 200, loss : 0.0239, accuracy : 99.18\n",
      "iteration : 250, loss : 0.0237, accuracy : 99.17\n",
      "iteration : 300, loss : 0.0231, accuracy : 99.20\n",
      "iteration : 350, loss : 0.0228, accuracy : 99.22\n",
      "Epoch : 148, training loss : 0.0231, training accuracy : 99.22, test loss : 0.2825, test accuracy : 92.81\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.0161, accuracy : 99.47\n",
      "iteration : 100, loss : 0.0153, accuracy : 99.46\n",
      "iteration : 150, loss : 0.0175, accuracy : 99.39\n",
      "iteration : 200, loss : 0.0195, accuracy : 99.33\n",
      "iteration : 250, loss : 0.0218, accuracy : 99.26\n",
      "iteration : 300, loss : 0.0224, accuracy : 99.26\n",
      "iteration : 350, loss : 0.0222, accuracy : 99.26\n",
      "Epoch : 149, training loss : 0.0219, training accuracy : 99.28, test loss : 0.2513, test accuracy : 93.71\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.0154, accuracy : 99.44\n",
      "iteration : 100, loss : 0.0136, accuracy : 99.59\n",
      "iteration : 150, loss : 0.0130, accuracy : 99.60\n",
      "iteration : 200, loss : 0.0131, accuracy : 99.60\n",
      "iteration : 250, loss : 0.0140, accuracy : 99.56\n",
      "iteration : 300, loss : 0.0149, accuracy : 99.53\n",
      "iteration : 350, loss : 0.0152, accuracy : 99.52\n",
      "Epoch : 150, training loss : 0.0161, training accuracy : 99.50, test loss : 0.2711, test accuracy : 93.40\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.0186, accuracy : 99.41\n",
      "iteration : 100, loss : 0.0192, accuracy : 99.34\n",
      "iteration : 150, loss : 0.0198, accuracy : 99.35\n",
      "iteration : 200, loss : 0.0203, accuracy : 99.32\n",
      "iteration : 250, loss : 0.0210, accuracy : 99.28\n",
      "iteration : 300, loss : 0.0217, accuracy : 99.25\n",
      "iteration : 350, loss : 0.0224, accuracy : 99.23\n",
      "Epoch : 151, training loss : 0.0220, training accuracy : 99.25, test loss : 0.2608, test accuracy : 93.53\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.0171, accuracy : 99.52\n",
      "iteration : 100, loss : 0.0200, accuracy : 99.35\n",
      "iteration : 150, loss : 0.0203, accuracy : 99.36\n",
      "iteration : 200, loss : 0.0193, accuracy : 99.39\n",
      "iteration : 250, loss : 0.0190, accuracy : 99.40\n",
      "iteration : 300, loss : 0.0193, accuracy : 99.38\n",
      "iteration : 350, loss : 0.0196, accuracy : 99.36\n",
      "Epoch : 152, training loss : 0.0200, training accuracy : 99.34, test loss : 0.2849, test accuracy : 92.93\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.0166, accuracy : 99.42\n",
      "iteration : 100, loss : 0.0138, accuracy : 99.53\n",
      "iteration : 150, loss : 0.0148, accuracy : 99.49\n",
      "iteration : 200, loss : 0.0162, accuracy : 99.45\n",
      "iteration : 250, loss : 0.0165, accuracy : 99.44\n",
      "iteration : 300, loss : 0.0160, accuracy : 99.45\n",
      "iteration : 350, loss : 0.0153, accuracy : 99.48\n",
      "Epoch : 153, training loss : 0.0153, training accuracy : 99.49, test loss : 0.2566, test accuracy : 93.64\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.0143, accuracy : 99.50\n",
      "iteration : 100, loss : 0.0128, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0128, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0129, accuracy : 99.57\n",
      "iteration : 250, loss : 0.0132, accuracy : 99.57\n",
      "iteration : 300, loss : 0.0138, accuracy : 99.54\n",
      "iteration : 350, loss : 0.0142, accuracy : 99.52\n",
      "Epoch : 154, training loss : 0.0147, training accuracy : 99.52, test loss : 0.2461, test accuracy : 93.79\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.0160, accuracy : 99.45\n",
      "iteration : 100, loss : 0.0129, accuracy : 99.57\n",
      "iteration : 150, loss : 0.0126, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0125, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0132, accuracy : 99.58\n",
      "iteration : 300, loss : 0.0132, accuracy : 99.58\n",
      "iteration : 350, loss : 0.0133, accuracy : 99.58\n",
      "Epoch : 155, training loss : 0.0138, training accuracy : 99.56, test loss : 0.2644, test accuracy : 93.53\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.0139, accuracy : 99.50\n",
      "iteration : 100, loss : 0.0112, accuracy : 99.65\n",
      "iteration : 150, loss : 0.0112, accuracy : 99.66\n",
      "iteration : 200, loss : 0.0116, accuracy : 99.65\n",
      "iteration : 250, loss : 0.0131, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0140, accuracy : 99.56\n",
      "iteration : 350, loss : 0.0142, accuracy : 99.55\n",
      "Epoch : 156, training loss : 0.0142, training accuracy : 99.55, test loss : 0.2568, test accuracy : 93.87\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.0122, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0126, accuracy : 99.61\n",
      "iteration : 150, loss : 0.0132, accuracy : 99.59\n",
      "iteration : 200, loss : 0.0127, accuracy : 99.60\n",
      "iteration : 250, loss : 0.0129, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0141, accuracy : 99.55\n",
      "iteration : 350, loss : 0.0144, accuracy : 99.53\n",
      "Epoch : 157, training loss : 0.0146, training accuracy : 99.52, test loss : 0.2637, test accuracy : 93.72\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.0134, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0131, accuracy : 99.60\n",
      "iteration : 150, loss : 0.0124, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0122, accuracy : 99.59\n",
      "iteration : 250, loss : 0.0127, accuracy : 99.60\n",
      "iteration : 300, loss : 0.0123, accuracy : 99.62\n",
      "iteration : 350, loss : 0.0122, accuracy : 99.62\n",
      "Epoch : 158, training loss : 0.0123, training accuracy : 99.60, test loss : 0.2821, test accuracy : 93.31\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.0079, accuracy : 99.77\n",
      "iteration : 100, loss : 0.0084, accuracy : 99.72\n",
      "iteration : 150, loss : 0.0085, accuracy : 99.73\n",
      "iteration : 200, loss : 0.0087, accuracy : 99.71\n",
      "iteration : 250, loss : 0.0092, accuracy : 99.68\n",
      "iteration : 300, loss : 0.0092, accuracy : 99.69\n",
      "iteration : 350, loss : 0.0093, accuracy : 99.69\n",
      "Epoch : 159, training loss : 0.0095, training accuracy : 99.69, test loss : 0.2553, test accuracy : 93.89\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.0130, accuracy : 99.59\n",
      "iteration : 100, loss : 0.0115, accuracy : 99.62\n",
      "iteration : 150, loss : 0.0119, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0114, accuracy : 99.63\n",
      "iteration : 250, loss : 0.0107, accuracy : 99.65\n",
      "iteration : 300, loss : 0.0098, accuracy : 99.69\n",
      "iteration : 350, loss : 0.0094, accuracy : 99.70\n",
      "Epoch : 160, training loss : 0.0094, training accuracy : 99.70, test loss : 0.2662, test accuracy : 93.78\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.0115, accuracy : 99.64\n",
      "iteration : 100, loss : 0.0112, accuracy : 99.62\n",
      "iteration : 150, loss : 0.0115, accuracy : 99.61\n",
      "iteration : 200, loss : 0.0114, accuracy : 99.63\n",
      "iteration : 250, loss : 0.0110, accuracy : 99.65\n",
      "iteration : 300, loss : 0.0107, accuracy : 99.65\n",
      "iteration : 350, loss : 0.0113, accuracy : 99.62\n",
      "Epoch : 161, training loss : 0.0113, training accuracy : 99.62, test loss : 0.2747, test accuracy : 93.68\n",
      "\n",
      "Epoch: 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0094, accuracy : 99.81\n",
      "iteration : 100, loss : 0.0080, accuracy : 99.79\n",
      "iteration : 150, loss : 0.0091, accuracy : 99.76\n",
      "iteration : 200, loss : 0.0100, accuracy : 99.71\n",
      "iteration : 250, loss : 0.0100, accuracy : 99.71\n",
      "iteration : 300, loss : 0.0095, accuracy : 99.72\n",
      "iteration : 350, loss : 0.0094, accuracy : 99.72\n",
      "Epoch : 162, training loss : 0.0093, training accuracy : 99.72, test loss : 0.2545, test accuracy : 94.14\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.0091, accuracy : 99.67\n",
      "iteration : 100, loss : 0.0081, accuracy : 99.73\n",
      "iteration : 150, loss : 0.0080, accuracy : 99.73\n",
      "iteration : 200, loss : 0.0082, accuracy : 99.72\n",
      "iteration : 250, loss : 0.0082, accuracy : 99.73\n",
      "iteration : 300, loss : 0.0086, accuracy : 99.72\n",
      "iteration : 350, loss : 0.0088, accuracy : 99.72\n",
      "Epoch : 163, training loss : 0.0085, training accuracy : 99.73, test loss : 0.2654, test accuracy : 93.83\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.0072, accuracy : 99.83\n",
      "iteration : 100, loss : 0.0065, accuracy : 99.84\n",
      "iteration : 150, loss : 0.0058, accuracy : 99.85\n",
      "iteration : 200, loss : 0.0058, accuracy : 99.85\n",
      "iteration : 250, loss : 0.0068, accuracy : 99.81\n",
      "iteration : 300, loss : 0.0070, accuracy : 99.79\n",
      "iteration : 350, loss : 0.0075, accuracy : 99.77\n",
      "Epoch : 164, training loss : 0.0080, training accuracy : 99.74, test loss : 0.2781, test accuracy : 93.43\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.0065, accuracy : 99.78\n",
      "iteration : 100, loss : 0.0069, accuracy : 99.77\n",
      "iteration : 150, loss : 0.0071, accuracy : 99.78\n",
      "iteration : 200, loss : 0.0070, accuracy : 99.79\n",
      "iteration : 250, loss : 0.0065, accuracy : 99.81\n",
      "iteration : 300, loss : 0.0063, accuracy : 99.82\n",
      "iteration : 350, loss : 0.0065, accuracy : 99.81\n",
      "Epoch : 165, training loss : 0.0066, training accuracy : 99.81, test loss : 0.2657, test accuracy : 93.83\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.0048, accuracy : 99.84\n",
      "iteration : 100, loss : 0.0056, accuracy : 99.84\n",
      "iteration : 150, loss : 0.0059, accuracy : 99.84\n",
      "iteration : 200, loss : 0.0060, accuracy : 99.85\n",
      "iteration : 250, loss : 0.0065, accuracy : 99.84\n",
      "iteration : 300, loss : 0.0064, accuracy : 99.84\n",
      "iteration : 350, loss : 0.0063, accuracy : 99.84\n",
      "Epoch : 166, training loss : 0.0064, training accuracy : 99.83, test loss : 0.2720, test accuracy : 93.73\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.0059, accuracy : 99.84\n",
      "iteration : 100, loss : 0.0051, accuracy : 99.85\n",
      "iteration : 150, loss : 0.0046, accuracy : 99.85\n",
      "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
      "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
      "iteration : 300, loss : 0.0056, accuracy : 99.83\n",
      "iteration : 350, loss : 0.0062, accuracy : 99.82\n",
      "Epoch : 167, training loss : 0.0064, training accuracy : 99.81, test loss : 0.2605, test accuracy : 93.84\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.0052, accuracy : 99.86\n",
      "iteration : 100, loss : 0.0049, accuracy : 99.89\n",
      "iteration : 150, loss : 0.0048, accuracy : 99.89\n",
      "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
      "iteration : 250, loss : 0.0051, accuracy : 99.87\n",
      "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
      "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
      "Epoch : 168, training loss : 0.0046, training accuracy : 99.88, test loss : 0.2533, test accuracy : 94.41\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.0051, accuracy : 99.88\n",
      "iteration : 100, loss : 0.0051, accuracy : 99.86\n",
      "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
      "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
      "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
      "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
      "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
      "Epoch : 169, training loss : 0.0050, training accuracy : 99.86, test loss : 0.2414, test accuracy : 94.41\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.0052, accuracy : 99.81\n",
      "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
      "iteration : 150, loss : 0.0040, accuracy : 99.91\n",
      "iteration : 200, loss : 0.0037, accuracy : 99.92\n",
      "iteration : 250, loss : 0.0036, accuracy : 99.91\n",
      "iteration : 300, loss : 0.0034, accuracy : 99.92\n",
      "iteration : 350, loss : 0.0031, accuracy : 99.93\n",
      "Epoch : 170, training loss : 0.0032, training accuracy : 99.93, test loss : 0.2408, test accuracy : 94.40\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.0018, accuracy : 99.97\n",
      "iteration : 100, loss : 0.0031, accuracy : 99.92\n",
      "iteration : 150, loss : 0.0035, accuracy : 99.92\n",
      "iteration : 200, loss : 0.0033, accuracy : 99.93\n",
      "iteration : 250, loss : 0.0033, accuracy : 99.92\n",
      "iteration : 300, loss : 0.0032, accuracy : 99.92\n",
      "iteration : 350, loss : 0.0036, accuracy : 99.90\n",
      "Epoch : 171, training loss : 0.0039, training accuracy : 99.89, test loss : 0.2603, test accuracy : 94.22\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.0033, accuracy : 99.95\n",
      "iteration : 100, loss : 0.0040, accuracy : 99.92\n",
      "iteration : 150, loss : 0.0038, accuracy : 99.93\n",
      "iteration : 200, loss : 0.0035, accuracy : 99.93\n",
      "iteration : 250, loss : 0.0034, accuracy : 99.93\n",
      "iteration : 300, loss : 0.0035, accuracy : 99.92\n",
      "iteration : 350, loss : 0.0033, accuracy : 99.93\n",
      "Epoch : 172, training loss : 0.0032, training accuracy : 99.93, test loss : 0.2498, test accuracy : 94.64\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.0026, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0029, accuracy : 99.93\n",
      "iteration : 150, loss : 0.0034, accuracy : 99.89\n",
      "iteration : 200, loss : 0.0034, accuracy : 99.89\n",
      "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
      "iteration : 300, loss : 0.0037, accuracy : 99.88\n",
      "iteration : 350, loss : 0.0038, accuracy : 99.88\n",
      "Epoch : 173, training loss : 0.0038, training accuracy : 99.88, test loss : 0.2435, test accuracy : 94.40\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
      "iteration : 100, loss : 0.0033, accuracy : 99.92\n",
      "iteration : 150, loss : 0.0033, accuracy : 99.93\n",
      "iteration : 200, loss : 0.0031, accuracy : 99.94\n",
      "iteration : 250, loss : 0.0033, accuracy : 99.93\n",
      "iteration : 300, loss : 0.0032, accuracy : 99.93\n",
      "iteration : 350, loss : 0.0031, accuracy : 99.94\n",
      "Epoch : 174, training loss : 0.0034, training accuracy : 99.93, test loss : 0.2446, test accuracy : 94.65\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
      "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
      "iteration : 150, loss : 0.0035, accuracy : 99.90\n",
      "iteration : 200, loss : 0.0035, accuracy : 99.89\n",
      "iteration : 250, loss : 0.0040, accuracy : 99.88\n",
      "iteration : 300, loss : 0.0039, accuracy : 99.89\n",
      "iteration : 350, loss : 0.0036, accuracy : 99.90\n",
      "Epoch : 175, training loss : 0.0038, training accuracy : 99.90, test loss : 0.2452, test accuracy : 94.53\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.0025, accuracy : 99.92\n",
      "iteration : 100, loss : 0.0024, accuracy : 99.92\n",
      "iteration : 150, loss : 0.0026, accuracy : 99.92\n",
      "iteration : 200, loss : 0.0027, accuracy : 99.92\n",
      "iteration : 250, loss : 0.0025, accuracy : 99.93\n",
      "iteration : 300, loss : 0.0024, accuracy : 99.94\n",
      "iteration : 350, loss : 0.0024, accuracy : 99.93\n",
      "Epoch : 176, training loss : 0.0025, training accuracy : 99.93, test loss : 0.2466, test accuracy : 94.41\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.0028, accuracy : 99.94\n",
      "iteration : 100, loss : 0.0024, accuracy : 99.94\n",
      "iteration : 150, loss : 0.0022, accuracy : 99.95\n",
      "iteration : 200, loss : 0.0020, accuracy : 99.95\n",
      "iteration : 250, loss : 0.0020, accuracy : 99.95\n",
      "iteration : 300, loss : 0.0020, accuracy : 99.95\n",
      "iteration : 350, loss : 0.0020, accuracy : 99.95\n",
      "Epoch : 177, training loss : 0.0019, training accuracy : 99.96, test loss : 0.2317, test accuracy : 94.92\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.0014, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0016, accuracy : 99.97\n",
      "iteration : 150, loss : 0.0021, accuracy : 99.95\n",
      "iteration : 200, loss : 0.0020, accuracy : 99.95\n",
      "iteration : 250, loss : 0.0018, accuracy : 99.95\n",
      "iteration : 300, loss : 0.0021, accuracy : 99.94\n",
      "iteration : 350, loss : 0.0020, accuracy : 99.94\n",
      "Epoch : 178, training loss : 0.0020, training accuracy : 99.95, test loss : 0.2395, test accuracy : 94.79\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.0013, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0016, accuracy : 99.95\n",
      "iteration : 150, loss : 0.0018, accuracy : 99.95\n",
      "iteration : 200, loss : 0.0018, accuracy : 99.95\n",
      "iteration : 250, loss : 0.0017, accuracy : 99.96\n",
      "iteration : 300, loss : 0.0016, accuracy : 99.97\n",
      "iteration : 350, loss : 0.0016, accuracy : 99.97\n",
      "Epoch : 179, training loss : 0.0015, training accuracy : 99.97, test loss : 0.2335, test accuracy : 95.00\n",
      "\n",
      "Epoch: 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0008, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0012, accuracy : 99.98\n",
      "iteration : 300, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 350, loss : 0.0011, accuracy : 99.98\n",
      "Epoch : 180, training loss : 0.0011, training accuracy : 99.98, test loss : 0.2346, test accuracy : 94.94\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.0012, accuracy : 99.97\n",
      "iteration : 100, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 150, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0010, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 300, loss : 0.0012, accuracy : 99.98\n",
      "iteration : 350, loss : 0.0012, accuracy : 99.98\n",
      "Epoch : 181, training loss : 0.0012, training accuracy : 99.98, test loss : 0.2425, test accuracy : 94.86\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.0008, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0008, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0010, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 300, loss : 0.0012, accuracy : 99.98\n",
      "iteration : 350, loss : 0.0012, accuracy : 99.98\n",
      "Epoch : 182, training loss : 0.0012, training accuracy : 99.98, test loss : 0.2360, test accuracy : 94.90\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.0012, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0011, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0013, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0012, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0011, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0011, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0011, accuracy : 99.98\n",
      "Epoch : 183, training loss : 0.0011, training accuracy : 99.99, test loss : 0.2393, test accuracy : 94.88\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.0011, accuracy : 99.97\n",
      "iteration : 100, loss : 0.0013, accuracy : 99.97\n",
      "iteration : 150, loss : 0.0011, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0010, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0009, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0010, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0009, accuracy : 99.99\n",
      "Epoch : 184, training loss : 0.0009, training accuracy : 99.99, test loss : 0.2303, test accuracy : 94.97\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0008, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0009, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0009, accuracy : 99.99\n",
      "Epoch : 185, training loss : 0.0009, training accuracy : 99.99, test loss : 0.2334, test accuracy : 94.90\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.0016, accuracy : 99.95\n",
      "iteration : 100, loss : 0.0015, accuracy : 99.96\n",
      "iteration : 150, loss : 0.0013, accuracy : 99.97\n",
      "iteration : 200, loss : 0.0012, accuracy : 99.97\n",
      "iteration : 250, loss : 0.0011, accuracy : 99.97\n",
      "iteration : 300, loss : 0.0010, accuracy : 99.98\n",
      "iteration : 350, loss : 0.0010, accuracy : 99.98\n",
      "Epoch : 186, training loss : 0.0010, training accuracy : 99.98, test loss : 0.2239, test accuracy : 94.97\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0009, accuracy : 99.98\n",
      "iteration : 150, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0008, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0008, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0008, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0007, accuracy : 99.99\n",
      "Epoch : 187, training loss : 0.0007, training accuracy : 99.99, test loss : 0.2261, test accuracy : 94.91\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 200, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0007, accuracy : 99.99\n",
      "Epoch : 188, training loss : 0.0007, training accuracy : 99.99, test loss : 0.2257, test accuracy : 94.99\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
      "iteration : 150, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0007, accuracy : 99.99\n",
      "Epoch : 189, training loss : 0.0007, training accuracy : 99.99, test loss : 0.2268, test accuracy : 94.97\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 200, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 300, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 350, loss : 0.0007, accuracy : 100.00\n",
      "Epoch : 190, training loss : 0.0006, training accuracy : 100.00, test loss : 0.2259, test accuracy : 94.89\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.0010, accuracy : 99.97\n",
      "iteration : 100, loss : 0.0009, accuracy : 99.98\n",
      "iteration : 150, loss : 0.0010, accuracy : 99.97\n",
      "iteration : 200, loss : 0.0009, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 300, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
      "Epoch : 191, training loss : 0.0007, training accuracy : 99.98, test loss : 0.2193, test accuracy : 95.04\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0008, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0007, accuracy : 99.99\n",
      "Epoch : 192, training loss : 0.0006, training accuracy : 99.99, test loss : 0.2219, test accuracy : 95.08\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.0007, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 150, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 200, loss : 0.0007, accuracy : 99.98\n",
      "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0006, accuracy : 99.99\n",
      "Epoch : 193, training loss : 0.0006, training accuracy : 99.99, test loss : 0.2232, test accuracy : 95.08\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 200, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 300, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 350, loss : 0.0005, accuracy : 100.00\n",
      "Epoch : 194, training loss : 0.0006, training accuracy : 100.00, test loss : 0.2226, test accuracy : 95.06\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0005, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 300, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 350, loss : 0.0005, accuracy : 100.00\n",
      "Epoch : 195, training loss : 0.0005, training accuracy : 100.00, test loss : 0.2210, test accuracy : 95.14\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0006, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0006, accuracy : 100.00\n",
      "Epoch : 196, training loss : 0.0006, training accuracy : 100.00, test loss : 0.2225, test accuracy : 95.03\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 100, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 150, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 200, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 300, loss : 0.0004, accuracy : 100.00\n",
      "iteration : 350, loss : 0.0005, accuracy : 100.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 197, training loss : 0.0004, training accuracy : 100.00, test loss : 0.2206, test accuracy : 95.04\n",
      "\n",
      "Epoch: 198\n",
      "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0005, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 250, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 300, loss : 0.0005, accuracy : 100.00\n",
      "iteration : 350, loss : 0.0005, accuracy : 100.00\n",
      "Epoch : 198, training loss : 0.0005, training accuracy : 100.00, test loss : 0.2236, test accuracy : 94.97\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
      "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 150, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 200, loss : 0.0007, accuracy : 99.99\n",
      "iteration : 250, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 300, loss : 0.0006, accuracy : 99.99\n",
      "iteration : 350, loss : 0.0006, accuracy : 99.99\n",
      "Epoch : 199, training loss : 0.0006, training accuracy : 99.99, test loss : 0.2218, test accuracy : 95.05\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list  = []\n",
    "test_loss_list  = []\n",
    "test_acc_list  = []\n",
    "\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "#optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      #momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "\n",
    "optimizer = AdamS(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "#print(scheduler)\n",
    "for epoch in range(0, 200):\n",
    "    \n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader,scheduler)\n",
    "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
    "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
    "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
    "\n",
    "plt.xlabel(\"N0. of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs No. of epochs\")\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
    "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
    "plt.xlabel(\"NO. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\" Accuracy vs N0. of epochs\")\n",
    "plt.legend(['train', 'test']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
