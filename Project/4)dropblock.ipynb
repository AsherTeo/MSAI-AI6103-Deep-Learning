{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfGrai_Qt7Ny"
   },
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgAiImV0uURP",
    "outputId": "948f3caa-995f-4d1d-a258-f324158afaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# these are commonly used data augmentations\n",
    "# random cropping and random horizontal flip\n",
    "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.4), ratio=(0.3, 3.3), value=(125,122,114), inplace=False)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hldipDVsv-Jt"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, criterion, trainloader,scheduler):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "            print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VkooK-hQu4a6"
   },
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss= criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jEj8J7xqwAxD"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vlCAjBEWwXNo"
   },
   "outputs": [],
   "source": [
    "# defining resnet models\n",
    "from dropblock import DropBlock2D, LinearScheduler\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, drop_prob=0.7, block_size=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # This is the \"stem\"\n",
    "        # For CIFAR (32x32 images), it does not perform downsampling\n",
    "        # It should downsample for ImageNet\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.dropblock = LinearScheduler(\n",
    "            DropBlock2D(drop_prob=drop_prob, block_size=block_size),\n",
    "            start_value=0.,\n",
    "            stop_value=drop_prob,\n",
    "            nr_steps=5e3\n",
    "        )\n",
    "        # four stages with three downsampling\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step() \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        #out = self.layer1(out)\n",
    "        #out = self.layer2(out)\n",
    "        out = self.dropblock(self.layer1(out))\n",
    "        out = self.dropblock(self.layer2(out))\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test_resnet18():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "ArgupDVRwB8i",
    "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "iteration :  50, loss : 1.9129, accuracy : 28.64\n",
      "iteration : 100, loss : 1.7705, accuracy : 34.20\n",
      "iteration : 150, loss : 1.6823, accuracy : 37.67\n",
      "iteration : 200, loss : 1.6112, accuracy : 40.42\n",
      "iteration : 250, loss : 1.5523, accuracy : 42.79\n",
      "iteration : 300, loss : 1.5009, accuracy : 44.76\n",
      "iteration : 350, loss : 1.4567, accuracy : 46.59\n",
      "Epoch :   0, training loss : 1.4263, training accuracy : 47.82, test loss : 1.2662, test accuracy : 57.85\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.1293, accuracy : 59.77\n",
      "iteration : 100, loss : 1.1067, accuracy : 60.27\n",
      "iteration : 150, loss : 1.0795, accuracy : 61.43\n",
      "iteration : 200, loss : 1.0601, accuracy : 62.12\n",
      "iteration : 250, loss : 1.0401, accuracy : 62.79\n",
      "iteration : 300, loss : 1.0272, accuracy : 63.29\n",
      "iteration : 350, loss : 1.0117, accuracy : 63.92\n",
      "Epoch :   1, training loss : 1.0001, training accuracy : 64.43, test loss : 0.9532, test accuracy : 66.85\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 0.9092, accuracy : 67.62\n",
      "iteration : 100, loss : 0.8825, accuracy : 68.89\n",
      "iteration : 150, loss : 0.8735, accuracy : 69.22\n",
      "iteration : 200, loss : 0.8647, accuracy : 69.49\n",
      "iteration : 250, loss : 0.8589, accuracy : 69.73\n",
      "iteration : 300, loss : 0.8509, accuracy : 70.02\n",
      "iteration : 350, loss : 0.8448, accuracy : 70.26\n",
      "Epoch :   2, training loss : 0.8397, training accuracy : 70.47, test loss : 0.7633, test accuracy : 73.70\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 0.8055, accuracy : 72.64\n",
      "iteration : 100, loss : 0.7909, accuracy : 72.81\n",
      "iteration : 150, loss : 0.7778, accuracy : 73.05\n",
      "iteration : 200, loss : 0.7708, accuracy : 73.18\n",
      "iteration : 250, loss : 0.7708, accuracy : 73.14\n",
      "iteration : 300, loss : 0.7681, accuracy : 73.14\n",
      "iteration : 350, loss : 0.7676, accuracy : 73.19\n",
      "Epoch :   3, training loss : 0.7656, training accuracy : 73.27, test loss : 0.7376, test accuracy : 74.44\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 0.7425, accuracy : 74.03\n",
      "iteration : 100, loss : 0.7456, accuracy : 73.86\n",
      "iteration : 150, loss : 0.7431, accuracy : 73.95\n",
      "iteration : 200, loss : 0.7423, accuracy : 73.96\n",
      "iteration : 250, loss : 0.7368, accuracy : 74.18\n",
      "iteration : 300, loss : 0.7330, accuracy : 74.26\n",
      "iteration : 350, loss : 0.7319, accuracy : 74.29\n",
      "Epoch :   4, training loss : 0.7323, training accuracy : 74.32, test loss : 0.6954, test accuracy : 77.54\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 0.7231, accuracy : 74.56\n",
      "iteration : 100, loss : 0.7135, accuracy : 75.25\n",
      "iteration : 150, loss : 0.7093, accuracy : 75.43\n",
      "iteration : 200, loss : 0.7098, accuracy : 75.29\n",
      "iteration : 250, loss : 0.7094, accuracy : 75.33\n",
      "iteration : 300, loss : 0.7106, accuracy : 75.18\n",
      "iteration : 350, loss : 0.7102, accuracy : 75.25\n",
      "Epoch :   5, training loss : 0.7096, training accuracy : 75.26, test loss : 0.6475, test accuracy : 79.02\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 0.7225, accuracy : 74.66\n",
      "iteration : 100, loss : 0.7187, accuracy : 74.75\n",
      "iteration : 150, loss : 0.7123, accuracy : 74.97\n",
      "iteration : 200, loss : 0.7097, accuracy : 75.06\n",
      "iteration : 250, loss : 0.7054, accuracy : 75.28\n",
      "iteration : 300, loss : 0.7033, accuracy : 75.30\n",
      "iteration : 350, loss : 0.6996, accuracy : 75.50\n",
      "Epoch :   6, training loss : 0.7005, training accuracy : 75.47, test loss : 0.6272, test accuracy : 79.31\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 0.6859, accuracy : 75.94\n",
      "iteration : 100, loss : 0.6699, accuracy : 76.29\n",
      "iteration : 150, loss : 0.6761, accuracy : 76.18\n",
      "iteration : 200, loss : 0.6868, accuracy : 75.85\n",
      "iteration : 250, loss : 0.6900, accuracy : 75.83\n",
      "iteration : 300, loss : 0.6911, accuracy : 75.81\n",
      "iteration : 350, loss : 0.6923, accuracy : 75.82\n",
      "Epoch :   7, training loss : 0.6907, training accuracy : 75.91, test loss : 0.5741, test accuracy : 80.56\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 0.6938, accuracy : 75.81\n",
      "iteration : 100, loss : 0.6842, accuracy : 75.98\n",
      "iteration : 150, loss : 0.6994, accuracy : 75.53\n",
      "iteration : 200, loss : 0.6959, accuracy : 75.64\n",
      "iteration : 250, loss : 0.6968, accuracy : 75.66\n",
      "iteration : 300, loss : 0.6968, accuracy : 75.65\n",
      "iteration : 350, loss : 0.6980, accuracy : 75.64\n",
      "Epoch :   8, training loss : 0.6975, training accuracy : 75.69, test loss : 0.6034, test accuracy : 80.08\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 0.6873, accuracy : 75.94\n",
      "iteration : 100, loss : 0.6962, accuracy : 75.69\n",
      "iteration : 150, loss : 0.6903, accuracy : 76.11\n",
      "iteration : 200, loss : 0.6830, accuracy : 76.31\n",
      "iteration : 250, loss : 0.6832, accuracy : 76.33\n",
      "iteration : 300, loss : 0.6867, accuracy : 76.26\n",
      "iteration : 350, loss : 0.6884, accuracy : 76.21\n",
      "Epoch :   9, training loss : 0.6883, training accuracy : 76.19, test loss : 0.5060, test accuracy : 82.82\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 0.6722, accuracy : 76.58\n",
      "iteration : 100, loss : 0.6871, accuracy : 75.97\n",
      "iteration : 150, loss : 0.6923, accuracy : 75.73\n",
      "iteration : 200, loss : 0.6978, accuracy : 75.45\n",
      "iteration : 250, loss : 0.6999, accuracy : 75.44\n",
      "iteration : 300, loss : 0.6978, accuracy : 75.61\n",
      "iteration : 350, loss : 0.6978, accuracy : 75.77\n",
      "Epoch :  10, training loss : 0.6978, training accuracy : 75.73, test loss : 0.5725, test accuracy : 81.12\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 0.6968, accuracy : 75.95\n",
      "iteration : 100, loss : 0.6998, accuracy : 75.88\n",
      "iteration : 150, loss : 0.6946, accuracy : 75.90\n",
      "iteration : 200, loss : 0.7013, accuracy : 75.65\n",
      "iteration : 250, loss : 0.6993, accuracy : 75.53\n",
      "iteration : 300, loss : 0.6995, accuracy : 75.51\n",
      "iteration : 350, loss : 0.6962, accuracy : 75.62\n",
      "Epoch :  11, training loss : 0.6963, training accuracy : 75.66, test loss : 0.5900, test accuracy : 80.43\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 0.6969, accuracy : 75.88\n",
      "iteration : 100, loss : 0.6818, accuracy : 76.46\n",
      "iteration : 150, loss : 0.6769, accuracy : 76.49\n",
      "iteration : 200, loss : 0.6741, accuracy : 76.61\n",
      "iteration : 250, loss : 0.6746, accuracy : 76.47\n",
      "iteration : 300, loss : 0.6705, accuracy : 76.54\n",
      "iteration : 350, loss : 0.6713, accuracy : 76.44\n",
      "Epoch :  12, training loss : 0.6712, training accuracy : 76.49, test loss : 0.4800, test accuracy : 83.93\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 0.6569, accuracy : 76.80\n",
      "iteration : 100, loss : 0.6557, accuracy : 76.94\n",
      "iteration : 150, loss : 0.6581, accuracy : 76.82\n",
      "iteration : 200, loss : 0.6610, accuracy : 76.75\n",
      "iteration : 250, loss : 0.6606, accuracy : 76.83\n",
      "iteration : 300, loss : 0.6599, accuracy : 77.00\n",
      "iteration : 350, loss : 0.6562, accuracy : 77.10\n",
      "Epoch :  13, training loss : 0.6547, training accuracy : 77.15, test loss : 0.6208, test accuracy : 79.78\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 0.6327, accuracy : 77.67\n",
      "iteration : 100, loss : 0.6396, accuracy : 77.32\n",
      "iteration : 150, loss : 0.6371, accuracy : 77.46\n",
      "iteration : 200, loss : 0.6329, accuracy : 77.69\n",
      "iteration : 250, loss : 0.6369, accuracy : 77.61\n",
      "iteration : 300, loss : 0.6314, accuracy : 77.80\n",
      "iteration : 350, loss : 0.6297, accuracy : 77.87\n",
      "Epoch :  14, training loss : 0.6326, training accuracy : 77.82, test loss : 0.4642, test accuracy : 84.23\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 0.6275, accuracy : 77.94\n",
      "iteration : 100, loss : 0.6276, accuracy : 77.90\n",
      "iteration : 150, loss : 0.6199, accuracy : 78.14\n",
      "iteration : 200, loss : 0.6154, accuracy : 78.27\n",
      "iteration : 250, loss : 0.6166, accuracy : 78.30\n",
      "iteration : 300, loss : 0.6153, accuracy : 78.47\n",
      "iteration : 350, loss : 0.6160, accuracy : 78.46\n",
      "Epoch :  15, training loss : 0.6176, training accuracy : 78.40, test loss : 0.5304, test accuracy : 82.49\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 0.5935, accuracy : 79.12\n",
      "iteration : 100, loss : 0.5968, accuracy : 79.19\n",
      "iteration : 150, loss : 0.5885, accuracy : 79.48\n",
      "iteration : 200, loss : 0.5869, accuracy : 79.51\n",
      "iteration : 250, loss : 0.5900, accuracy : 79.43\n",
      "iteration : 300, loss : 0.5852, accuracy : 79.58\n",
      "iteration : 350, loss : 0.5852, accuracy : 79.58\n",
      "Epoch :  16, training loss : 0.5847, training accuracy : 79.54, test loss : 0.4909, test accuracy : 83.43\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 0.5726, accuracy : 79.78\n",
      "iteration : 100, loss : 0.5736, accuracy : 79.64\n",
      "iteration : 150, loss : 0.5722, accuracy : 79.79\n",
      "iteration : 200, loss : 0.5739, accuracy : 79.82\n",
      "iteration : 250, loss : 0.5758, accuracy : 79.77\n",
      "iteration : 300, loss : 0.5756, accuracy : 79.79\n",
      "iteration : 350, loss : 0.5779, accuracy : 79.71\n",
      "Epoch :  17, training loss : 0.5769, training accuracy : 79.77, test loss : 0.4509, test accuracy : 85.06\n",
      "\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.5705, accuracy : 80.38\n",
      "iteration : 100, loss : 0.5634, accuracy : 80.33\n",
      "iteration : 150, loss : 0.5600, accuracy : 80.40\n",
      "iteration : 200, loss : 0.5626, accuracy : 80.31\n",
      "iteration : 250, loss : 0.5605, accuracy : 80.34\n",
      "iteration : 300, loss : 0.5599, accuracy : 80.34\n",
      "iteration : 350, loss : 0.5605, accuracy : 80.30\n",
      "Epoch :  18, training loss : 0.5622, training accuracy : 80.31, test loss : 0.3869, test accuracy : 86.72\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 0.5470, accuracy : 81.28\n",
      "iteration : 100, loss : 0.5570, accuracy : 80.50\n",
      "iteration : 150, loss : 0.5559, accuracy : 80.47\n",
      "iteration : 200, loss : 0.5559, accuracy : 80.49\n",
      "iteration : 250, loss : 0.5533, accuracy : 80.67\n",
      "iteration : 300, loss : 0.5512, accuracy : 80.67\n",
      "iteration : 350, loss : 0.5507, accuracy : 80.63\n",
      "Epoch :  19, training loss : 0.5501, training accuracy : 80.61, test loss : 0.4437, test accuracy : 85.22\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 0.5459, accuracy : 81.02\n",
      "iteration : 100, loss : 0.5397, accuracy : 81.09\n",
      "iteration : 150, loss : 0.5382, accuracy : 81.12\n",
      "iteration : 200, loss : 0.5402, accuracy : 81.04\n",
      "iteration : 250, loss : 0.5417, accuracy : 81.03\n",
      "iteration : 300, loss : 0.5405, accuracy : 81.05\n",
      "iteration : 350, loss : 0.5418, accuracy : 81.01\n",
      "Epoch :  20, training loss : 0.5435, training accuracy : 80.94, test loss : 0.4582, test accuracy : 84.76\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 0.5349, accuracy : 81.20\n",
      "iteration : 100, loss : 0.5335, accuracy : 81.10\n",
      "iteration : 150, loss : 0.5345, accuracy : 81.20\n",
      "iteration : 200, loss : 0.5324, accuracy : 81.38\n",
      "iteration : 250, loss : 0.5330, accuracy : 81.35\n",
      "iteration : 300, loss : 0.5297, accuracy : 81.41\n",
      "iteration : 350, loss : 0.5302, accuracy : 81.40\n",
      "Epoch :  21, training loss : 0.5309, training accuracy : 81.38, test loss : 0.4078, test accuracy : 86.13\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 0.5211, accuracy : 81.81\n",
      "iteration : 100, loss : 0.5186, accuracy : 81.86\n",
      "iteration : 150, loss : 0.5150, accuracy : 82.05\n",
      "iteration : 200, loss : 0.5193, accuracy : 81.99\n",
      "iteration : 250, loss : 0.5197, accuracy : 81.87\n",
      "iteration : 300, loss : 0.5211, accuracy : 81.81\n",
      "iteration : 350, loss : 0.5209, accuracy : 81.81\n",
      "Epoch :  22, training loss : 0.5196, training accuracy : 81.89, test loss : 0.4688, test accuracy : 84.04\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 0.4847, accuracy : 82.98\n",
      "iteration : 100, loss : 0.5054, accuracy : 82.46\n",
      "iteration : 150, loss : 0.5066, accuracy : 82.47\n",
      "iteration : 200, loss : 0.5093, accuracy : 82.36\n",
      "iteration : 250, loss : 0.5080, accuracy : 82.40\n",
      "iteration : 300, loss : 0.5083, accuracy : 82.22\n",
      "iteration : 350, loss : 0.5082, accuracy : 82.23\n",
      "Epoch :  23, training loss : 0.5102, training accuracy : 82.16, test loss : 0.4222, test accuracy : 86.09\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 0.4811, accuracy : 82.94\n",
      "iteration : 100, loss : 0.4847, accuracy : 83.09\n",
      "iteration : 150, loss : 0.4845, accuracy : 83.08\n",
      "iteration : 200, loss : 0.4934, accuracy : 82.68\n",
      "iteration : 250, loss : 0.4975, accuracy : 82.63\n",
      "iteration : 300, loss : 0.4980, accuracy : 82.66\n",
      "iteration : 350, loss : 0.4998, accuracy : 82.66\n",
      "Epoch :  24, training loss : 0.4998, training accuracy : 82.64, test loss : 0.3952, test accuracy : 86.85\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 0.4742, accuracy : 83.16\n",
      "iteration : 100, loss : 0.4877, accuracy : 82.79\n",
      "iteration : 150, loss : 0.4898, accuracy : 82.82\n",
      "iteration : 200, loss : 0.4921, accuracy : 82.81\n",
      "iteration : 250, loss : 0.4924, accuracy : 82.81\n",
      "iteration : 300, loss : 0.4912, accuracy : 82.84\n",
      "iteration : 350, loss : 0.4918, accuracy : 82.82\n",
      "Epoch :  25, training loss : 0.4921, training accuracy : 82.82, test loss : 0.3640, test accuracy : 87.65\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 0.4945, accuracy : 82.81\n",
      "iteration : 100, loss : 0.4937, accuracy : 82.63\n",
      "iteration : 150, loss : 0.4891, accuracy : 82.73\n",
      "iteration : 200, loss : 0.4856, accuracy : 82.78\n",
      "iteration : 250, loss : 0.4817, accuracy : 82.95\n",
      "iteration : 300, loss : 0.4784, accuracy : 83.09\n",
      "iteration : 350, loss : 0.4815, accuracy : 83.03\n",
      "Epoch :  26, training loss : 0.4803, training accuracy : 83.06, test loss : 0.3674, test accuracy : 87.38\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.4544, accuracy : 84.09\n",
      "iteration : 100, loss : 0.4665, accuracy : 83.84\n",
      "iteration : 150, loss : 0.4632, accuracy : 83.82\n",
      "iteration : 200, loss : 0.4696, accuracy : 83.60\n",
      "iteration : 250, loss : 0.4701, accuracy : 83.53\n",
      "iteration : 300, loss : 0.4700, accuracy : 83.48\n",
      "iteration : 350, loss : 0.4708, accuracy : 83.42\n",
      "Epoch :  27, training loss : 0.4730, training accuracy : 83.43, test loss : 0.4720, test accuracy : 84.77\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 0.4586, accuracy : 84.22\n",
      "iteration : 100, loss : 0.4624, accuracy : 83.65\n",
      "iteration : 150, loss : 0.4625, accuracy : 83.71\n",
      "iteration : 200, loss : 0.4631, accuracy : 83.66\n",
      "iteration : 250, loss : 0.4627, accuracy : 83.68\n",
      "iteration : 300, loss : 0.4676, accuracy : 83.53\n",
      "iteration : 350, loss : 0.4676, accuracy : 83.53\n",
      "Epoch :  28, training loss : 0.4660, training accuracy : 83.56, test loss : 0.3599, test accuracy : 87.92\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 0.4598, accuracy : 83.52\n",
      "iteration : 100, loss : 0.4552, accuracy : 83.95\n",
      "iteration : 150, loss : 0.4587, accuracy : 83.72\n",
      "iteration : 200, loss : 0.4609, accuracy : 83.74\n",
      "iteration : 250, loss : 0.4612, accuracy : 83.68\n",
      "iteration : 300, loss : 0.4617, accuracy : 83.74\n",
      "iteration : 350, loss : 0.4592, accuracy : 83.77\n",
      "Epoch :  29, training loss : 0.4614, training accuracy : 83.70, test loss : 0.3485, test accuracy : 88.31\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 0.4459, accuracy : 83.92\n",
      "iteration : 100, loss : 0.4501, accuracy : 83.82\n",
      "iteration : 150, loss : 0.4487, accuracy : 84.02\n",
      "iteration : 200, loss : 0.4526, accuracy : 83.99\n",
      "iteration : 250, loss : 0.4534, accuracy : 84.04\n",
      "iteration : 300, loss : 0.4532, accuracy : 84.08\n",
      "iteration : 350, loss : 0.4514, accuracy : 84.20\n",
      "Epoch :  30, training loss : 0.4520, training accuracy : 84.19, test loss : 0.3808, test accuracy : 87.27\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.4368, accuracy : 84.64\n",
      "iteration : 100, loss : 0.4426, accuracy : 84.50\n",
      "iteration : 150, loss : 0.4456, accuracy : 84.48\n",
      "iteration : 200, loss : 0.4474, accuracy : 84.42\n",
      "iteration : 250, loss : 0.4468, accuracy : 84.43\n",
      "iteration : 300, loss : 0.4457, accuracy : 84.41\n",
      "iteration : 350, loss : 0.4452, accuracy : 84.50\n",
      "Epoch :  31, training loss : 0.4444, training accuracy : 84.53, test loss : 0.4087, test accuracy : 86.30\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 0.4263, accuracy : 84.50\n",
      "iteration : 100, loss : 0.4366, accuracy : 84.48\n",
      "iteration : 150, loss : 0.4407, accuracy : 84.42\n",
      "iteration : 200, loss : 0.4399, accuracy : 84.53\n",
      "iteration : 250, loss : 0.4390, accuracy : 84.49\n",
      "iteration : 300, loss : 0.4393, accuracy : 84.52\n",
      "iteration : 350, loss : 0.4401, accuracy : 84.48\n",
      "Epoch :  32, training loss : 0.4408, training accuracy : 84.48, test loss : 0.3919, test accuracy : 86.57\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 0.4450, accuracy : 84.00\n",
      "iteration : 100, loss : 0.4459, accuracy : 84.34\n",
      "iteration : 150, loss : 0.4363, accuracy : 84.78\n",
      "iteration : 200, loss : 0.4402, accuracy : 84.66\n",
      "iteration : 250, loss : 0.4394, accuracy : 84.68\n",
      "iteration : 300, loss : 0.4384, accuracy : 84.74\n",
      "iteration : 350, loss : 0.4367, accuracy : 84.79\n",
      "Epoch :  33, training loss : 0.4354, training accuracy : 84.84, test loss : 0.4422, test accuracy : 85.65\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.4296, accuracy : 84.66\n",
      "iteration : 100, loss : 0.4325, accuracy : 84.48\n",
      "iteration : 150, loss : 0.4317, accuracy : 84.70\n",
      "iteration : 200, loss : 0.4315, accuracy : 84.78\n",
      "iteration : 250, loss : 0.4300, accuracy : 84.96\n",
      "iteration : 300, loss : 0.4306, accuracy : 84.85\n",
      "iteration : 350, loss : 0.4322, accuracy : 84.84\n",
      "Epoch :  34, training loss : 0.4313, training accuracy : 84.93, test loss : 0.3393, test accuracy : 88.63\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 0.4184, accuracy : 84.61\n",
      "iteration : 100, loss : 0.4122, accuracy : 85.10\n",
      "iteration : 150, loss : 0.4115, accuracy : 85.33\n",
      "iteration : 200, loss : 0.4200, accuracy : 85.08\n",
      "iteration : 250, loss : 0.4240, accuracy : 85.00\n",
      "iteration : 300, loss : 0.4239, accuracy : 84.96\n",
      "iteration : 350, loss : 0.4262, accuracy : 84.85\n",
      "Epoch :  35, training loss : 0.4268, training accuracy : 84.83, test loss : 0.3713, test accuracy : 87.72\n",
      "\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.3906, accuracy : 85.95\n",
      "iteration : 100, loss : 0.4031, accuracy : 85.75\n",
      "iteration : 150, loss : 0.4069, accuracy : 85.69\n",
      "iteration : 200, loss : 0.4149, accuracy : 85.44\n",
      "iteration : 250, loss : 0.4177, accuracy : 85.33\n",
      "iteration : 300, loss : 0.4164, accuracy : 85.41\n",
      "iteration : 350, loss : 0.4160, accuracy : 85.41\n",
      "Epoch :  36, training loss : 0.4162, training accuracy : 85.40, test loss : 0.3955, test accuracy : 86.98\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.4030, accuracy : 85.83\n",
      "iteration : 100, loss : 0.4094, accuracy : 85.80\n",
      "iteration : 150, loss : 0.4124, accuracy : 85.75\n",
      "iteration : 200, loss : 0.4129, accuracy : 85.64\n",
      "iteration : 250, loss : 0.4116, accuracy : 85.79\n",
      "iteration : 300, loss : 0.4138, accuracy : 85.66\n",
      "iteration : 350, loss : 0.4126, accuracy : 85.73\n",
      "Epoch :  37, training loss : 0.4124, training accuracy : 85.70, test loss : 0.3516, test accuracy : 88.55\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 0.4012, accuracy : 85.41\n",
      "iteration : 100, loss : 0.4028, accuracy : 85.68\n",
      "iteration : 150, loss : 0.4069, accuracy : 85.55\n",
      "iteration : 200, loss : 0.4030, accuracy : 85.75\n",
      "iteration : 250, loss : 0.4021, accuracy : 85.84\n",
      "iteration : 300, loss : 0.4037, accuracy : 85.81\n",
      "iteration : 350, loss : 0.4050, accuracy : 85.79\n",
      "Epoch :  38, training loss : 0.4057, training accuracy : 85.73, test loss : 0.3303, test accuracy : 88.84\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.4143, accuracy : 85.47\n",
      "iteration : 100, loss : 0.4128, accuracy : 85.56\n",
      "iteration : 150, loss : 0.4076, accuracy : 85.84\n",
      "iteration : 200, loss : 0.4108, accuracy : 85.70\n",
      "iteration : 250, loss : 0.4116, accuracy : 85.67\n",
      "iteration : 300, loss : 0.4113, accuracy : 85.73\n",
      "iteration : 350, loss : 0.4083, accuracy : 85.81\n",
      "Epoch :  39, training loss : 0.4071, training accuracy : 85.85, test loss : 0.3821, test accuracy : 87.41\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.3946, accuracy : 85.83\n",
      "iteration : 100, loss : 0.3834, accuracy : 86.34\n",
      "iteration : 150, loss : 0.3856, accuracy : 86.41\n",
      "iteration : 200, loss : 0.3885, accuracy : 86.38\n",
      "iteration : 250, loss : 0.3909, accuracy : 86.24\n",
      "iteration : 300, loss : 0.3923, accuracy : 86.19\n",
      "iteration : 350, loss : 0.3927, accuracy : 86.21\n",
      "Epoch :  40, training loss : 0.3935, training accuracy : 86.18, test loss : 0.3929, test accuracy : 87.00\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.3843, accuracy : 86.58\n",
      "iteration : 100, loss : 0.3899, accuracy : 86.33\n",
      "iteration : 150, loss : 0.3923, accuracy : 86.26\n",
      "iteration : 200, loss : 0.3883, accuracy : 86.46\n",
      "iteration : 250, loss : 0.3909, accuracy : 86.42\n",
      "iteration : 300, loss : 0.3960, accuracy : 86.22\n",
      "iteration : 350, loss : 0.3944, accuracy : 86.27\n",
      "Epoch :  41, training loss : 0.3968, training accuracy : 86.16, test loss : 0.3944, test accuracy : 86.92\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.3895, accuracy : 86.36\n",
      "iteration : 100, loss : 0.3809, accuracy : 86.78\n",
      "iteration : 150, loss : 0.3892, accuracy : 86.48\n",
      "iteration : 200, loss : 0.3888, accuracy : 86.47\n",
      "iteration : 250, loss : 0.3911, accuracy : 86.29\n",
      "iteration : 300, loss : 0.3932, accuracy : 86.20\n",
      "iteration : 350, loss : 0.3930, accuracy : 86.20\n",
      "Epoch :  42, training loss : 0.3942, training accuracy : 86.15, test loss : 0.3268, test accuracy : 89.31\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.3920, accuracy : 86.67\n",
      "iteration : 100, loss : 0.3941, accuracy : 86.34\n",
      "iteration : 150, loss : 0.3855, accuracy : 86.61\n",
      "iteration : 200, loss : 0.3879, accuracy : 86.53\n",
      "iteration : 250, loss : 0.3844, accuracy : 86.64\n",
      "iteration : 300, loss : 0.3846, accuracy : 86.62\n",
      "iteration : 350, loss : 0.3861, accuracy : 86.57\n",
      "Epoch :  43, training loss : 0.3850, training accuracy : 86.60, test loss : 0.3327, test accuracy : 88.68\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.3618, accuracy : 87.38\n",
      "iteration : 100, loss : 0.3590, accuracy : 87.36\n",
      "iteration : 150, loss : 0.3595, accuracy : 87.39\n",
      "iteration : 200, loss : 0.3626, accuracy : 87.23\n",
      "iteration : 250, loss : 0.3665, accuracy : 87.13\n",
      "iteration : 300, loss : 0.3687, accuracy : 87.10\n",
      "iteration : 350, loss : 0.3719, accuracy : 86.99\n",
      "Epoch :  44, training loss : 0.3735, training accuracy : 86.90, test loss : 0.3374, test accuracy : 88.86\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.3750, accuracy : 86.81\n",
      "iteration : 100, loss : 0.3845, accuracy : 86.47\n",
      "iteration : 150, loss : 0.3790, accuracy : 86.74\n",
      "iteration : 200, loss : 0.3761, accuracy : 86.89\n",
      "iteration : 250, loss : 0.3735, accuracy : 87.00\n",
      "iteration : 300, loss : 0.3734, accuracy : 87.00\n",
      "iteration : 350, loss : 0.3742, accuracy : 86.96\n",
      "Epoch :  45, training loss : 0.3736, training accuracy : 87.01, test loss : 0.3312, test accuracy : 89.10\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.3599, accuracy : 87.33\n",
      "iteration : 100, loss : 0.3623, accuracy : 87.05\n",
      "iteration : 150, loss : 0.3595, accuracy : 87.29\n",
      "iteration : 200, loss : 0.3708, accuracy : 86.95\n",
      "iteration : 250, loss : 0.3719, accuracy : 86.91\n",
      "iteration : 300, loss : 0.3714, accuracy : 86.94\n",
      "iteration : 350, loss : 0.3714, accuracy : 86.94\n",
      "Epoch :  46, training loss : 0.3708, training accuracy : 86.94, test loss : 0.3673, test accuracy : 87.64\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.3694, accuracy : 87.44\n",
      "iteration : 100, loss : 0.3678, accuracy : 87.29\n",
      "iteration : 150, loss : 0.3657, accuracy : 87.39\n",
      "iteration : 200, loss : 0.3672, accuracy : 87.34\n",
      "iteration : 250, loss : 0.3676, accuracy : 87.32\n",
      "iteration : 300, loss : 0.3693, accuracy : 87.24\n",
      "iteration : 350, loss : 0.3710, accuracy : 87.19\n",
      "Epoch :  47, training loss : 0.3716, training accuracy : 87.19, test loss : 0.3149, test accuracy : 89.20\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.3517, accuracy : 87.42\n",
      "iteration : 100, loss : 0.3604, accuracy : 87.25\n",
      "iteration : 150, loss : 0.3688, accuracy : 86.88\n",
      "iteration : 200, loss : 0.3737, accuracy : 86.77\n",
      "iteration : 250, loss : 0.3735, accuracy : 86.87\n",
      "iteration : 300, loss : 0.3705, accuracy : 86.93\n",
      "iteration : 350, loss : 0.3676, accuracy : 87.07\n",
      "Epoch :  48, training loss : 0.3696, training accuracy : 87.02, test loss : 0.3244, test accuracy : 89.38\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.3486, accuracy : 87.92\n",
      "iteration : 100, loss : 0.3512, accuracy : 87.66\n",
      "iteration : 150, loss : 0.3505, accuracy : 87.78\n",
      "iteration : 200, loss : 0.3516, accuracy : 87.70\n",
      "iteration : 250, loss : 0.3556, accuracy : 87.60\n",
      "iteration : 300, loss : 0.3535, accuracy : 87.70\n",
      "iteration : 350, loss : 0.3555, accuracy : 87.65\n",
      "Epoch :  49, training loss : 0.3549, training accuracy : 87.67, test loss : 0.3358, test accuracy : 88.98\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.3501, accuracy : 87.86\n",
      "iteration : 100, loss : 0.3533, accuracy : 87.77\n",
      "iteration : 150, loss : 0.3500, accuracy : 87.99\n",
      "iteration : 200, loss : 0.3503, accuracy : 87.96\n",
      "iteration : 250, loss : 0.3516, accuracy : 87.88\n",
      "iteration : 300, loss : 0.3547, accuracy : 87.75\n",
      "iteration : 350, loss : 0.3544, accuracy : 87.73\n",
      "Epoch :  50, training loss : 0.3562, training accuracy : 87.65, test loss : 0.3467, test accuracy : 88.50\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.3629, accuracy : 87.47\n",
      "iteration : 100, loss : 0.3606, accuracy : 87.46\n",
      "iteration : 150, loss : 0.3585, accuracy : 87.39\n",
      "iteration : 200, loss : 0.3566, accuracy : 87.56\n",
      "iteration : 250, loss : 0.3545, accuracy : 87.61\n",
      "iteration : 300, loss : 0.3550, accuracy : 87.55\n",
      "iteration : 350, loss : 0.3557, accuracy : 87.52\n",
      "Epoch :  51, training loss : 0.3586, training accuracy : 87.39, test loss : 0.3172, test accuracy : 89.74\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.3332, accuracy : 88.19\n",
      "iteration : 100, loss : 0.3422, accuracy : 88.23\n",
      "iteration : 150, loss : 0.3466, accuracy : 88.00\n",
      "iteration : 200, loss : 0.3459, accuracy : 88.00\n",
      "iteration : 250, loss : 0.3497, accuracy : 87.84\n",
      "iteration : 300, loss : 0.3504, accuracy : 87.77\n",
      "iteration : 350, loss : 0.3517, accuracy : 87.74\n",
      "Epoch :  52, training loss : 0.3537, training accuracy : 87.60, test loss : 0.3087, test accuracy : 89.94\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.3549, accuracy : 87.31\n",
      "iteration : 100, loss : 0.3433, accuracy : 87.78\n",
      "iteration : 150, loss : 0.3463, accuracy : 87.69\n",
      "iteration : 200, loss : 0.3499, accuracy : 87.52\n",
      "iteration : 250, loss : 0.3480, accuracy : 87.58\n",
      "iteration : 300, loss : 0.3474, accuracy : 87.66\n",
      "iteration : 350, loss : 0.3464, accuracy : 87.69\n",
      "Epoch :  53, training loss : 0.3452, training accuracy : 87.75, test loss : 0.3198, test accuracy : 89.38\n",
      "\n",
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.3635, accuracy : 87.25\n",
      "iteration : 100, loss : 0.3470, accuracy : 87.88\n",
      "iteration : 150, loss : 0.3471, accuracy : 87.89\n",
      "iteration : 200, loss : 0.3416, accuracy : 88.10\n",
      "iteration : 250, loss : 0.3452, accuracy : 87.99\n",
      "iteration : 300, loss : 0.3442, accuracy : 87.99\n",
      "iteration : 350, loss : 0.3439, accuracy : 87.93\n",
      "Epoch :  54, training loss : 0.3448, training accuracy : 87.93, test loss : 0.3824, test accuracy : 87.84\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.3329, accuracy : 88.72\n",
      "iteration : 100, loss : 0.3322, accuracy : 88.41\n",
      "iteration : 150, loss : 0.3361, accuracy : 88.26\n",
      "iteration : 200, loss : 0.3342, accuracy : 88.24\n",
      "iteration : 250, loss : 0.3338, accuracy : 88.24\n",
      "iteration : 300, loss : 0.3366, accuracy : 88.09\n",
      "iteration : 350, loss : 0.3387, accuracy : 88.05\n",
      "Epoch :  55, training loss : 0.3402, training accuracy : 88.06, test loss : 0.2997, test accuracy : 90.29\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.3186, accuracy : 88.95\n",
      "iteration : 100, loss : 0.3386, accuracy : 88.17\n",
      "iteration : 150, loss : 0.3372, accuracy : 88.15\n",
      "iteration : 200, loss : 0.3389, accuracy : 88.13\n",
      "iteration : 250, loss : 0.3399, accuracy : 88.05\n",
      "iteration : 300, loss : 0.3411, accuracy : 88.08\n",
      "iteration : 350, loss : 0.3421, accuracy : 88.00\n",
      "Epoch :  56, training loss : 0.3424, training accuracy : 87.98, test loss : 0.3349, test accuracy : 89.15\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.3504, accuracy : 87.64\n",
      "iteration : 100, loss : 0.3387, accuracy : 88.01\n",
      "iteration : 150, loss : 0.3400, accuracy : 88.00\n",
      "iteration : 200, loss : 0.3414, accuracy : 87.95\n",
      "iteration : 250, loss : 0.3398, accuracy : 88.04\n",
      "iteration : 300, loss : 0.3403, accuracy : 88.05\n",
      "iteration : 350, loss : 0.3380, accuracy : 88.16\n",
      "Epoch :  57, training loss : 0.3390, training accuracy : 88.14, test loss : 0.3559, test accuracy : 88.40\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.3124, accuracy : 89.33\n",
      "iteration : 100, loss : 0.3236, accuracy : 88.88\n",
      "iteration : 150, loss : 0.3267, accuracy : 88.69\n",
      "iteration : 200, loss : 0.3265, accuracy : 88.64\n",
      "iteration : 250, loss : 0.3274, accuracy : 88.57\n",
      "iteration : 300, loss : 0.3282, accuracy : 88.57\n",
      "iteration : 350, loss : 0.3289, accuracy : 88.51\n",
      "Epoch :  58, training loss : 0.3292, training accuracy : 88.45, test loss : 0.3190, test accuracy : 89.41\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.2928, accuracy : 89.61\n",
      "iteration : 100, loss : 0.3068, accuracy : 89.16\n",
      "iteration : 150, loss : 0.3094, accuracy : 88.98\n",
      "iteration : 200, loss : 0.3130, accuracy : 88.87\n",
      "iteration : 250, loss : 0.3182, accuracy : 88.79\n",
      "iteration : 300, loss : 0.3217, accuracy : 88.68\n",
      "iteration : 350, loss : 0.3235, accuracy : 88.61\n",
      "Epoch :  59, training loss : 0.3259, training accuracy : 88.51, test loss : 0.3668, test accuracy : 88.24\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.3257, accuracy : 88.61\n",
      "iteration : 100, loss : 0.3135, accuracy : 88.88\n",
      "iteration : 150, loss : 0.3066, accuracy : 89.21\n",
      "iteration : 200, loss : 0.3091, accuracy : 89.07\n",
      "iteration : 250, loss : 0.3103, accuracy : 89.10\n",
      "iteration : 300, loss : 0.3162, accuracy : 88.89\n",
      "iteration : 350, loss : 0.3212, accuracy : 88.71\n",
      "Epoch :  60, training loss : 0.3232, training accuracy : 88.65, test loss : 0.3433, test accuracy : 88.85\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.3135, accuracy : 89.17\n",
      "iteration : 100, loss : 0.3177, accuracy : 88.97\n",
      "iteration : 150, loss : 0.3177, accuracy : 88.91\n",
      "iteration : 200, loss : 0.3204, accuracy : 88.86\n",
      "iteration : 250, loss : 0.3247, accuracy : 88.69\n",
      "iteration : 300, loss : 0.3213, accuracy : 88.78\n",
      "iteration : 350, loss : 0.3238, accuracy : 88.66\n",
      "Epoch :  61, training loss : 0.3265, training accuracy : 88.60, test loss : 0.3032, test accuracy : 90.09\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.3014, accuracy : 89.41\n",
      "iteration : 100, loss : 0.3074, accuracy : 89.16\n",
      "iteration : 150, loss : 0.3092, accuracy : 89.03\n",
      "iteration : 200, loss : 0.3115, accuracy : 89.09\n",
      "iteration : 250, loss : 0.3153, accuracy : 88.97\n",
      "iteration : 300, loss : 0.3138, accuracy : 88.94\n",
      "iteration : 350, loss : 0.3160, accuracy : 88.84\n",
      "Epoch :  62, training loss : 0.3160, training accuracy : 88.87, test loss : 0.3431, test accuracy : 89.08\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.2966, accuracy : 89.48\n",
      "iteration : 100, loss : 0.3035, accuracy : 89.34\n",
      "iteration : 150, loss : 0.3143, accuracy : 89.01\n",
      "iteration : 200, loss : 0.3146, accuracy : 89.05\n",
      "iteration : 250, loss : 0.3137, accuracy : 89.00\n",
      "iteration : 300, loss : 0.3116, accuracy : 89.07\n",
      "iteration : 350, loss : 0.3124, accuracy : 89.02\n",
      "Epoch :  63, training loss : 0.3123, training accuracy : 89.00, test loss : 0.3437, test accuracy : 88.58\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.3142, accuracy : 89.30\n",
      "iteration : 100, loss : 0.3132, accuracy : 89.00\n",
      "iteration : 150, loss : 0.3123, accuracy : 89.16\n",
      "iteration : 200, loss : 0.3106, accuracy : 89.12\n",
      "iteration : 250, loss : 0.3117, accuracy : 89.17\n",
      "iteration : 300, loss : 0.3139, accuracy : 89.11\n",
      "iteration : 350, loss : 0.3147, accuracy : 89.01\n",
      "Epoch :  64, training loss : 0.3141, training accuracy : 89.02, test loss : 0.3228, test accuracy : 89.54\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.3124, accuracy : 89.42\n",
      "iteration : 100, loss : 0.3121, accuracy : 89.24\n",
      "iteration : 150, loss : 0.3192, accuracy : 88.91\n",
      "iteration : 200, loss : 0.3172, accuracy : 89.02\n",
      "iteration : 250, loss : 0.3133, accuracy : 89.09\n",
      "iteration : 300, loss : 0.3144, accuracy : 89.00\n",
      "iteration : 350, loss : 0.3125, accuracy : 89.12\n",
      "Epoch :  65, training loss : 0.3105, training accuracy : 89.15, test loss : 0.2710, test accuracy : 91.05\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.3032, accuracy : 89.69\n",
      "iteration : 100, loss : 0.3032, accuracy : 89.62\n",
      "iteration : 150, loss : 0.3011, accuracy : 89.59\n",
      "iteration : 200, loss : 0.3010, accuracy : 89.54\n",
      "iteration : 250, loss : 0.3044, accuracy : 89.38\n",
      "iteration : 300, loss : 0.3055, accuracy : 89.37\n",
      "iteration : 350, loss : 0.3039, accuracy : 89.42\n",
      "Epoch :  66, training loss : 0.3033, training accuracy : 89.48, test loss : 0.3645, test accuracy : 88.03\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.3007, accuracy : 89.25\n",
      "iteration : 100, loss : 0.2970, accuracy : 89.56\n",
      "iteration : 150, loss : 0.2961, accuracy : 89.56\n",
      "iteration : 200, loss : 0.2982, accuracy : 89.41\n",
      "iteration : 250, loss : 0.3036, accuracy : 89.26\n",
      "iteration : 300, loss : 0.3038, accuracy : 89.30\n",
      "iteration : 350, loss : 0.3020, accuracy : 89.42\n",
      "Epoch :  67, training loss : 0.3017, training accuracy : 89.44, test loss : 0.3095, test accuracy : 89.75\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.2895, accuracy : 89.62\n",
      "iteration : 100, loss : 0.2926, accuracy : 89.45\n",
      "iteration : 150, loss : 0.2934, accuracy : 89.54\n",
      "iteration : 200, loss : 0.2920, accuracy : 89.60\n",
      "iteration : 250, loss : 0.2945, accuracy : 89.48\n",
      "iteration : 300, loss : 0.2954, accuracy : 89.49\n",
      "iteration : 350, loss : 0.2949, accuracy : 89.54\n",
      "Epoch :  68, training loss : 0.2978, training accuracy : 89.43, test loss : 0.3711, test accuracy : 88.32\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.2967, accuracy : 90.00\n",
      "iteration : 100, loss : 0.2880, accuracy : 90.25\n",
      "iteration : 150, loss : 0.2914, accuracy : 90.00\n",
      "iteration : 200, loss : 0.2983, accuracy : 89.71\n",
      "iteration : 250, loss : 0.2978, accuracy : 89.78\n",
      "iteration : 300, loss : 0.2961, accuracy : 89.84\n",
      "iteration : 350, loss : 0.2972, accuracy : 89.71\n",
      "Epoch :  69, training loss : 0.2969, training accuracy : 89.71, test loss : 0.2918, test accuracy : 90.64\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.2899, accuracy : 89.98\n",
      "iteration : 100, loss : 0.2872, accuracy : 90.00\n",
      "iteration : 150, loss : 0.2929, accuracy : 89.83\n",
      "iteration : 200, loss : 0.2909, accuracy : 89.86\n",
      "iteration : 250, loss : 0.2957, accuracy : 89.67\n",
      "iteration : 300, loss : 0.2927, accuracy : 89.76\n",
      "iteration : 350, loss : 0.2940, accuracy : 89.68\n",
      "Epoch :  70, training loss : 0.2937, training accuracy : 89.69, test loss : 0.3320, test accuracy : 89.38\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.2906, accuracy : 89.62\n",
      "iteration : 100, loss : 0.2892, accuracy : 89.56\n",
      "iteration : 150, loss : 0.2935, accuracy : 89.48\n",
      "iteration : 200, loss : 0.2961, accuracy : 89.41\n",
      "iteration : 250, loss : 0.2960, accuracy : 89.49\n",
      "iteration : 300, loss : 0.2985, accuracy : 89.50\n",
      "iteration : 350, loss : 0.2966, accuracy : 89.58\n",
      "Epoch :  71, training loss : 0.2976, training accuracy : 89.54, test loss : 0.3006, test accuracy : 90.29\n",
      "\n",
      "Epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.2919, accuracy : 89.83\n",
      "iteration : 100, loss : 0.2827, accuracy : 90.13\n",
      "iteration : 150, loss : 0.2862, accuracy : 89.91\n",
      "iteration : 200, loss : 0.2908, accuracy : 89.83\n",
      "iteration : 250, loss : 0.2878, accuracy : 89.88\n",
      "iteration : 300, loss : 0.2894, accuracy : 89.85\n",
      "iteration : 350, loss : 0.2916, accuracy : 89.80\n",
      "Epoch :  72, training loss : 0.2920, training accuracy : 89.76, test loss : 0.3740, test accuracy : 88.38\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.2852, accuracy : 90.06\n",
      "iteration : 100, loss : 0.2811, accuracy : 90.14\n",
      "iteration : 150, loss : 0.2832, accuracy : 90.11\n",
      "iteration : 200, loss : 0.2874, accuracy : 89.85\n",
      "iteration : 250, loss : 0.2891, accuracy : 89.74\n",
      "iteration : 300, loss : 0.2867, accuracy : 89.85\n",
      "iteration : 350, loss : 0.2897, accuracy : 89.79\n",
      "Epoch :  73, training loss : 0.2899, training accuracy : 89.82, test loss : 0.3308, test accuracy : 89.70\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.2737, accuracy : 90.36\n",
      "iteration : 100, loss : 0.2732, accuracy : 90.59\n",
      "iteration : 150, loss : 0.2769, accuracy : 90.40\n",
      "iteration : 200, loss : 0.2790, accuracy : 90.29\n",
      "iteration : 250, loss : 0.2847, accuracy : 90.04\n",
      "iteration : 300, loss : 0.2876, accuracy : 89.92\n",
      "iteration : 350, loss : 0.2876, accuracy : 89.88\n",
      "Epoch :  74, training loss : 0.2896, training accuracy : 89.83, test loss : 0.3100, test accuracy : 90.31\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.2798, accuracy : 89.95\n",
      "iteration : 100, loss : 0.2679, accuracy : 90.48\n",
      "iteration : 150, loss : 0.2681, accuracy : 90.55\n",
      "iteration : 200, loss : 0.2766, accuracy : 90.23\n",
      "iteration : 250, loss : 0.2827, accuracy : 90.06\n",
      "iteration : 300, loss : 0.2813, accuracy : 90.21\n",
      "iteration : 350, loss : 0.2816, accuracy : 90.17\n",
      "Epoch :  75, training loss : 0.2839, training accuracy : 90.08, test loss : 0.3076, test accuracy : 90.02\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.2722, accuracy : 90.22\n",
      "iteration : 100, loss : 0.2786, accuracy : 90.03\n",
      "iteration : 150, loss : 0.2795, accuracy : 89.99\n",
      "iteration : 200, loss : 0.2817, accuracy : 89.86\n",
      "iteration : 250, loss : 0.2817, accuracy : 89.92\n",
      "iteration : 300, loss : 0.2818, accuracy : 89.92\n",
      "iteration : 350, loss : 0.2806, accuracy : 90.01\n",
      "Epoch :  76, training loss : 0.2780, training accuracy : 90.15, test loss : 0.2716, test accuracy : 90.96\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.2440, accuracy : 91.53\n",
      "iteration : 100, loss : 0.2562, accuracy : 90.94\n",
      "iteration : 150, loss : 0.2615, accuracy : 90.58\n",
      "iteration : 200, loss : 0.2606, accuracy : 90.66\n",
      "iteration : 250, loss : 0.2644, accuracy : 90.62\n",
      "iteration : 300, loss : 0.2696, accuracy : 90.41\n",
      "iteration : 350, loss : 0.2719, accuracy : 90.31\n",
      "Epoch :  77, training loss : 0.2738, training accuracy : 90.26, test loss : 0.3200, test accuracy : 89.80\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.2732, accuracy : 90.44\n",
      "iteration : 100, loss : 0.2658, accuracy : 90.62\n",
      "iteration : 150, loss : 0.2724, accuracy : 90.40\n",
      "iteration : 200, loss : 0.2728, accuracy : 90.50\n",
      "iteration : 250, loss : 0.2716, accuracy : 90.53\n",
      "iteration : 300, loss : 0.2753, accuracy : 90.38\n",
      "iteration : 350, loss : 0.2755, accuracy : 90.37\n",
      "Epoch :  78, training loss : 0.2748, training accuracy : 90.37, test loss : 0.3479, test accuracy : 88.91\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.2644, accuracy : 90.78\n",
      "iteration : 100, loss : 0.2614, accuracy : 91.03\n",
      "iteration : 150, loss : 0.2610, accuracy : 91.08\n",
      "iteration : 200, loss : 0.2631, accuracy : 91.01\n",
      "iteration : 250, loss : 0.2653, accuracy : 90.94\n",
      "iteration : 300, loss : 0.2678, accuracy : 90.85\n",
      "iteration : 350, loss : 0.2701, accuracy : 90.70\n",
      "Epoch :  79, training loss : 0.2693, training accuracy : 90.69, test loss : 0.3178, test accuracy : 89.55\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.2462, accuracy : 91.45\n",
      "iteration : 100, loss : 0.2520, accuracy : 91.39\n",
      "iteration : 150, loss : 0.2565, accuracy : 91.14\n",
      "iteration : 200, loss : 0.2563, accuracy : 91.16\n",
      "iteration : 250, loss : 0.2585, accuracy : 91.09\n",
      "iteration : 300, loss : 0.2610, accuracy : 90.98\n",
      "iteration : 350, loss : 0.2610, accuracy : 90.96\n",
      "Epoch :  80, training loss : 0.2621, training accuracy : 90.88, test loss : 0.3340, test accuracy : 89.55\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.2732, accuracy : 90.50\n",
      "iteration : 100, loss : 0.2696, accuracy : 90.76\n",
      "iteration : 150, loss : 0.2694, accuracy : 90.69\n",
      "iteration : 200, loss : 0.2690, accuracy : 90.75\n",
      "iteration : 250, loss : 0.2705, accuracy : 90.67\n",
      "iteration : 300, loss : 0.2691, accuracy : 90.66\n",
      "iteration : 350, loss : 0.2699, accuracy : 90.60\n",
      "Epoch :  81, training loss : 0.2699, training accuracy : 90.59, test loss : 0.3244, test accuracy : 89.31\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.2579, accuracy : 91.06\n",
      "iteration : 100, loss : 0.2613, accuracy : 90.72\n",
      "iteration : 150, loss : 0.2630, accuracy : 90.65\n",
      "iteration : 200, loss : 0.2662, accuracy : 90.67\n",
      "iteration : 250, loss : 0.2673, accuracy : 90.67\n",
      "iteration : 300, loss : 0.2624, accuracy : 90.89\n",
      "iteration : 350, loss : 0.2617, accuracy : 90.86\n",
      "Epoch :  82, training loss : 0.2623, training accuracy : 90.87, test loss : 0.3303, test accuracy : 89.95\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.2611, accuracy : 90.89\n",
      "iteration : 100, loss : 0.2541, accuracy : 91.02\n",
      "iteration : 150, loss : 0.2567, accuracy : 90.99\n",
      "iteration : 200, loss : 0.2538, accuracy : 91.22\n",
      "iteration : 250, loss : 0.2538, accuracy : 91.24\n",
      "iteration : 300, loss : 0.2531, accuracy : 91.16\n",
      "iteration : 350, loss : 0.2573, accuracy : 91.07\n",
      "Epoch :  83, training loss : 0.2597, training accuracy : 90.95, test loss : 0.2891, test accuracy : 90.61\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.2532, accuracy : 91.05\n",
      "iteration : 100, loss : 0.2556, accuracy : 90.94\n",
      "iteration : 150, loss : 0.2541, accuracy : 91.06\n",
      "iteration : 200, loss : 0.2578, accuracy : 90.93\n",
      "iteration : 250, loss : 0.2589, accuracy : 90.88\n",
      "iteration : 300, loss : 0.2573, accuracy : 90.98\n",
      "iteration : 350, loss : 0.2579, accuracy : 90.96\n",
      "Epoch :  84, training loss : 0.2587, training accuracy : 90.91, test loss : 0.3269, test accuracy : 89.22\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.2482, accuracy : 91.16\n",
      "iteration : 100, loss : 0.2491, accuracy : 91.12\n",
      "iteration : 150, loss : 0.2524, accuracy : 91.20\n",
      "iteration : 200, loss : 0.2542, accuracy : 91.08\n",
      "iteration : 250, loss : 0.2525, accuracy : 91.13\n",
      "iteration : 300, loss : 0.2539, accuracy : 91.05\n",
      "iteration : 350, loss : 0.2548, accuracy : 91.03\n",
      "Epoch :  85, training loss : 0.2566, training accuracy : 90.98, test loss : 0.2790, test accuracy : 91.02\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.2607, accuracy : 90.78\n",
      "iteration : 100, loss : 0.2536, accuracy : 91.15\n",
      "iteration : 150, loss : 0.2531, accuracy : 91.09\n",
      "iteration : 200, loss : 0.2496, accuracy : 91.21\n",
      "iteration : 250, loss : 0.2512, accuracy : 91.15\n",
      "iteration : 300, loss : 0.2520, accuracy : 91.09\n",
      "iteration : 350, loss : 0.2567, accuracy : 90.92\n",
      "Epoch :  86, training loss : 0.2569, training accuracy : 90.92, test loss : 0.2858, test accuracy : 90.70\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.2316, accuracy : 92.00\n",
      "iteration : 100, loss : 0.2365, accuracy : 91.77\n",
      "iteration : 150, loss : 0.2402, accuracy : 91.66\n",
      "iteration : 200, loss : 0.2416, accuracy : 91.52\n",
      "iteration : 250, loss : 0.2439, accuracy : 91.55\n",
      "iteration : 300, loss : 0.2451, accuracy : 91.48\n",
      "iteration : 350, loss : 0.2472, accuracy : 91.38\n",
      "Epoch :  87, training loss : 0.2481, training accuracy : 91.33, test loss : 0.3443, test accuracy : 89.52\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.2416, accuracy : 91.55\n",
      "iteration : 100, loss : 0.2414, accuracy : 91.57\n",
      "iteration : 150, loss : 0.2464, accuracy : 91.44\n",
      "iteration : 200, loss : 0.2456, accuracy : 91.38\n",
      "iteration : 250, loss : 0.2428, accuracy : 91.51\n",
      "iteration : 300, loss : 0.2439, accuracy : 91.48\n",
      "iteration : 350, loss : 0.2460, accuracy : 91.38\n",
      "Epoch :  88, training loss : 0.2458, training accuracy : 91.36, test loss : 0.2802, test accuracy : 91.02\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.2506, accuracy : 91.22\n",
      "iteration : 100, loss : 0.2495, accuracy : 91.29\n",
      "iteration : 150, loss : 0.2421, accuracy : 91.56\n",
      "iteration : 200, loss : 0.2411, accuracy : 91.64\n",
      "iteration : 250, loss : 0.2417, accuracy : 91.61\n",
      "iteration : 300, loss : 0.2428, accuracy : 91.61\n",
      "iteration : 350, loss : 0.2471, accuracy : 91.49\n",
      "Epoch :  89, training loss : 0.2475, training accuracy : 91.45, test loss : 0.2846, test accuracy : 90.76\n",
      "\n",
      "Epoch: 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.2408, accuracy : 91.81\n",
      "iteration : 100, loss : 0.2388, accuracy : 91.90\n",
      "iteration : 150, loss : 0.2381, accuracy : 91.81\n",
      "iteration : 200, loss : 0.2433, accuracy : 91.57\n",
      "iteration : 250, loss : 0.2446, accuracy : 91.50\n",
      "iteration : 300, loss : 0.2454, accuracy : 91.43\n",
      "iteration : 350, loss : 0.2443, accuracy : 91.46\n",
      "Epoch :  90, training loss : 0.2451, training accuracy : 91.40, test loss : 0.3257, test accuracy : 89.95\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.2315, accuracy : 92.06\n",
      "iteration : 100, loss : 0.2393, accuracy : 91.78\n",
      "iteration : 150, loss : 0.2438, accuracy : 91.67\n",
      "iteration : 200, loss : 0.2397, accuracy : 91.72\n",
      "iteration : 250, loss : 0.2427, accuracy : 91.58\n",
      "iteration : 300, loss : 0.2413, accuracy : 91.63\n",
      "iteration : 350, loss : 0.2416, accuracy : 91.59\n",
      "Epoch :  91, training loss : 0.2418, training accuracy : 91.57, test loss : 0.2503, test accuracy : 91.75\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.2250, accuracy : 91.80\n",
      "iteration : 100, loss : 0.2307, accuracy : 91.60\n",
      "iteration : 150, loss : 0.2286, accuracy : 91.66\n",
      "iteration : 200, loss : 0.2352, accuracy : 91.58\n",
      "iteration : 250, loss : 0.2374, accuracy : 91.53\n",
      "iteration : 300, loss : 0.2381, accuracy : 91.52\n",
      "iteration : 350, loss : 0.2369, accuracy : 91.60\n",
      "Epoch :  92, training loss : 0.2346, training accuracy : 91.69, test loss : 0.2626, test accuracy : 91.61\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.2262, accuracy : 91.75\n",
      "iteration : 100, loss : 0.2301, accuracy : 91.71\n",
      "iteration : 150, loss : 0.2313, accuracy : 91.81\n",
      "iteration : 200, loss : 0.2324, accuracy : 91.78\n",
      "iteration : 250, loss : 0.2357, accuracy : 91.63\n",
      "iteration : 300, loss : 0.2364, accuracy : 91.58\n",
      "iteration : 350, loss : 0.2350, accuracy : 91.63\n",
      "Epoch :  93, training loss : 0.2349, training accuracy : 91.65, test loss : 0.2583, test accuracy : 92.05\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.2238, accuracy : 92.44\n",
      "iteration : 100, loss : 0.2244, accuracy : 92.18\n",
      "iteration : 150, loss : 0.2253, accuracy : 92.09\n",
      "iteration : 200, loss : 0.2230, accuracy : 92.20\n",
      "iteration : 250, loss : 0.2251, accuracy : 92.12\n",
      "iteration : 300, loss : 0.2253, accuracy : 92.09\n",
      "iteration : 350, loss : 0.2287, accuracy : 92.01\n",
      "Epoch :  94, training loss : 0.2304, training accuracy : 91.95, test loss : 0.3143, test accuracy : 90.27\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.2202, accuracy : 92.59\n",
      "iteration : 100, loss : 0.2178, accuracy : 92.48\n",
      "iteration : 150, loss : 0.2229, accuracy : 92.20\n",
      "iteration : 200, loss : 0.2250, accuracy : 92.18\n",
      "iteration : 250, loss : 0.2248, accuracy : 92.21\n",
      "iteration : 300, loss : 0.2249, accuracy : 92.19\n",
      "iteration : 350, loss : 0.2272, accuracy : 92.10\n",
      "Epoch :  95, training loss : 0.2275, training accuracy : 92.10, test loss : 0.3308, test accuracy : 90.09\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.2197, accuracy : 92.25\n",
      "iteration : 100, loss : 0.2239, accuracy : 92.14\n",
      "iteration : 150, loss : 0.2249, accuracy : 92.21\n",
      "iteration : 200, loss : 0.2268, accuracy : 92.07\n",
      "iteration : 250, loss : 0.2218, accuracy : 92.28\n",
      "iteration : 300, loss : 0.2228, accuracy : 92.24\n",
      "iteration : 350, loss : 0.2221, accuracy : 92.22\n",
      "Epoch :  96, training loss : 0.2205, training accuracy : 92.31, test loss : 0.2895, test accuracy : 90.90\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.1985, accuracy : 92.75\n",
      "iteration : 100, loss : 0.2081, accuracy : 92.50\n",
      "iteration : 150, loss : 0.2133, accuracy : 92.44\n",
      "iteration : 200, loss : 0.2160, accuracy : 92.33\n",
      "iteration : 250, loss : 0.2180, accuracy : 92.29\n",
      "iteration : 300, loss : 0.2211, accuracy : 92.21\n",
      "iteration : 350, loss : 0.2206, accuracy : 92.23\n",
      "Epoch :  97, training loss : 0.2235, training accuracy : 92.17, test loss : 0.3254, test accuracy : 89.66\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.2054, accuracy : 92.73\n",
      "iteration : 100, loss : 0.2089, accuracy : 92.57\n",
      "iteration : 150, loss : 0.2132, accuracy : 92.39\n",
      "iteration : 200, loss : 0.2166, accuracy : 92.28\n",
      "iteration : 250, loss : 0.2181, accuracy : 92.25\n",
      "iteration : 300, loss : 0.2206, accuracy : 92.19\n",
      "iteration : 350, loss : 0.2209, accuracy : 92.18\n",
      "Epoch :  98, training loss : 0.2200, training accuracy : 92.25, test loss : 0.3013, test accuracy : 90.33\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.2050, accuracy : 92.78\n",
      "iteration : 100, loss : 0.2134, accuracy : 92.38\n",
      "iteration : 150, loss : 0.2125, accuracy : 92.45\n",
      "iteration : 200, loss : 0.2128, accuracy : 92.50\n",
      "iteration : 250, loss : 0.2129, accuracy : 92.52\n",
      "iteration : 300, loss : 0.2146, accuracy : 92.46\n",
      "iteration : 350, loss : 0.2150, accuracy : 92.46\n",
      "Epoch :  99, training loss : 0.2171, training accuracy : 92.36, test loss : 0.3219, test accuracy : 89.96\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.2192, accuracy : 92.45\n",
      "iteration : 100, loss : 0.2225, accuracy : 92.25\n",
      "iteration : 150, loss : 0.2187, accuracy : 92.46\n",
      "iteration : 200, loss : 0.2228, accuracy : 92.39\n",
      "iteration : 250, loss : 0.2214, accuracy : 92.34\n",
      "iteration : 300, loss : 0.2192, accuracy : 92.44\n",
      "iteration : 350, loss : 0.2182, accuracy : 92.47\n",
      "Epoch : 100, training loss : 0.2175, training accuracy : 92.48, test loss : 0.2684, test accuracy : 91.24\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.1965, accuracy : 93.20\n",
      "iteration : 100, loss : 0.2011, accuracy : 93.15\n",
      "iteration : 150, loss : 0.2031, accuracy : 92.98\n",
      "iteration : 200, loss : 0.2073, accuracy : 92.83\n",
      "iteration : 250, loss : 0.2080, accuracy : 92.82\n",
      "iteration : 300, loss : 0.2072, accuracy : 92.80\n",
      "iteration : 350, loss : 0.2091, accuracy : 92.71\n",
      "Epoch : 101, training loss : 0.2110, training accuracy : 92.64, test loss : 0.3468, test accuracy : 88.92\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.2003, accuracy : 92.92\n",
      "iteration : 100, loss : 0.2019, accuracy : 92.80\n",
      "iteration : 150, loss : 0.2041, accuracy : 92.79\n",
      "iteration : 200, loss : 0.2071, accuracy : 92.68\n",
      "iteration : 250, loss : 0.2056, accuracy : 92.78\n",
      "iteration : 300, loss : 0.2070, accuracy : 92.72\n",
      "iteration : 350, loss : 0.2123, accuracy : 92.52\n",
      "Epoch : 102, training loss : 0.2134, training accuracy : 92.50, test loss : 0.2846, test accuracy : 90.96\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.1980, accuracy : 93.09\n",
      "iteration : 100, loss : 0.2014, accuracy : 93.06\n",
      "iteration : 150, loss : 0.2025, accuracy : 92.97\n",
      "iteration : 200, loss : 0.2031, accuracy : 92.96\n",
      "iteration : 250, loss : 0.2061, accuracy : 92.83\n",
      "iteration : 300, loss : 0.2077, accuracy : 92.72\n",
      "iteration : 350, loss : 0.2074, accuracy : 92.74\n",
      "Epoch : 103, training loss : 0.2067, training accuracy : 92.73, test loss : 0.2612, test accuracy : 91.84\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.1860, accuracy : 93.66\n",
      "iteration : 100, loss : 0.1933, accuracy : 93.44\n",
      "iteration : 150, loss : 0.1975, accuracy : 93.26\n",
      "iteration : 200, loss : 0.1992, accuracy : 93.14\n",
      "iteration : 250, loss : 0.2037, accuracy : 92.94\n",
      "iteration : 300, loss : 0.2052, accuracy : 92.86\n",
      "iteration : 350, loss : 0.2052, accuracy : 92.88\n",
      "Epoch : 104, training loss : 0.2053, training accuracy : 92.91, test loss : 0.2718, test accuracy : 91.71\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.1875, accuracy : 93.31\n",
      "iteration : 100, loss : 0.1814, accuracy : 93.57\n",
      "iteration : 150, loss : 0.1889, accuracy : 93.27\n",
      "iteration : 200, loss : 0.1890, accuracy : 93.32\n",
      "iteration : 250, loss : 0.1894, accuracy : 93.35\n",
      "iteration : 300, loss : 0.1914, accuracy : 93.25\n",
      "iteration : 350, loss : 0.1931, accuracy : 93.20\n",
      "Epoch : 105, training loss : 0.1956, training accuracy : 93.10, test loss : 0.2685, test accuracy : 91.90\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.2019, accuracy : 92.91\n",
      "iteration : 100, loss : 0.2030, accuracy : 92.88\n",
      "iteration : 150, loss : 0.1997, accuracy : 93.08\n",
      "iteration : 200, loss : 0.1988, accuracy : 93.11\n",
      "iteration : 250, loss : 0.1956, accuracy : 93.22\n",
      "iteration : 300, loss : 0.1969, accuracy : 93.06\n",
      "iteration : 350, loss : 0.1965, accuracy : 93.06\n",
      "Epoch : 106, training loss : 0.1964, training accuracy : 93.09, test loss : 0.2539, test accuracy : 92.38\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.1888, accuracy : 93.30\n",
      "iteration : 100, loss : 0.1900, accuracy : 93.24\n",
      "iteration : 150, loss : 0.1872, accuracy : 93.34\n",
      "iteration : 200, loss : 0.1897, accuracy : 93.23\n",
      "iteration : 250, loss : 0.1919, accuracy : 93.20\n",
      "iteration : 300, loss : 0.1945, accuracy : 93.16\n",
      "iteration : 350, loss : 0.1943, accuracy : 93.17\n",
      "Epoch : 107, training loss : 0.1936, training accuracy : 93.20, test loss : 0.2716, test accuracy : 91.56\n",
      "\n",
      "Epoch: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.1987, accuracy : 92.94\n",
      "iteration : 100, loss : 0.1924, accuracy : 93.20\n",
      "iteration : 150, loss : 0.1929, accuracy : 93.25\n",
      "iteration : 200, loss : 0.1912, accuracy : 93.32\n",
      "iteration : 250, loss : 0.1901, accuracy : 93.33\n",
      "iteration : 300, loss : 0.1925, accuracy : 93.22\n",
      "iteration : 350, loss : 0.1942, accuracy : 93.19\n",
      "Epoch : 108, training loss : 0.1955, training accuracy : 93.15, test loss : 0.2617, test accuracy : 91.92\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.1823, accuracy : 93.55\n",
      "iteration : 100, loss : 0.1878, accuracy : 93.45\n",
      "iteration : 150, loss : 0.1833, accuracy : 93.58\n",
      "iteration : 200, loss : 0.1849, accuracy : 93.61\n",
      "iteration : 250, loss : 0.1844, accuracy : 93.65\n",
      "iteration : 300, loss : 0.1845, accuracy : 93.67\n",
      "iteration : 350, loss : 0.1853, accuracy : 93.64\n",
      "Epoch : 109, training loss : 0.1863, training accuracy : 93.60, test loss : 0.2847, test accuracy : 91.49\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.1811, accuracy : 93.56\n",
      "iteration : 100, loss : 0.1873, accuracy : 93.41\n",
      "iteration : 150, loss : 0.1914, accuracy : 93.20\n",
      "iteration : 200, loss : 0.1906, accuracy : 93.14\n",
      "iteration : 250, loss : 0.1907, accuracy : 93.13\n",
      "iteration : 300, loss : 0.1901, accuracy : 93.16\n",
      "iteration : 350, loss : 0.1887, accuracy : 93.23\n",
      "Epoch : 110, training loss : 0.1910, training accuracy : 93.18, test loss : 0.2880, test accuracy : 91.35\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.1828, accuracy : 93.52\n",
      "iteration : 100, loss : 0.1841, accuracy : 93.59\n",
      "iteration : 150, loss : 0.1877, accuracy : 93.44\n",
      "iteration : 200, loss : 0.1880, accuracy : 93.45\n",
      "iteration : 250, loss : 0.1867, accuracy : 93.50\n",
      "iteration : 300, loss : 0.1863, accuracy : 93.51\n",
      "iteration : 350, loss : 0.1862, accuracy : 93.51\n",
      "Epoch : 111, training loss : 0.1861, training accuracy : 93.50, test loss : 0.2977, test accuracy : 90.70\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.1735, accuracy : 93.97\n",
      "iteration : 100, loss : 0.1749, accuracy : 93.98\n",
      "iteration : 150, loss : 0.1779, accuracy : 93.88\n",
      "iteration : 200, loss : 0.1773, accuracy : 93.90\n",
      "iteration : 250, loss : 0.1791, accuracy : 93.78\n",
      "iteration : 300, loss : 0.1799, accuracy : 93.70\n",
      "iteration : 350, loss : 0.1804, accuracy : 93.70\n",
      "Epoch : 112, training loss : 0.1815, training accuracy : 93.62, test loss : 0.2841, test accuracy : 91.46\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.1711, accuracy : 93.89\n",
      "iteration : 100, loss : 0.1709, accuracy : 93.97\n",
      "iteration : 150, loss : 0.1744, accuracy : 93.83\n",
      "iteration : 200, loss : 0.1768, accuracy : 93.70\n",
      "iteration : 250, loss : 0.1778, accuracy : 93.64\n",
      "iteration : 300, loss : 0.1773, accuracy : 93.70\n",
      "iteration : 350, loss : 0.1766, accuracy : 93.72\n",
      "Epoch : 113, training loss : 0.1772, training accuracy : 93.69, test loss : 0.2674, test accuracy : 91.71\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.1763, accuracy : 93.70\n",
      "iteration : 100, loss : 0.1756, accuracy : 93.84\n",
      "iteration : 150, loss : 0.1729, accuracy : 93.95\n",
      "iteration : 200, loss : 0.1730, accuracy : 93.94\n",
      "iteration : 250, loss : 0.1758, accuracy : 93.83\n",
      "iteration : 300, loss : 0.1754, accuracy : 93.88\n",
      "iteration : 350, loss : 0.1753, accuracy : 93.82\n",
      "Epoch : 114, training loss : 0.1753, training accuracy : 93.83, test loss : 0.2504, test accuracy : 92.37\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.1758, accuracy : 94.30\n",
      "iteration : 100, loss : 0.1691, accuracy : 94.33\n",
      "iteration : 150, loss : 0.1702, accuracy : 94.15\n",
      "iteration : 200, loss : 0.1742, accuracy : 93.93\n",
      "iteration : 250, loss : 0.1728, accuracy : 94.02\n",
      "iteration : 300, loss : 0.1719, accuracy : 94.04\n",
      "iteration : 350, loss : 0.1729, accuracy : 94.01\n",
      "Epoch : 115, training loss : 0.1747, training accuracy : 93.95, test loss : 0.2683, test accuracy : 91.55\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.1649, accuracy : 94.06\n",
      "iteration : 100, loss : 0.1620, accuracy : 94.19\n",
      "iteration : 150, loss : 0.1657, accuracy : 94.12\n",
      "iteration : 200, loss : 0.1638, accuracy : 94.29\n",
      "iteration : 250, loss : 0.1654, accuracy : 94.19\n",
      "iteration : 300, loss : 0.1672, accuracy : 94.12\n",
      "iteration : 350, loss : 0.1693, accuracy : 94.02\n",
      "Epoch : 116, training loss : 0.1704, training accuracy : 93.98, test loss : 0.2741, test accuracy : 91.83\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.1664, accuracy : 94.12\n",
      "iteration : 100, loss : 0.1681, accuracy : 94.20\n",
      "iteration : 150, loss : 0.1664, accuracy : 94.20\n",
      "iteration : 200, loss : 0.1658, accuracy : 94.26\n",
      "iteration : 250, loss : 0.1673, accuracy : 94.21\n",
      "iteration : 300, loss : 0.1686, accuracy : 94.11\n",
      "iteration : 350, loss : 0.1699, accuracy : 94.06\n",
      "Epoch : 117, training loss : 0.1709, training accuracy : 94.01, test loss : 0.2644, test accuracy : 91.95\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.1617, accuracy : 94.59\n",
      "iteration : 100, loss : 0.1596, accuracy : 94.62\n",
      "iteration : 150, loss : 0.1596, accuracy : 94.52\n",
      "iteration : 200, loss : 0.1593, accuracy : 94.55\n",
      "iteration : 250, loss : 0.1595, accuracy : 94.59\n",
      "iteration : 300, loss : 0.1611, accuracy : 94.47\n",
      "iteration : 350, loss : 0.1620, accuracy : 94.43\n",
      "Epoch : 118, training loss : 0.1623, training accuracy : 94.45, test loss : 0.2705, test accuracy : 91.86\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.1578, accuracy : 94.53\n",
      "iteration : 100, loss : 0.1590, accuracy : 94.50\n",
      "iteration : 150, loss : 0.1558, accuracy : 94.59\n",
      "iteration : 200, loss : 0.1517, accuracy : 94.69\n",
      "iteration : 250, loss : 0.1542, accuracy : 94.54\n",
      "iteration : 300, loss : 0.1569, accuracy : 94.45\n",
      "iteration : 350, loss : 0.1588, accuracy : 94.39\n",
      "Epoch : 119, training loss : 0.1595, training accuracy : 94.36, test loss : 0.2364, test accuracy : 92.52\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.1504, accuracy : 94.64\n",
      "iteration : 100, loss : 0.1546, accuracy : 94.68\n",
      "iteration : 150, loss : 0.1549, accuracy : 94.64\n",
      "iteration : 200, loss : 0.1550, accuracy : 94.62\n",
      "iteration : 250, loss : 0.1571, accuracy : 94.49\n",
      "iteration : 300, loss : 0.1588, accuracy : 94.41\n",
      "iteration : 350, loss : 0.1593, accuracy : 94.42\n",
      "Epoch : 120, training loss : 0.1617, training accuracy : 94.34, test loss : 0.2540, test accuracy : 92.16\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.1596, accuracy : 94.27\n",
      "iteration : 100, loss : 0.1595, accuracy : 94.35\n",
      "iteration : 150, loss : 0.1603, accuracy : 94.27\n",
      "iteration : 200, loss : 0.1581, accuracy : 94.41\n",
      "iteration : 250, loss : 0.1584, accuracy : 94.36\n",
      "iteration : 300, loss : 0.1584, accuracy : 94.36\n",
      "iteration : 350, loss : 0.1576, accuracy : 94.41\n",
      "Epoch : 121, training loss : 0.1579, training accuracy : 94.38, test loss : 0.2786, test accuracy : 90.96\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.1475, accuracy : 95.02\n",
      "iteration : 100, loss : 0.1476, accuracy : 94.92\n",
      "iteration : 150, loss : 0.1528, accuracy : 94.62\n",
      "iteration : 200, loss : 0.1515, accuracy : 94.64\n",
      "iteration : 250, loss : 0.1529, accuracy : 94.67\n",
      "iteration : 300, loss : 0.1555, accuracy : 94.57\n",
      "iteration : 350, loss : 0.1568, accuracy : 94.48\n",
      "Epoch : 122, training loss : 0.1591, training accuracy : 94.43, test loss : 0.2707, test accuracy : 91.66\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.1556, accuracy : 94.52\n",
      "iteration : 100, loss : 0.1540, accuracy : 94.54\n",
      "iteration : 150, loss : 0.1512, accuracy : 94.73\n",
      "iteration : 200, loss : 0.1492, accuracy : 94.73\n",
      "iteration : 250, loss : 0.1489, accuracy : 94.74\n",
      "iteration : 300, loss : 0.1490, accuracy : 94.76\n",
      "iteration : 350, loss : 0.1503, accuracy : 94.69\n",
      "Epoch : 123, training loss : 0.1501, training accuracy : 94.70, test loss : 0.2743, test accuracy : 91.79\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.1497, accuracy : 94.81\n",
      "iteration : 100, loss : 0.1505, accuracy : 94.67\n",
      "iteration : 150, loss : 0.1516, accuracy : 94.55\n",
      "iteration : 200, loss : 0.1515, accuracy : 94.57\n",
      "iteration : 250, loss : 0.1507, accuracy : 94.68\n",
      "iteration : 300, loss : 0.1488, accuracy : 94.76\n",
      "iteration : 350, loss : 0.1492, accuracy : 94.78\n",
      "Epoch : 124, training loss : 0.1498, training accuracy : 94.71, test loss : 0.2606, test accuracy : 91.87\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.1383, accuracy : 95.11\n",
      "iteration : 100, loss : 0.1372, accuracy : 95.16\n",
      "iteration : 150, loss : 0.1369, accuracy : 95.15\n",
      "iteration : 200, loss : 0.1366, accuracy : 95.17\n",
      "iteration : 250, loss : 0.1380, accuracy : 95.11\n",
      "iteration : 300, loss : 0.1392, accuracy : 95.09\n",
      "iteration : 350, loss : 0.1409, accuracy : 95.00\n",
      "Epoch : 125, training loss : 0.1424, training accuracy : 94.98, test loss : 0.2947, test accuracy : 90.94\n",
      "\n",
      "Epoch: 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.1455, accuracy : 94.77\n",
      "iteration : 100, loss : 0.1460, accuracy : 94.66\n",
      "iteration : 150, loss : 0.1440, accuracy : 94.76\n",
      "iteration : 200, loss : 0.1433, accuracy : 94.85\n",
      "iteration : 250, loss : 0.1426, accuracy : 94.94\n",
      "iteration : 300, loss : 0.1420, accuracy : 94.99\n",
      "iteration : 350, loss : 0.1433, accuracy : 94.92\n",
      "Epoch : 126, training loss : 0.1441, training accuracy : 94.92, test loss : 0.2622, test accuracy : 92.41\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.1335, accuracy : 95.31\n",
      "iteration : 100, loss : 0.1302, accuracy : 95.53\n",
      "iteration : 150, loss : 0.1331, accuracy : 95.38\n",
      "iteration : 200, loss : 0.1351, accuracy : 95.32\n",
      "iteration : 250, loss : 0.1365, accuracy : 95.25\n",
      "iteration : 300, loss : 0.1372, accuracy : 95.20\n",
      "iteration : 350, loss : 0.1393, accuracy : 95.10\n",
      "Epoch : 127, training loss : 0.1402, training accuracy : 95.10, test loss : 0.2583, test accuracy : 92.54\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.1277, accuracy : 95.52\n",
      "iteration : 100, loss : 0.1280, accuracy : 95.48\n",
      "iteration : 150, loss : 0.1336, accuracy : 95.29\n",
      "iteration : 200, loss : 0.1345, accuracy : 95.32\n",
      "iteration : 250, loss : 0.1335, accuracy : 95.33\n",
      "iteration : 300, loss : 0.1343, accuracy : 95.32\n",
      "iteration : 350, loss : 0.1343, accuracy : 95.31\n",
      "Epoch : 128, training loss : 0.1364, training accuracy : 95.22, test loss : 0.2730, test accuracy : 91.83\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.1268, accuracy : 95.42\n",
      "iteration : 100, loss : 0.1335, accuracy : 95.23\n",
      "iteration : 150, loss : 0.1343, accuracy : 95.21\n",
      "iteration : 200, loss : 0.1329, accuracy : 95.32\n",
      "iteration : 250, loss : 0.1353, accuracy : 95.26\n",
      "iteration : 300, loss : 0.1355, accuracy : 95.26\n",
      "iteration : 350, loss : 0.1355, accuracy : 95.25\n",
      "Epoch : 129, training loss : 0.1355, training accuracy : 95.28, test loss : 0.2606, test accuracy : 92.42\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.1286, accuracy : 95.44\n",
      "iteration : 100, loss : 0.1293, accuracy : 95.50\n",
      "iteration : 150, loss : 0.1326, accuracy : 95.35\n",
      "iteration : 200, loss : 0.1335, accuracy : 95.34\n",
      "iteration : 250, loss : 0.1334, accuracy : 95.35\n",
      "iteration : 300, loss : 0.1330, accuracy : 95.37\n",
      "iteration : 350, loss : 0.1339, accuracy : 95.31\n",
      "Epoch : 130, training loss : 0.1337, training accuracy : 95.31, test loss : 0.2555, test accuracy : 92.51\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.1243, accuracy : 95.91\n",
      "iteration : 100, loss : 0.1218, accuracy : 95.83\n",
      "iteration : 150, loss : 0.1220, accuracy : 95.78\n",
      "iteration : 200, loss : 0.1242, accuracy : 95.73\n",
      "iteration : 250, loss : 0.1232, accuracy : 95.79\n",
      "iteration : 300, loss : 0.1249, accuracy : 95.70\n",
      "iteration : 350, loss : 0.1273, accuracy : 95.61\n",
      "Epoch : 131, training loss : 0.1275, training accuracy : 95.60, test loss : 0.2618, test accuracy : 92.18\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.1252, accuracy : 95.89\n",
      "iteration : 100, loss : 0.1262, accuracy : 95.88\n",
      "iteration : 150, loss : 0.1249, accuracy : 95.86\n",
      "iteration : 200, loss : 0.1235, accuracy : 95.80\n",
      "iteration : 250, loss : 0.1228, accuracy : 95.76\n",
      "iteration : 300, loss : 0.1226, accuracy : 95.78\n",
      "iteration : 350, loss : 0.1247, accuracy : 95.68\n",
      "Epoch : 132, training loss : 0.1269, training accuracy : 95.64, test loss : 0.2558, test accuracy : 92.49\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.1262, accuracy : 95.41\n",
      "iteration : 100, loss : 0.1235, accuracy : 95.55\n",
      "iteration : 150, loss : 0.1294, accuracy : 95.35\n",
      "iteration : 200, loss : 0.1273, accuracy : 95.51\n",
      "iteration : 250, loss : 0.1260, accuracy : 95.56\n",
      "iteration : 300, loss : 0.1250, accuracy : 95.59\n",
      "iteration : 350, loss : 0.1263, accuracy : 95.60\n",
      "Epoch : 133, training loss : 0.1260, training accuracy : 95.62, test loss : 0.2950, test accuracy : 91.64\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.1218, accuracy : 95.50\n",
      "iteration : 100, loss : 0.1201, accuracy : 95.84\n",
      "iteration : 150, loss : 0.1190, accuracy : 95.85\n",
      "iteration : 200, loss : 0.1190, accuracy : 95.78\n",
      "iteration : 250, loss : 0.1213, accuracy : 95.72\n",
      "iteration : 300, loss : 0.1217, accuracy : 95.78\n",
      "iteration : 350, loss : 0.1218, accuracy : 95.76\n",
      "Epoch : 134, training loss : 0.1218, training accuracy : 95.76, test loss : 0.2453, test accuracy : 92.74\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.1147, accuracy : 96.06\n",
      "iteration : 100, loss : 0.1139, accuracy : 96.09\n",
      "iteration : 150, loss : 0.1137, accuracy : 96.22\n",
      "iteration : 200, loss : 0.1130, accuracy : 96.14\n",
      "iteration : 250, loss : 0.1155, accuracy : 96.04\n",
      "iteration : 300, loss : 0.1176, accuracy : 95.97\n",
      "iteration : 350, loss : 0.1189, accuracy : 95.92\n",
      "Epoch : 135, training loss : 0.1191, training accuracy : 95.90, test loss : 0.2337, test accuracy : 92.96\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.1159, accuracy : 95.80\n",
      "iteration : 100, loss : 0.1159, accuracy : 95.96\n",
      "iteration : 150, loss : 0.1139, accuracy : 96.01\n",
      "iteration : 200, loss : 0.1112, accuracy : 96.15\n",
      "iteration : 250, loss : 0.1125, accuracy : 96.14\n",
      "iteration : 300, loss : 0.1139, accuracy : 96.10\n",
      "iteration : 350, loss : 0.1136, accuracy : 96.11\n",
      "Epoch : 136, training loss : 0.1147, training accuracy : 96.07, test loss : 0.2404, test accuracy : 92.64\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.1120, accuracy : 95.98\n",
      "iteration : 100, loss : 0.1081, accuracy : 96.28\n",
      "iteration : 150, loss : 0.1077, accuracy : 96.29\n",
      "iteration : 200, loss : 0.1100, accuracy : 96.23\n",
      "iteration : 250, loss : 0.1108, accuracy : 96.17\n",
      "iteration : 300, loss : 0.1136, accuracy : 96.08\n",
      "iteration : 350, loss : 0.1124, accuracy : 96.16\n",
      "Epoch : 137, training loss : 0.1124, training accuracy : 96.20, test loss : 0.2693, test accuracy : 92.45\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.1085, accuracy : 96.42\n",
      "iteration : 100, loss : 0.1052, accuracy : 96.47\n",
      "iteration : 150, loss : 0.1082, accuracy : 96.30\n",
      "iteration : 200, loss : 0.1085, accuracy : 96.30\n",
      "iteration : 250, loss : 0.1110, accuracy : 96.21\n",
      "iteration : 300, loss : 0.1129, accuracy : 96.13\n",
      "iteration : 350, loss : 0.1125, accuracy : 96.16\n",
      "Epoch : 138, training loss : 0.1136, training accuracy : 96.12, test loss : 0.2735, test accuracy : 92.16\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.1055, accuracy : 96.48\n",
      "iteration : 100, loss : 0.1097, accuracy : 96.30\n",
      "iteration : 150, loss : 0.1093, accuracy : 96.31\n",
      "iteration : 200, loss : 0.1082, accuracy : 96.35\n",
      "iteration : 250, loss : 0.1085, accuracy : 96.33\n",
      "iteration : 300, loss : 0.1087, accuracy : 96.33\n",
      "iteration : 350, loss : 0.1089, accuracy : 96.28\n",
      "Epoch : 139, training loss : 0.1076, training accuracy : 96.33, test loss : 0.2441, test accuracy : 92.70\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.0926, accuracy : 96.70\n",
      "iteration : 100, loss : 0.0988, accuracy : 96.55\n",
      "iteration : 150, loss : 0.1037, accuracy : 96.47\n",
      "iteration : 200, loss : 0.1029, accuracy : 96.47\n",
      "iteration : 250, loss : 0.1032, accuracy : 96.49\n",
      "iteration : 300, loss : 0.1038, accuracy : 96.44\n",
      "iteration : 350, loss : 0.1036, accuracy : 96.42\n",
      "Epoch : 140, training loss : 0.1046, training accuracy : 96.40, test loss : 0.2393, test accuracy : 93.11\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.1040, accuracy : 96.39\n",
      "iteration : 100, loss : 0.1060, accuracy : 96.34\n",
      "iteration : 150, loss : 0.1042, accuracy : 96.39\n",
      "iteration : 200, loss : 0.1072, accuracy : 96.29\n",
      "iteration : 250, loss : 0.1086, accuracy : 96.25\n",
      "iteration : 300, loss : 0.1081, accuracy : 96.29\n",
      "iteration : 350, loss : 0.1068, accuracy : 96.33\n",
      "Epoch : 141, training loss : 0.1068, training accuracy : 96.35, test loss : 0.2696, test accuracy : 92.32\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.0955, accuracy : 96.69\n",
      "iteration : 100, loss : 0.0967, accuracy : 96.64\n",
      "iteration : 150, loss : 0.0986, accuracy : 96.62\n",
      "iteration : 200, loss : 0.0981, accuracy : 96.61\n",
      "iteration : 250, loss : 0.0981, accuracy : 96.65\n",
      "iteration : 300, loss : 0.0983, accuracy : 96.61\n",
      "iteration : 350, loss : 0.0984, accuracy : 96.60\n",
      "Epoch : 142, training loss : 0.0992, training accuracy : 96.57, test loss : 0.2403, test accuracy : 93.24\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.1001, accuracy : 96.31\n",
      "iteration : 100, loss : 0.0979, accuracy : 96.37\n",
      "iteration : 150, loss : 0.1008, accuracy : 96.40\n",
      "iteration : 200, loss : 0.1012, accuracy : 96.43\n",
      "iteration : 250, loss : 0.0995, accuracy : 96.54\n",
      "iteration : 300, loss : 0.0990, accuracy : 96.54\n",
      "iteration : 350, loss : 0.0996, accuracy : 96.51\n",
      "Epoch : 143, training loss : 0.0989, training accuracy : 96.54, test loss : 0.2439, test accuracy : 92.94\n",
      "\n",
      "Epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0898, accuracy : 96.89\n",
      "iteration : 100, loss : 0.0888, accuracy : 96.96\n",
      "iteration : 150, loss : 0.0883, accuracy : 96.99\n",
      "iteration : 200, loss : 0.0921, accuracy : 96.86\n",
      "iteration : 250, loss : 0.0922, accuracy : 96.88\n",
      "iteration : 300, loss : 0.0924, accuracy : 96.88\n",
      "iteration : 350, loss : 0.0938, accuracy : 96.82\n",
      "Epoch : 144, training loss : 0.0938, training accuracy : 96.82, test loss : 0.2502, test accuracy : 92.89\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.1030, accuracy : 96.36\n",
      "iteration : 100, loss : 0.0937, accuracy : 96.59\n",
      "iteration : 150, loss : 0.0936, accuracy : 96.67\n",
      "iteration : 200, loss : 0.0915, accuracy : 96.82\n",
      "iteration : 250, loss : 0.0911, accuracy : 96.86\n",
      "iteration : 300, loss : 0.0915, accuracy : 96.84\n",
      "iteration : 350, loss : 0.0914, accuracy : 96.88\n",
      "Epoch : 145, training loss : 0.0914, training accuracy : 96.89, test loss : 0.2438, test accuracy : 93.08\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.0877, accuracy : 97.16\n",
      "iteration : 100, loss : 0.0861, accuracy : 97.12\n",
      "iteration : 150, loss : 0.0889, accuracy : 96.99\n",
      "iteration : 200, loss : 0.0886, accuracy : 97.03\n",
      "iteration : 250, loss : 0.0903, accuracy : 96.96\n",
      "iteration : 300, loss : 0.0909, accuracy : 96.93\n",
      "iteration : 350, loss : 0.0920, accuracy : 96.89\n",
      "Epoch : 146, training loss : 0.0921, training accuracy : 96.87, test loss : 0.2662, test accuracy : 92.36\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.0861, accuracy : 97.20\n",
      "iteration : 100, loss : 0.0875, accuracy : 97.12\n",
      "iteration : 150, loss : 0.0908, accuracy : 97.01\n",
      "iteration : 200, loss : 0.0911, accuracy : 96.96\n",
      "iteration : 250, loss : 0.0903, accuracy : 97.03\n",
      "iteration : 300, loss : 0.0902, accuracy : 97.04\n",
      "iteration : 350, loss : 0.0909, accuracy : 97.00\n",
      "Epoch : 147, training loss : 0.0902, training accuracy : 96.99, test loss : 0.2456, test accuracy : 92.90\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.0793, accuracy : 97.42\n",
      "iteration : 100, loss : 0.0864, accuracy : 97.05\n",
      "iteration : 150, loss : 0.0847, accuracy : 97.10\n",
      "iteration : 200, loss : 0.0853, accuracy : 97.07\n",
      "iteration : 250, loss : 0.0857, accuracy : 97.02\n",
      "iteration : 300, loss : 0.0864, accuracy : 97.00\n",
      "iteration : 350, loss : 0.0865, accuracy : 97.00\n",
      "Epoch : 148, training loss : 0.0866, training accuracy : 97.00, test loss : 0.2546, test accuracy : 92.87\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.0731, accuracy : 97.67\n",
      "iteration : 100, loss : 0.0785, accuracy : 97.47\n",
      "iteration : 150, loss : 0.0796, accuracy : 97.41\n",
      "iteration : 200, loss : 0.0817, accuracy : 97.30\n",
      "iteration : 250, loss : 0.0842, accuracy : 97.18\n",
      "iteration : 300, loss : 0.0831, accuracy : 97.22\n",
      "iteration : 350, loss : 0.0832, accuracy : 97.20\n",
      "Epoch : 149, training loss : 0.0837, training accuracy : 97.19, test loss : 0.2711, test accuracy : 92.42\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.0830, accuracy : 97.50\n",
      "iteration : 100, loss : 0.0840, accuracy : 97.45\n",
      "iteration : 150, loss : 0.0827, accuracy : 97.45\n",
      "iteration : 200, loss : 0.0820, accuracy : 97.42\n",
      "iteration : 250, loss : 0.0827, accuracy : 97.34\n",
      "iteration : 300, loss : 0.0826, accuracy : 97.30\n",
      "iteration : 350, loss : 0.0833, accuracy : 97.27\n",
      "Epoch : 150, training loss : 0.0831, training accuracy : 97.26, test loss : 0.2334, test accuracy : 93.18\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.0818, accuracy : 97.28\n",
      "iteration : 100, loss : 0.0815, accuracy : 97.31\n",
      "iteration : 150, loss : 0.0809, accuracy : 97.27\n",
      "iteration : 200, loss : 0.0811, accuracy : 97.25\n",
      "iteration : 250, loss : 0.0814, accuracy : 97.23\n",
      "iteration : 300, loss : 0.0817, accuracy : 97.22\n",
      "iteration : 350, loss : 0.0812, accuracy : 97.22\n",
      "Epoch : 151, training loss : 0.0823, training accuracy : 97.18, test loss : 0.2469, test accuracy : 92.67\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.0755, accuracy : 97.55\n",
      "iteration : 100, loss : 0.0782, accuracy : 97.41\n",
      "iteration : 150, loss : 0.0771, accuracy : 97.41\n",
      "iteration : 200, loss : 0.0779, accuracy : 97.39\n",
      "iteration : 250, loss : 0.0783, accuracy : 97.37\n",
      "iteration : 300, loss : 0.0768, accuracy : 97.41\n",
      "iteration : 350, loss : 0.0770, accuracy : 97.39\n",
      "Epoch : 152, training loss : 0.0775, training accuracy : 97.38, test loss : 0.2594, test accuracy : 92.71\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.0823, accuracy : 97.09\n",
      "iteration : 100, loss : 0.0780, accuracy : 97.21\n",
      "iteration : 150, loss : 0.0795, accuracy : 97.22\n",
      "iteration : 200, loss : 0.0800, accuracy : 97.23\n",
      "iteration : 250, loss : 0.0794, accuracy : 97.30\n",
      "iteration : 300, loss : 0.0779, accuracy : 97.33\n",
      "iteration : 350, loss : 0.0781, accuracy : 97.31\n",
      "Epoch : 153, training loss : 0.0781, training accuracy : 97.33, test loss : 0.2467, test accuracy : 92.98\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.0688, accuracy : 97.84\n",
      "iteration : 100, loss : 0.0728, accuracy : 97.62\n",
      "iteration : 150, loss : 0.0743, accuracy : 97.53\n",
      "iteration : 200, loss : 0.0739, accuracy : 97.51\n",
      "iteration : 250, loss : 0.0743, accuracy : 97.50\n",
      "iteration : 300, loss : 0.0742, accuracy : 97.53\n",
      "iteration : 350, loss : 0.0747, accuracy : 97.51\n",
      "Epoch : 154, training loss : 0.0747, training accuracy : 97.49, test loss : 0.2550, test accuracy : 92.88\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.0688, accuracy : 97.64\n",
      "iteration : 100, loss : 0.0690, accuracy : 97.66\n",
      "iteration : 150, loss : 0.0702, accuracy : 97.61\n",
      "iteration : 200, loss : 0.0717, accuracy : 97.56\n",
      "iteration : 250, loss : 0.0715, accuracy : 97.53\n",
      "iteration : 300, loss : 0.0724, accuracy : 97.54\n",
      "iteration : 350, loss : 0.0732, accuracy : 97.51\n",
      "Epoch : 155, training loss : 0.0733, training accuracy : 97.52, test loss : 0.2418, test accuracy : 93.36\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.0770, accuracy : 97.27\n",
      "iteration : 100, loss : 0.0720, accuracy : 97.48\n",
      "iteration : 150, loss : 0.0722, accuracy : 97.48\n",
      "iteration : 200, loss : 0.0713, accuracy : 97.57\n",
      "iteration : 250, loss : 0.0702, accuracy : 97.62\n",
      "iteration : 300, loss : 0.0696, accuracy : 97.64\n",
      "iteration : 350, loss : 0.0707, accuracy : 97.60\n",
      "Epoch : 156, training loss : 0.0705, training accuracy : 97.60, test loss : 0.2434, test accuracy : 93.00\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.0657, accuracy : 97.89\n",
      "iteration : 100, loss : 0.0680, accuracy : 97.81\n",
      "iteration : 150, loss : 0.0691, accuracy : 97.70\n",
      "iteration : 200, loss : 0.0690, accuracy : 97.71\n",
      "iteration : 250, loss : 0.0691, accuracy : 97.72\n",
      "iteration : 300, loss : 0.0686, accuracy : 97.75\n",
      "iteration : 350, loss : 0.0684, accuracy : 97.75\n",
      "Epoch : 157, training loss : 0.0685, training accuracy : 97.74, test loss : 0.2502, test accuracy : 92.97\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.0671, accuracy : 97.73\n",
      "iteration : 100, loss : 0.0644, accuracy : 97.88\n",
      "iteration : 150, loss : 0.0642, accuracy : 97.88\n",
      "iteration : 200, loss : 0.0635, accuracy : 97.90\n",
      "iteration : 250, loss : 0.0639, accuracy : 97.89\n",
      "iteration : 300, loss : 0.0643, accuracy : 97.87\n",
      "iteration : 350, loss : 0.0651, accuracy : 97.84\n",
      "Epoch : 158, training loss : 0.0651, training accuracy : 97.84, test loss : 0.2506, test accuracy : 93.16\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.0636, accuracy : 97.88\n",
      "iteration : 100, loss : 0.0660, accuracy : 97.74\n",
      "iteration : 150, loss : 0.0667, accuracy : 97.76\n",
      "iteration : 200, loss : 0.0685, accuracy : 97.69\n",
      "iteration : 250, loss : 0.0672, accuracy : 97.78\n",
      "iteration : 300, loss : 0.0653, accuracy : 97.81\n",
      "iteration : 350, loss : 0.0649, accuracy : 97.81\n",
      "Epoch : 159, training loss : 0.0656, training accuracy : 97.79, test loss : 0.2539, test accuracy : 92.97\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.0600, accuracy : 98.05\n",
      "iteration : 100, loss : 0.0590, accuracy : 98.06\n",
      "iteration : 150, loss : 0.0618, accuracy : 97.93\n",
      "iteration : 200, loss : 0.0635, accuracy : 97.90\n",
      "iteration : 250, loss : 0.0639, accuracy : 97.86\n",
      "iteration : 300, loss : 0.0636, accuracy : 97.89\n",
      "iteration : 350, loss : 0.0628, accuracy : 97.93\n",
      "Epoch : 160, training loss : 0.0631, training accuracy : 97.93, test loss : 0.2429, test accuracy : 93.51\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.0639, accuracy : 97.80\n",
      "iteration : 100, loss : 0.0634, accuracy : 97.86\n",
      "iteration : 150, loss : 0.0636, accuracy : 97.84\n",
      "iteration : 200, loss : 0.0635, accuracy : 97.88\n",
      "iteration : 250, loss : 0.0632, accuracy : 97.93\n",
      "iteration : 300, loss : 0.0643, accuracy : 97.87\n",
      "iteration : 350, loss : 0.0636, accuracy : 97.92\n",
      "Epoch : 161, training loss : 0.0636, training accuracy : 97.89, test loss : 0.2399, test accuracy : 93.37\n",
      "\n",
      "Epoch: 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0573, accuracy : 98.16\n",
      "iteration : 100, loss : 0.0583, accuracy : 98.12\n",
      "iteration : 150, loss : 0.0599, accuracy : 98.03\n",
      "iteration : 200, loss : 0.0614, accuracy : 98.00\n",
      "iteration : 250, loss : 0.0608, accuracy : 98.04\n",
      "iteration : 300, loss : 0.0611, accuracy : 98.02\n",
      "iteration : 350, loss : 0.0617, accuracy : 97.99\n",
      "Epoch : 162, training loss : 0.0625, training accuracy : 97.97, test loss : 0.2453, test accuracy : 93.23\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.0606, accuracy : 98.03\n",
      "iteration : 100, loss : 0.0590, accuracy : 98.10\n",
      "iteration : 150, loss : 0.0556, accuracy : 98.17\n",
      "iteration : 200, loss : 0.0577, accuracy : 98.04\n",
      "iteration : 250, loss : 0.0591, accuracy : 98.01\n",
      "iteration : 300, loss : 0.0586, accuracy : 98.02\n",
      "iteration : 350, loss : 0.0586, accuracy : 98.03\n",
      "Epoch : 163, training loss : 0.0591, training accuracy : 97.98, test loss : 0.2508, test accuracy : 93.02\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.0591, accuracy : 98.05\n",
      "iteration : 100, loss : 0.0621, accuracy : 98.00\n",
      "iteration : 150, loss : 0.0608, accuracy : 98.11\n",
      "iteration : 200, loss : 0.0595, accuracy : 98.18\n",
      "iteration : 250, loss : 0.0592, accuracy : 98.18\n",
      "iteration : 300, loss : 0.0575, accuracy : 98.22\n",
      "iteration : 350, loss : 0.0570, accuracy : 98.22\n",
      "Epoch : 164, training loss : 0.0574, training accuracy : 98.19, test loss : 0.2486, test accuracy : 93.17\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.0605, accuracy : 98.16\n",
      "iteration : 100, loss : 0.0605, accuracy : 98.09\n",
      "iteration : 150, loss : 0.0597, accuracy : 98.08\n",
      "iteration : 200, loss : 0.0591, accuracy : 98.05\n",
      "iteration : 250, loss : 0.0593, accuracy : 98.06\n",
      "iteration : 300, loss : 0.0585, accuracy : 98.10\n",
      "iteration : 350, loss : 0.0575, accuracy : 98.14\n",
      "Epoch : 165, training loss : 0.0570, training accuracy : 98.15, test loss : 0.2458, test accuracy : 93.11\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.0531, accuracy : 98.30\n",
      "iteration : 100, loss : 0.0553, accuracy : 98.23\n",
      "iteration : 150, loss : 0.0547, accuracy : 98.17\n",
      "iteration : 200, loss : 0.0553, accuracy : 98.16\n",
      "iteration : 250, loss : 0.0543, accuracy : 98.21\n",
      "iteration : 300, loss : 0.0541, accuracy : 98.21\n",
      "iteration : 350, loss : 0.0537, accuracy : 98.24\n",
      "Epoch : 166, training loss : 0.0536, training accuracy : 98.24, test loss : 0.2504, test accuracy : 93.19\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.0514, accuracy : 98.36\n",
      "iteration : 100, loss : 0.0530, accuracy : 98.17\n",
      "iteration : 150, loss : 0.0543, accuracy : 98.19\n",
      "iteration : 200, loss : 0.0545, accuracy : 98.21\n",
      "iteration : 250, loss : 0.0535, accuracy : 98.28\n",
      "iteration : 300, loss : 0.0530, accuracy : 98.30\n",
      "iteration : 350, loss : 0.0544, accuracy : 98.24\n",
      "Epoch : 167, training loss : 0.0547, training accuracy : 98.24, test loss : 0.2422, test accuracy : 93.26\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.0557, accuracy : 98.08\n",
      "iteration : 100, loss : 0.0556, accuracy : 98.10\n",
      "iteration : 150, loss : 0.0556, accuracy : 98.11\n",
      "iteration : 200, loss : 0.0555, accuracy : 98.12\n",
      "iteration : 250, loss : 0.0554, accuracy : 98.11\n",
      "iteration : 300, loss : 0.0560, accuracy : 98.10\n",
      "iteration : 350, loss : 0.0555, accuracy : 98.14\n",
      "Epoch : 168, training loss : 0.0546, training accuracy : 98.18, test loss : 0.2434, test accuracy : 93.46\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.0502, accuracy : 98.34\n",
      "iteration : 100, loss : 0.0486, accuracy : 98.40\n",
      "iteration : 150, loss : 0.0507, accuracy : 98.33\n",
      "iteration : 200, loss : 0.0504, accuracy : 98.36\n",
      "iteration : 250, loss : 0.0506, accuracy : 98.35\n",
      "iteration : 300, loss : 0.0505, accuracy : 98.36\n",
      "iteration : 350, loss : 0.0497, accuracy : 98.41\n",
      "Epoch : 169, training loss : 0.0492, training accuracy : 98.43, test loss : 0.2397, test accuracy : 93.32\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.0528, accuracy : 98.33\n",
      "iteration : 100, loss : 0.0512, accuracy : 98.30\n",
      "iteration : 150, loss : 0.0518, accuracy : 98.28\n",
      "iteration : 200, loss : 0.0507, accuracy : 98.34\n",
      "iteration : 250, loss : 0.0504, accuracy : 98.31\n",
      "iteration : 300, loss : 0.0497, accuracy : 98.34\n",
      "iteration : 350, loss : 0.0490, accuracy : 98.35\n",
      "Epoch : 170, training loss : 0.0489, training accuracy : 98.37, test loss : 0.2408, test accuracy : 93.43\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.0493, accuracy : 98.45\n",
      "iteration : 100, loss : 0.0488, accuracy : 98.34\n",
      "iteration : 150, loss : 0.0499, accuracy : 98.33\n",
      "iteration : 200, loss : 0.0487, accuracy : 98.39\n",
      "iteration : 250, loss : 0.0492, accuracy : 98.36\n",
      "iteration : 300, loss : 0.0489, accuracy : 98.36\n",
      "iteration : 350, loss : 0.0487, accuracy : 98.36\n",
      "Epoch : 171, training loss : 0.0493, training accuracy : 98.34, test loss : 0.2302, test accuracy : 93.38\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.0431, accuracy : 98.67\n",
      "iteration : 100, loss : 0.0450, accuracy : 98.58\n",
      "iteration : 150, loss : 0.0480, accuracy : 98.46\n",
      "iteration : 200, loss : 0.0474, accuracy : 98.46\n",
      "iteration : 250, loss : 0.0478, accuracy : 98.43\n",
      "iteration : 300, loss : 0.0476, accuracy : 98.44\n",
      "iteration : 350, loss : 0.0476, accuracy : 98.47\n",
      "Epoch : 172, training loss : 0.0474, training accuracy : 98.48, test loss : 0.2396, test accuracy : 93.25\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.0551, accuracy : 98.09\n",
      "iteration : 100, loss : 0.0542, accuracy : 98.22\n",
      "iteration : 150, loss : 0.0526, accuracy : 98.27\n",
      "iteration : 200, loss : 0.0513, accuracy : 98.32\n",
      "iteration : 250, loss : 0.0510, accuracy : 98.35\n",
      "iteration : 300, loss : 0.0506, accuracy : 98.35\n",
      "iteration : 350, loss : 0.0504, accuracy : 98.38\n",
      "Epoch : 173, training loss : 0.0499, training accuracy : 98.41, test loss : 0.2320, test accuracy : 93.36\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.0498, accuracy : 98.41\n",
      "iteration : 100, loss : 0.0473, accuracy : 98.51\n",
      "iteration : 150, loss : 0.0474, accuracy : 98.53\n",
      "iteration : 200, loss : 0.0467, accuracy : 98.52\n",
      "iteration : 250, loss : 0.0465, accuracy : 98.49\n",
      "iteration : 300, loss : 0.0459, accuracy : 98.47\n",
      "iteration : 350, loss : 0.0452, accuracy : 98.50\n",
      "Epoch : 174, training loss : 0.0461, training accuracy : 98.48, test loss : 0.2383, test accuracy : 93.43\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.0484, accuracy : 98.34\n",
      "iteration : 100, loss : 0.0464, accuracy : 98.52\n",
      "iteration : 150, loss : 0.0467, accuracy : 98.49\n",
      "iteration : 200, loss : 0.0462, accuracy : 98.52\n",
      "iteration : 250, loss : 0.0463, accuracy : 98.53\n",
      "iteration : 300, loss : 0.0459, accuracy : 98.54\n",
      "iteration : 350, loss : 0.0452, accuracy : 98.57\n",
      "Epoch : 175, training loss : 0.0451, training accuracy : 98.58, test loss : 0.2313, test accuracy : 93.65\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.0442, accuracy : 98.59\n",
      "iteration : 100, loss : 0.0441, accuracy : 98.63\n",
      "iteration : 150, loss : 0.0433, accuracy : 98.61\n",
      "iteration : 200, loss : 0.0429, accuracy : 98.63\n",
      "iteration : 250, loss : 0.0435, accuracy : 98.61\n",
      "iteration : 300, loss : 0.0436, accuracy : 98.63\n",
      "iteration : 350, loss : 0.0440, accuracy : 98.60\n",
      "Epoch : 176, training loss : 0.0441, training accuracy : 98.61, test loss : 0.2310, test accuracy : 93.68\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.0398, accuracy : 98.69\n",
      "iteration : 100, loss : 0.0398, accuracy : 98.64\n",
      "iteration : 150, loss : 0.0394, accuracy : 98.66\n",
      "iteration : 200, loss : 0.0401, accuracy : 98.65\n",
      "iteration : 250, loss : 0.0410, accuracy : 98.62\n",
      "iteration : 300, loss : 0.0403, accuracy : 98.63\n",
      "iteration : 350, loss : 0.0410, accuracy : 98.66\n",
      "Epoch : 177, training loss : 0.0413, training accuracy : 98.64, test loss : 0.2311, test accuracy : 93.69\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.0423, accuracy : 98.64\n",
      "iteration : 100, loss : 0.0431, accuracy : 98.61\n",
      "iteration : 150, loss : 0.0428, accuracy : 98.64\n",
      "iteration : 200, loss : 0.0429, accuracy : 98.63\n",
      "iteration : 250, loss : 0.0422, accuracy : 98.63\n",
      "iteration : 300, loss : 0.0429, accuracy : 98.63\n",
      "iteration : 350, loss : 0.0430, accuracy : 98.62\n",
      "Epoch : 178, training loss : 0.0434, training accuracy : 98.58, test loss : 0.2380, test accuracy : 93.36\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.0440, accuracy : 98.72\n",
      "iteration : 100, loss : 0.0420, accuracy : 98.76\n",
      "iteration : 150, loss : 0.0411, accuracy : 98.77\n",
      "iteration : 200, loss : 0.0403, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0403, accuracy : 98.76\n",
      "iteration : 300, loss : 0.0404, accuracy : 98.76\n",
      "iteration : 350, loss : 0.0414, accuracy : 98.69\n",
      "Epoch : 179, training loss : 0.0418, training accuracy : 98.68, test loss : 0.2406, test accuracy : 93.56\n",
      "\n",
      "Epoch: 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0449, accuracy : 98.55\n",
      "iteration : 100, loss : 0.0446, accuracy : 98.57\n",
      "iteration : 150, loss : 0.0447, accuracy : 98.56\n",
      "iteration : 200, loss : 0.0447, accuracy : 98.55\n",
      "iteration : 250, loss : 0.0447, accuracy : 98.57\n",
      "iteration : 300, loss : 0.0442, accuracy : 98.57\n",
      "iteration : 350, loss : 0.0444, accuracy : 98.56\n",
      "Epoch : 180, training loss : 0.0441, training accuracy : 98.57, test loss : 0.2446, test accuracy : 93.39\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.0457, accuracy : 98.73\n",
      "iteration : 100, loss : 0.0428, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0406, accuracy : 98.80\n",
      "iteration : 200, loss : 0.0421, accuracy : 98.72\n",
      "iteration : 250, loss : 0.0421, accuracy : 98.68\n",
      "iteration : 300, loss : 0.0415, accuracy : 98.70\n",
      "iteration : 350, loss : 0.0412, accuracy : 98.71\n",
      "Epoch : 181, training loss : 0.0415, training accuracy : 98.72, test loss : 0.2360, test accuracy : 93.65\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.0423, accuracy : 98.73\n",
      "iteration : 100, loss : 0.0440, accuracy : 98.66\n",
      "iteration : 150, loss : 0.0426, accuracy : 98.69\n",
      "iteration : 200, loss : 0.0432, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0425, accuracy : 98.69\n",
      "iteration : 300, loss : 0.0414, accuracy : 98.72\n",
      "iteration : 350, loss : 0.0411, accuracy : 98.73\n",
      "Epoch : 182, training loss : 0.0408, training accuracy : 98.75, test loss : 0.2312, test accuracy : 93.63\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.0475, accuracy : 98.45\n",
      "iteration : 100, loss : 0.0439, accuracy : 98.63\n",
      "iteration : 150, loss : 0.0419, accuracy : 98.70\n",
      "iteration : 200, loss : 0.0412, accuracy : 98.68\n",
      "iteration : 250, loss : 0.0413, accuracy : 98.69\n",
      "iteration : 300, loss : 0.0408, accuracy : 98.69\n",
      "iteration : 350, loss : 0.0407, accuracy : 98.68\n",
      "Epoch : 183, training loss : 0.0405, training accuracy : 98.68, test loss : 0.2363, test accuracy : 93.62\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.0368, accuracy : 98.84\n",
      "iteration : 100, loss : 0.0370, accuracy : 98.83\n",
      "iteration : 150, loss : 0.0402, accuracy : 98.74\n",
      "iteration : 200, loss : 0.0401, accuracy : 98.75\n",
      "iteration : 250, loss : 0.0394, accuracy : 98.78\n",
      "iteration : 300, loss : 0.0400, accuracy : 98.74\n",
      "iteration : 350, loss : 0.0401, accuracy : 98.74\n",
      "Epoch : 184, training loss : 0.0394, training accuracy : 98.76, test loss : 0.2255, test accuracy : 93.85\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.0398, accuracy : 98.80\n",
      "iteration : 100, loss : 0.0365, accuracy : 98.88\n",
      "iteration : 150, loss : 0.0381, accuracy : 98.80\n",
      "iteration : 200, loss : 0.0391, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0394, accuracy : 98.77\n",
      "iteration : 300, loss : 0.0396, accuracy : 98.75\n",
      "iteration : 350, loss : 0.0396, accuracy : 98.76\n",
      "Epoch : 185, training loss : 0.0393, training accuracy : 98.76, test loss : 0.2332, test accuracy : 93.76\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.0468, accuracy : 98.50\n",
      "iteration : 100, loss : 0.0429, accuracy : 98.62\n",
      "iteration : 150, loss : 0.0410, accuracy : 98.69\n",
      "iteration : 200, loss : 0.0402, accuracy : 98.76\n",
      "iteration : 250, loss : 0.0401, accuracy : 98.73\n",
      "iteration : 300, loss : 0.0394, accuracy : 98.75\n",
      "iteration : 350, loss : 0.0382, accuracy : 98.81\n",
      "Epoch : 186, training loss : 0.0385, training accuracy : 98.80, test loss : 0.2374, test accuracy : 93.52\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.0416, accuracy : 98.56\n",
      "iteration : 100, loss : 0.0400, accuracy : 98.64\n",
      "iteration : 150, loss : 0.0400, accuracy : 98.69\n",
      "iteration : 200, loss : 0.0404, accuracy : 98.71\n",
      "iteration : 250, loss : 0.0398, accuracy : 98.72\n",
      "iteration : 300, loss : 0.0386, accuracy : 98.78\n",
      "iteration : 350, loss : 0.0387, accuracy : 98.77\n",
      "Epoch : 187, training loss : 0.0389, training accuracy : 98.76, test loss : 0.2360, test accuracy : 93.54\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.0365, accuracy : 98.81\n",
      "iteration : 100, loss : 0.0376, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0388, accuracy : 98.77\n",
      "iteration : 200, loss : 0.0389, accuracy : 98.77\n",
      "iteration : 250, loss : 0.0383, accuracy : 98.81\n",
      "iteration : 300, loss : 0.0383, accuracy : 98.83\n",
      "iteration : 350, loss : 0.0382, accuracy : 98.83\n",
      "Epoch : 188, training loss : 0.0376, training accuracy : 98.84, test loss : 0.2385, test accuracy : 93.57\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.0433, accuracy : 98.56\n",
      "iteration : 100, loss : 0.0397, accuracy : 98.70\n",
      "iteration : 150, loss : 0.0383, accuracy : 98.74\n",
      "iteration : 200, loss : 0.0385, accuracy : 98.72\n",
      "iteration : 250, loss : 0.0385, accuracy : 98.73\n",
      "iteration : 300, loss : 0.0385, accuracy : 98.76\n",
      "iteration : 350, loss : 0.0385, accuracy : 98.75\n",
      "Epoch : 189, training loss : 0.0380, training accuracy : 98.78, test loss : 0.2341, test accuracy : 93.71\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.0371, accuracy : 98.69\n",
      "iteration : 100, loss : 0.0375, accuracy : 98.74\n",
      "iteration : 150, loss : 0.0359, accuracy : 98.82\n",
      "iteration : 200, loss : 0.0353, accuracy : 98.86\n",
      "iteration : 250, loss : 0.0355, accuracy : 98.86\n",
      "iteration : 300, loss : 0.0365, accuracy : 98.83\n",
      "iteration : 350, loss : 0.0367, accuracy : 98.83\n",
      "Epoch : 190, training loss : 0.0367, training accuracy : 98.85, test loss : 0.2329, test accuracy : 93.57\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.0387, accuracy : 98.81\n",
      "iteration : 100, loss : 0.0392, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0382, accuracy : 98.78\n",
      "iteration : 200, loss : 0.0381, accuracy : 98.81\n",
      "iteration : 250, loss : 0.0386, accuracy : 98.78\n",
      "iteration : 300, loss : 0.0394, accuracy : 98.75\n",
      "iteration : 350, loss : 0.0389, accuracy : 98.75\n",
      "Epoch : 191, training loss : 0.0391, training accuracy : 98.74, test loss : 0.2292, test accuracy : 93.74\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.0286, accuracy : 99.14\n",
      "iteration : 100, loss : 0.0325, accuracy : 99.05\n",
      "iteration : 150, loss : 0.0323, accuracy : 99.03\n",
      "iteration : 200, loss : 0.0333, accuracy : 98.98\n",
      "iteration : 250, loss : 0.0344, accuracy : 98.92\n",
      "iteration : 300, loss : 0.0346, accuracy : 98.93\n",
      "iteration : 350, loss : 0.0352, accuracy : 98.91\n",
      "Epoch : 192, training loss : 0.0361, training accuracy : 98.88, test loss : 0.2394, test accuracy : 93.57\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.0458, accuracy : 98.52\n",
      "iteration : 100, loss : 0.0398, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0386, accuracy : 98.83\n",
      "iteration : 200, loss : 0.0393, accuracy : 98.76\n",
      "iteration : 250, loss : 0.0395, accuracy : 98.75\n",
      "iteration : 300, loss : 0.0404, accuracy : 98.72\n",
      "iteration : 350, loss : 0.0398, accuracy : 98.74\n",
      "Epoch : 193, training loss : 0.0389, training accuracy : 98.76, test loss : 0.2335, test accuracy : 93.67\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.0430, accuracy : 98.58\n",
      "iteration : 100, loss : 0.0386, accuracy : 98.77\n",
      "iteration : 150, loss : 0.0380, accuracy : 98.82\n",
      "iteration : 200, loss : 0.0377, accuracy : 98.79\n",
      "iteration : 250, loss : 0.0374, accuracy : 98.78\n",
      "iteration : 300, loss : 0.0370, accuracy : 98.81\n",
      "iteration : 350, loss : 0.0371, accuracy : 98.80\n",
      "Epoch : 194, training loss : 0.0369, training accuracy : 98.80, test loss : 0.2349, test accuracy : 93.66\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.0352, accuracy : 98.88\n",
      "iteration : 100, loss : 0.0372, accuracy : 98.79\n",
      "iteration : 150, loss : 0.0365, accuracy : 98.80\n",
      "iteration : 200, loss : 0.0373, accuracy : 98.79\n",
      "iteration : 250, loss : 0.0372, accuracy : 98.83\n",
      "iteration : 300, loss : 0.0369, accuracy : 98.86\n",
      "iteration : 350, loss : 0.0371, accuracy : 98.86\n",
      "Epoch : 195, training loss : 0.0373, training accuracy : 98.85, test loss : 0.2368, test accuracy : 93.54\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.0407, accuracy : 98.75\n",
      "iteration : 100, loss : 0.0394, accuracy : 98.80\n",
      "iteration : 150, loss : 0.0376, accuracy : 98.81\n",
      "iteration : 200, loss : 0.0376, accuracy : 98.82\n",
      "iteration : 250, loss : 0.0371, accuracy : 98.83\n",
      "iteration : 300, loss : 0.0369, accuracy : 98.84\n",
      "iteration : 350, loss : 0.0375, accuracy : 98.81\n",
      "Epoch : 196, training loss : 0.0378, training accuracy : 98.80, test loss : 0.2342, test accuracy : 93.62\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.0328, accuracy : 99.00\n",
      "iteration : 100, loss : 0.0338, accuracy : 98.94\n",
      "iteration : 150, loss : 0.0342, accuracy : 98.96\n",
      "iteration : 200, loss : 0.0347, accuracy : 98.92\n",
      "iteration : 250, loss : 0.0339, accuracy : 98.96\n",
      "iteration : 300, loss : 0.0339, accuracy : 98.96\n",
      "iteration : 350, loss : 0.0345, accuracy : 98.93\n",
      "Epoch : 197, training loss : 0.0349, training accuracy : 98.93, test loss : 0.2334, test accuracy : 93.67\n",
      "\n",
      "Epoch: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  50, loss : 0.0355, accuracy : 98.84\n",
      "iteration : 100, loss : 0.0342, accuracy : 98.91\n",
      "iteration : 150, loss : 0.0356, accuracy : 98.90\n",
      "iteration : 200, loss : 0.0345, accuracy : 98.93\n",
      "iteration : 250, loss : 0.0352, accuracy : 98.92\n",
      "iteration : 300, loss : 0.0370, accuracy : 98.86\n",
      "iteration : 350, loss : 0.0372, accuracy : 98.86\n",
      "Epoch : 198, training loss : 0.0367, training accuracy : 98.87, test loss : 0.2302, test accuracy : 93.71\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.0388, accuracy : 98.77\n",
      "iteration : 100, loss : 0.0356, accuracy : 98.87\n",
      "iteration : 150, loss : 0.0368, accuracy : 98.86\n",
      "iteration : 200, loss : 0.0382, accuracy : 98.79\n",
      "iteration : 250, loss : 0.0388, accuracy : 98.76\n",
      "iteration : 300, loss : 0.0381, accuracy : 98.78\n",
      "iteration : 350, loss : 0.0382, accuracy : 98.79\n",
      "Epoch : 199, training loss : 0.0383, training accuracy : 98.80, test loss : 0.2373, test accuracy : 93.59\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4\n",
    "}\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list  = []\n",
    "test_loss_list  = []\n",
    "test_acc_list  = []\n",
    "\n",
    "\n",
    "net = ResNet18().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "#print(scheduler)\n",
    "for epoch in range(0, 200):\n",
    "    # normal - train, jaco- jaco_train, mma - mma_train\n",
    "    train_loss, train_acc = train(epoch, net, criterion, trainloader,scheduler)\n",
    "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
    "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
    "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
    "\n",
    "plt.xlabel(\"N0. of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs No. of epochs\")\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
    "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
    "plt.xlabel(\"NO. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\" Accuracy vs N0. of epochs\")\n",
    "plt.legend(['train', 'test']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
